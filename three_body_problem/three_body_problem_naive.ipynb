{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c314520",
   "metadata": {
    "id": "2c314520"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.rcParams['pgf.texsystem'] = 'pdflatex'\n",
    "# matplotlib.rcParams.update({'font.family': 'serif', 'font.size': 10})\n",
    "# matplotlib.rcParams['text.usetex'] = True\n",
    "from matplotlib.lines import Line2D\n",
    "import pickle\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "import time\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35979de5",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This jupyter notebook implements the plain time-consistent physics-informed neural network (tcPINN) idea for the planar three-body problem. We have observed that during training, the tcPINN is unable to completely learn the dynamics. That is, the PINN loss does not converge to zero.\n",
    "\n",
    "We hypothesize that this phenomenon is not caused by the ODE system being chaotic, but rather by the velocities of bodies converging to infinity during close encounters. This causes the loss function to have singularities (e.g. when $r_1(t) = r_2(t)$), which makes training significantly more difficult. Default single-precision floating-point format used by Pytorch might be insufficient when denominators in summands of the PINN loss become very small. Therefore, using double- or even higher precsion might still allow to learn the dynamics of the three-body problem with our proposed approach. \n",
    "\n",
    "Instead of focusing on floating-point precision during training, we tried to simplify the training task by exploiting the symmetry and scale invariance of the ODE system. This allowed us to decrease the domain of the initial values. When the barycenter of the three bodies is constant in time, the size of the ODE system can also be reduced from twelve to eight. Further details to these ideas are given in the notebooks $\\texttt{three_body_problem_center_barycenter}$ and $\\texttt{three_body_problem_center_barycenter_restricted_ivp.ipynb}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6d595",
   "metadata": {
    "id": "dab6d595"
   },
   "source": [
    "Consider three gravitationally interacting identical bodies with positions $r_i(t) \\in \\mathbb{R}^2$. Assuming a gravitational force of $G=1$, the Newtonian equations governing their motion reads\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{d^2}{dt^2} \\begin{pmatrix} r_1(t) \\\\ r_2(t) \\\\ r_3(t) \\end{pmatrix} = \\begin{pmatrix} - \\frac{r_1(t) - r_2(t)}{|r_1(t) - r_2(t)|^3} - \\frac{r_1(t) - r_3(t)}{|r_1(t) - r_3(t)|^3} \\\\ - \\frac{r_2(t) - r_1(t)}{|r_2(t) - r_1(t)|^3} - \\frac{r_2(t) - r_3(t)}{|r_2(t) - r_3(t)|^3} \\\\ - \\frac{r_3(t) - r_1(t)}{|r_3(t) - r_1(t)|^3} - \\frac{r_3(t) - r_2(t)}{|r_3(t) - r_2(t)|^3} \\end{pmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "This is a second-order ODE system of $6$ equations. By introducing the velocities $v_i(t) = \\frac{d}{dt}r_i(t)$, it can be rewritten as the following first-order ODE system of $12$ equations:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{d}{dt} \\begin{pmatrix} r_1 \\\\ r_2 \\\\ r_3 \\\\ v_1 \\\\ v_2 \\\\ v_3 \\end{pmatrix} = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ - \\frac{r_1 - r_2}{|r_1 - r_2|^3} - \\frac{r_1 - r_3}{|r_1 - r_3|^3} \\\\ - \\frac{r_2 - r_1}{|r_2 - r_1|^3} - \\frac{r_2 - r_3}{|r_2 - r_3|^3} \\\\ - \\frac{r_3 - r_1}{|r_3 - r_1|^3} - \\frac{r_3 - r_2}{|r_3 - r_2|^3} \\end{pmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "For completeness, all $12$ equations with written-out components are\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{d}{dt} \\begin{pmatrix} r_{11} \\\\ r_{12} \\\\ r_{21} \\\\ r_{22} \\\\ r_{31} \\\\ r_{32} \\\\ v_{11} \\\\ v_{12} \\\\ v_{21} \\\\ v_{22} \\\\ v_{31} \\\\ v_{32} \\end{pmatrix} = \\begin{pmatrix} \n",
    "    v_{11} \\\\ v_{12} \\\\ v_{21} \\\\ v_{22} \\\\ v_{31} \\\\ v_{32} \\\\ \n",
    "    - \\frac{r_{11} - r_{21}}{|r_1 - r_2|^3} - \\frac{r_{11} - r_{31}}{|r_1 - r_3|^3} \\\\\n",
    "    - \\frac{r_{12} - r_{22}}{|r_1 - r_2|^3} - \\frac{r_{12} - r_{32}}{|r_1 - r_3|^3} \\\\\n",
    "    - \\frac{r_{21} - r_{11}}{|r_2 - r_1|^3} - \\frac{r_{21} - r_{31}}{|r_2 - r_3|^3} \\\\\n",
    "    - \\frac{r_{22} - r_{12}}{|r_2 - r_1|^3} - \\frac{r_{22} - r_{32}}{|r_2 - r_3|^3} \\\\ \n",
    "    - \\frac{r_{31} - r_{11}}{|r_3 - r_1|^3} - \\frac{r_{31} - r_{21}}{|r_3 - r_2|^3} \\\\\n",
    "    - \\frac{r_{32} - r_{12}}{|r_3 - r_1|^3} - \\frac{r_{32} - r_{22}}{|r_3 - r_2|^3}\n",
    "\\end{pmatrix}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1809c5",
   "metadata": {
    "id": "0a1809c5"
   },
   "source": [
    "With the notation $y = (r_{11}, r_{12}, r_{21}, r_{22}, r_{31}, r_{32}, v_{11}, v_{12}, v_{21}, v_{22}, v_{31}, v_{32})$ used in the implementation, the system reads\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{d}{dt} y = \\begin{pmatrix} \n",
    "    y_6 \\\\ y_7 \\\\ y_8 \\\\ y_9 \\\\ y_{10} \\\\ y_{11} \\\\ \n",
    "    - \\frac{y_0 - y_2}{((y_0 - y_2)^2 + (y_1 - y_3)^2)^{3/2}} - \\frac{y_0 - y_4}{((y_0 - y_4)^2 + (y_1 - y_5)^2)^{3/2}} \\\\\n",
    "    - \\frac{y_1 - y_3}{((y_0 - y_2)^2 + (y_1 - y_3)^2)^{3/2}} - \\frac{y_1 - y_5}{((y_0 - y_4)^2 + (y_1 - y_5)^2)^{3/2}} \\\\\n",
    "    - \\frac{y_2 - y_0}{((y_0 - y_2)^2 + (y_1 - y_3)^2)^{3/2}} - \\frac{y_2 - y_4}{((y_2 - y_4)^2 + (y_3 - y_5)^2)^{3/2}} \\\\\n",
    "    - \\frac{y_3 - y_1}{((y_0 - y_2)^2 + (y_1 - y_3)^2)^{3/2}} - \\frac{y_3 - y_5}{((y_2 - y_4)^2 + (y_3 - y_5)^2)^{3/2}} \\\\ \n",
    "    - \\frac{y_4 - y_0}{((y_0 - y_4)^2 + (y_1 - y_5)^2)^{3/2}} - \\frac{y_4 - y_2}{((y_2 - y_4)^2 + (y_3 - y_5)^2)^{3/2}} \\\\\n",
    "    - \\frac{y_5 - y_1}{((y_0 - y_4)^2 + (y_1 - y_5)^2)^{3/2}} - \\frac{y_5 - y_3}{((y_2 - y_4)^2 + (y_3 - y_5)^2)^{3/2}}\n",
    "\\end{pmatrix}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "135abf6b",
   "metadata": {
    "id": "135abf6b"
   },
   "outputs": [],
   "source": [
    "# CUDA support \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92eed791",
   "metadata": {
    "id": "92eed791"
   },
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class DNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.Tanh\n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x = (t, y0)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "300b459c",
   "metadata": {
    "id": "300b459c"
   },
   "outputs": [],
   "source": [
    "# tcPINN: time-consistent physics-informed neural network\n",
    "class TcPINN():\n",
    "\n",
    "    def __init__(self, X_pinn, X_semigroup, X_smooth, layers, T):\n",
    "\n",
    "        # neural network architecture\n",
    "        self.layers = layers\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "        \n",
    "        # semigroup PINN step time\n",
    "        self.T = torch.tensor(T).float().to(device)\n",
    "\n",
    "        # training data\n",
    "        self.t_pinn = torch.tensor(X_pinn[:, :1], requires_grad=True).float().to(device)\n",
    "        self.y_pinn = torch.tensor(X_pinn[:, 1:], requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.s_semigroup = torch.tensor(X_semigroup[:, :1], requires_grad=True).float().to(device)\n",
    "        self.t_semigroup = torch.tensor(X_semigroup[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.y_semigroup = torch.tensor(X_semigroup[:, 2:], requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.t_smooth = torch.tensor(X_smooth[:, :1], requires_grad=True).float().to(device)\n",
    "        self.y_smooth = torch.tensor(X_smooth[:, 1:], requires_grad=True).float().to(device)\n",
    "        \n",
    "        # optimization\n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(), lr=1.0, max_iter=50000, max_eval=50000, \n",
    "            history_size=50, tolerance_grad=1e-5, tolerance_change=np.finfo(float).eps, \n",
    "            line_search_fn=\"strong_wolfe\"\n",
    "        )\n",
    "\n",
    "        self.iter = 0\n",
    "    \n",
    "    \n",
    "    def net_y(self, t, y0):\n",
    "        \n",
    "        # The M(t, y0) = y0 + t N(t, y0) scheme seems to drastically increase the accuracy\n",
    "        # This works perfectly fine with automatic differentiation\n",
    "        y = y0 + t * self.dnn(torch.cat([t, y0], dim=1))\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    \n",
    "    def net_derivative(self, t, y0):\n",
    "        \"\"\"\n",
    "        Pytorch automatic differentiation to compute the derivative of the neural network\n",
    "        \"\"\"\n",
    "        y = self.net_y(t, y0)\n",
    "        \n",
    "        # vectors for the autograd vector Jacobian product \n",
    "        # to compute the derivatives w.r.t. every output dimension\n",
    "        vectors = [torch.zeros_like(y) for _ in range(12)]\n",
    "        \n",
    "        for i, vec in enumerate(vectors):\n",
    "            \n",
    "            vec[:,i] = 1.\n",
    "        \n",
    "        # list of derivative tensors\n",
    "        # the first entry is a tensor with \\partial_t PINN_0(t, y0) for all (t, y0) in the batch,\n",
    "        # each input (t, y0) corresponds to one row in each tensor\n",
    "        derivatives = [\n",
    "            torch.autograd.grad(\n",
    "                y, t, \n",
    "                grad_outputs=vec,\n",
    "                retain_graph=True,\n",
    "                create_graph=True\n",
    "            )[0]\n",
    "            for vec in vectors\n",
    "        ]\n",
    "        \n",
    "        return derivatives\n",
    "    \n",
    "    \n",
    "    def loss_function(self):\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = self.net_y(self.t_pinn, self.y_pinn)\n",
    "        deriv_pred = self.net_derivative(self.t_pinn, self.y_pinn)\n",
    "        \n",
    "        # This is specific to the ODE\n",
    "        loss_pinn0 = torch.mean((deriv_pred[0] - y_pred[:,6:7]) ** 2)\n",
    "        loss_pinn1 = torch.mean((deriv_pred[1] - y_pred[:,7:8]) ** 2)\n",
    "        loss_pinn2 = torch.mean((deriv_pred[2] - y_pred[:,8:9]) ** 2)\n",
    "        loss_pinn3 = torch.mean((deriv_pred[3] - y_pred[:,9:10]) ** 2)\n",
    "        loss_pinn4 = torch.mean((deriv_pred[4] - y_pred[:,10:11]) ** 2)\n",
    "        loss_pinn5 = torch.mean((deriv_pred[5] - y_pred[:,11:12]) ** 2)\n",
    "        \n",
    "        loss_pinn6 = torch.mean((deriv_pred[6] + (y_pred[:,0:1] - y_pred[:,2:3]) / ((y_pred[:,0:1] - y_pred[:,2:3])**2 + (y_pred[:,1:2] - y_pred[:,3:4])**2)**(3/2) + (y_pred[:,0:1] - y_pred[:,4:5]) / ((y_pred[:,0:1] - y_pred[:,4:5])**2 + (y_pred[:,1:2] - y_pred[:,5:6])**2)**(3/2)) ** 2)\n",
    "        loss_pinn7 = torch.mean((deriv_pred[7] + (y_pred[:,1:2] - y_pred[:,3:4]) / ((y_pred[:,0:1] - y_pred[:,2:3])**2 + (y_pred[:,1:2] - y_pred[:,3:4])**2)**(3/2) + (y_pred[:,1:2] - y_pred[:,5:6]) / ((y_pred[:,0:1] - y_pred[:,4:5])**2 + (y_pred[:,1:2] - y_pred[:,5:6])**2)**(3/2)) ** 2)\n",
    "        loss_pinn8 = torch.mean((deriv_pred[8] + (y_pred[:,2:3] - y_pred[:,0:1]) / ((y_pred[:,0:1] - y_pred[:,2:3])**2 + (y_pred[:,1:2] - y_pred[:,3:4])**2)**(3/2) + (y_pred[:,2:3] - y_pred[:,4:5]) / ((y_pred[:,2:3] - y_pred[:,4:5])**2 + (y_pred[:,3:4] - y_pred[:,5:6])**2)**(3/2)) ** 2)\n",
    "        loss_pinn9 = torch.mean((deriv_pred[9] + (y_pred[:,3:4] - y_pred[:,1:2]) / ((y_pred[:,0:1] - y_pred[:,2:3])**2 + (y_pred[:,1:2] - y_pred[:,3:4])**2)**(3/2) + (y_pred[:,3:4] - y_pred[:,5:6]) / ((y_pred[:,2:3] - y_pred[:,4:5])**2 + (y_pred[:,3:4] - y_pred[:,5:6])**2)**(3/2)) ** 2)\n",
    "        loss_pinn10 = torch.mean((deriv_pred[10] + (y_pred[:,4:5] - y_pred[:,0:1]) / ((y_pred[:,0:1] - y_pred[:,4:5])**2 + (y_pred[:,1:2] - y_pred[:,5:6])**2)**(3/2) + (y_pred[:,4:5] - y_pred[:,2:3]) / ((y_pred[:,2:3] - y_pred[:,4:5])**2 + (y_pred[:,3:4] - y_pred[:,5:6])**2)**(3/2)) ** 2)\n",
    "        loss_pinn11 = torch.mean((deriv_pred[11] + (y_pred[:,5:6] - y_pred[:,1:2]) / ((y_pred[:,0:1] - y_pred[:,4:5])**2 + (y_pred[:,1:2] - y_pred[:,5:6])**2)**(3/2) + (y_pred[:,5:6] - y_pred[:,3:4]) / ((y_pred[:,2:3] - y_pred[:,4:5])**2 + (y_pred[:,3:4] - y_pred[:,5:6])**2)**(3/2)) ** 2)\n",
    "\n",
    "        loss_pinn = loss_pinn0 + loss_pinn1 + loss_pinn2 + loss_pinn3 + loss_pinn4 + loss_pinn5 + loss_pinn6 + loss_pinn7 + loss_pinn8 + loss_pinn9 + loss_pinn10 + loss_pinn11\n",
    "        \n",
    "        # The general semigroup loss for autonomous ODEs\n",
    "        y_pred_tps = self.net_y(self.s_semigroup + self.t_semigroup, self.y_semigroup)\n",
    "        y_pred_s = self.net_y(self.s_semigroup, self.y_semigroup)\n",
    "        y_pred_restart = self.net_y(self.t_semigroup, y_pred_s)\n",
    "        loss_semigroup = torch.mean((y_pred_tps - y_pred_restart) ** 2)\n",
    "        \n",
    "        # The general smoothness loss\n",
    "        y_pred_smooth = self.net_y(self.t_smooth, self.y_smooth)\n",
    "        deriv_pred_below = self.net_derivative(self.t_smooth, self.y_smooth)\n",
    "        deriv_pred_above = self.net_derivative(torch.zeros_like(self.t_smooth, requires_grad=True), y_pred_smooth)\n",
    "        \n",
    "        loss_smooth = .0\n",
    "        \n",
    "        for t1, t2 in zip(deriv_pred_below, deriv_pred_above):\n",
    "            \n",
    "            loss_smooth += torch.mean((t1 - t2) ** 2)\n",
    "        \n",
    "        loss = loss_pinn + loss_smooth + loss_semigroup\n",
    "        \n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "        \n",
    "        if self.iter % 10 == 0:\n",
    "            print(\n",
    "                f\"Iter {self.iter}, Loss: {loss.item():.5f}, Loss_pinn: {loss_pinn.item():.5f} \" \\\n",
    "                f\"Loss_smooth: {loss_smooth.item():.5f}, Loss_semigroup: {loss_semigroup.item():.5f}\"\n",
    "            )\n",
    "        \n",
    "        if self.iter % 100 == 0:\n",
    "            \n",
    "            with open(f\"./model_iter{self.iter}.pkl\", \"wb\") as handle:\n",
    "                pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        self.dnn.train()\n",
    "        self.optimizer.step(self.loss_function)\n",
    "    \n",
    "    \n",
    "    def predict(self, t, y0):\n",
    "        \n",
    "        t = torch.tensor(t, requires_grad=True).float().to(device)\n",
    "        y0 = torch.tensor(y0, requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.dnn.eval()\n",
    "        y = self.net_y(t, y0)\n",
    "        y = y.detach().cpu().numpy()\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68934f41",
   "metadata": {
    "id": "68934f41"
   },
   "source": [
    "### Setup Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b5ae371e",
   "metadata": {
    "id": "b5ae371e"
   },
   "outputs": [],
   "source": [
    "layers = [13] + 10 * [64] + [12]\n",
    "\n",
    "\n",
    "T = 2\n",
    "max_r = 2.\n",
    "max_v = 0.25\n",
    "\n",
    "# standard PINN loss function training samples\n",
    "N_pinn = 10000\n",
    "N_semigroup = 10000\n",
    "N_smooth = 10000\n",
    "\n",
    "\n",
    "def sample_y(max_r, max_v, N):\n",
    "\n",
    "    r = np.random.uniform(-max_r, max_r, (N, 6))\n",
    "    v = np.random.uniform(-max_v, max_v, (N, 6))\n",
    "    \n",
    "    return np.hstack([r, v])\n",
    "\n",
    "\n",
    "t_pinn = np.random.uniform(0, T, (N_pinn, 1))\n",
    "y_pinn = sample_y(max_r, max_v, N_pinn)\n",
    "X_pinn = np.hstack([t_pinn, y_pinn])\n",
    "\n",
    "\n",
    "# uniformly sample s, t with s+t \\leq T\n",
    "r1 = np.random.uniform(0, 1, N_semigroup)\n",
    "r2 = np.random.uniform(0, 1, N_semigroup)\n",
    "s_semigroup, t_semigroup = np.sqrt(r1) * (1 - r2), r2 * np.sqrt(r1)\n",
    "s_semigroup, t_semigroup = T * s_semigroup[:, np.newaxis], T * t_semigroup[:, np.newaxis]\n",
    "y_semigroup = sample_y(max_r, max_v, N_semigroup)\n",
    "X_semigroup = np.hstack([s_semigroup, t_semigroup, y_semigroup])\n",
    "\n",
    "\n",
    "t_smooth = np.random.uniform(0, T, (N_smooth, 1))\n",
    "y_smooth = sample_y(max_r, max_v, N_smooth)\n",
    "X_smooth = np.hstack([t_smooth, y_smooth])\n",
    "\n",
    "\n",
    "with open(\"./X_pinn_naive.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(X_pinn, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"./X_semigroup_naive.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(X_semigroup, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"./X_smooth_naive.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(X_smooth, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5e50f9b7",
   "metadata": {
    "id": "5e50f9b7"
   },
   "outputs": [],
   "source": [
    "model = TcPINN(X_pinn, X_semigroup, X_smooth, layers, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eeb83529",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eeb83529",
    "outputId": "6f402ad4-ec52-4952-e62f-8047096de27f",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10, Loss: 9436.41406, Loss_pinn: 9436.41406 Loss_smooth: 0.00001, Loss_semigroup: 0.00000\n",
      "Iter 20, Loss: 8380.43164, Loss_pinn: 8380.43164 Loss_smooth: 0.00001, Loss_semigroup: 0.00000\n",
      "Iter 30, Loss: 3440.80420, Loss_pinn: 3440.80420 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 40, Loss: 2640.09766, Loss_pinn: 2640.09766 Loss_smooth: 0.00006, Loss_semigroup: 0.00000\n",
      "Iter 50, Loss: 1950.46606, Loss_pinn: 1950.46594 Loss_smooth: 0.00009, Loss_semigroup: 0.00000\n",
      "Iter 60, Loss: 1453.66809, Loss_pinn: 1453.66797 Loss_smooth: 0.00012, Loss_semigroup: 0.00000\n",
      "Iter 70, Loss: 1027.11914, Loss_pinn: 1027.11890 Loss_smooth: 0.00026, Loss_semigroup: 0.00000\n",
      "Iter 80, Loss: 812.82050, Loss_pinn: 812.82007 Loss_smooth: 0.00042, Loss_semigroup: 0.00001\n",
      "Iter 90, Loss: 672.88214, Loss_pinn: 672.88116 Loss_smooth: 0.00097, Loss_semigroup: 0.00001\n",
      "Iter 100, Loss: 530.15588, Loss_pinn: 530.15082 Loss_smooth: 0.00499, Loss_semigroup: 0.00006\n",
      "Iter 110, Loss: 413.55954, Loss_pinn: 413.54645 Loss_smooth: 0.01293, Loss_semigroup: 0.00016\n",
      "Iter 120, Loss: 385.39758, Loss_pinn: 385.38605 Loss_smooth: 0.01139, Loss_semigroup: 0.00014\n",
      "Iter 130, Loss: 354.11853, Loss_pinn: 354.10248 Loss_smooth: 0.01584, Loss_semigroup: 0.00020\n",
      "Iter 140, Loss: 327.72415, Loss_pinn: 327.69357 Loss_smooth: 0.03020, Loss_semigroup: 0.00040\n",
      "Iter 150, Loss: 303.33188, Loss_pinn: 303.21323 Loss_smooth: 0.11709, Loss_semigroup: 0.00154\n",
      "Iter 160, Loss: 285.39349, Loss_pinn: 285.25171 Loss_smooth: 0.13994, Loss_semigroup: 0.00187\n",
      "Iter 170, Loss: 258.59763, Loss_pinn: 258.43027 Loss_smooth: 0.16509, Loss_semigroup: 0.00227\n",
      "Iter 180, Loss: 231.74522, Loss_pinn: 231.61588 Loss_smooth: 0.12753, Loss_semigroup: 0.00181\n",
      "Iter 190, Loss: 208.32939, Loss_pinn: 208.19687 Loss_smooth: 0.13060, Loss_semigroup: 0.00193\n",
      "Iter 200, Loss: 192.98457, Loss_pinn: 192.82124 Loss_smooth: 0.16086, Loss_semigroup: 0.00246\n",
      "Iter 210, Loss: 158.99174, Loss_pinn: 158.73395 Loss_smooth: 0.25361, Loss_semigroup: 0.00420\n",
      "Iter 220, Loss: 129.20953, Loss_pinn: 128.77312 Loss_smooth: 0.42821, Loss_semigroup: 0.00821\n",
      "Iter 230, Loss: 106.94463, Loss_pinn: 106.27535 Loss_smooth: 0.65520, Loss_semigroup: 0.01407\n",
      "Iter 240, Loss: 85.22533, Loss_pinn: 84.17851 Loss_smooth: 1.02172, Loss_semigroup: 0.02510\n",
      "Iter 250, Loss: 67.25806, Loss_pinn: 65.88683 Loss_smooth: 1.33448, Loss_semigroup: 0.03676\n",
      "Iter 260, Loss: 50.49552, Loss_pinn: 48.34888 Loss_smooth: 2.08323, Loss_semigroup: 0.06341\n",
      "Iter 270, Loss: 42.67751, Loss_pinn: 40.00999 Loss_smooth: 2.58569, Loss_semigroup: 0.08183\n",
      "Iter 280, Loss: 37.07045, Loss_pinn: 34.85763 Loss_smooth: 2.14586, Loss_semigroup: 0.06696\n",
      "Iter 290, Loss: 32.43156, Loss_pinn: 29.92479 Loss_smooth: 2.42479, Loss_semigroup: 0.08198\n",
      "Iter 300, Loss: 29.40503, Loss_pinn: 27.03976 Loss_smooth: 2.28643, Loss_semigroup: 0.07884\n",
      "Iter 310, Loss: 26.17587, Loss_pinn: 24.20461 Loss_smooth: 1.90522, Loss_semigroup: 0.06604\n",
      "Iter 320, Loss: 23.49658, Loss_pinn: 21.64145 Loss_smooth: 1.79156, Loss_semigroup: 0.06357\n",
      "Iter 330, Loss: 21.70627, Loss_pinn: 20.15164 Loss_smooth: 1.50186, Loss_semigroup: 0.05277\n",
      "Iter 340, Loss: 19.44091, Loss_pinn: 17.92840 Loss_smooth: 1.46259, Loss_semigroup: 0.04993\n",
      "Iter 350, Loss: 18.32375, Loss_pinn: 17.02680 Loss_smooth: 1.25385, Loss_semigroup: 0.04310\n",
      "Iter 360, Loss: 17.05798, Loss_pinn: 15.92363 Loss_smooth: 1.09459, Loss_semigroup: 0.03976\n",
      "Iter 370, Loss: 15.89373, Loss_pinn: 14.99570 Loss_smooth: 0.86387, Loss_semigroup: 0.03417\n",
      "Iter 380, Loss: 14.82958, Loss_pinn: 14.04800 Loss_smooth: 0.75454, Loss_semigroup: 0.02703\n",
      "Iter 390, Loss: 13.48408, Loss_pinn: 12.73203 Loss_smooth: 0.72483, Loss_semigroup: 0.02723\n",
      "Iter 400, Loss: 12.53362, Loss_pinn: 11.76154 Loss_smooth: 0.74283, Loss_semigroup: 0.02925\n",
      "Iter 410, Loss: 11.51135, Loss_pinn: 10.84803 Loss_smooth: 0.63819, Loss_semigroup: 0.02514\n",
      "Iter 420, Loss: 10.91197, Loss_pinn: 10.31328 Loss_smooth: 0.57510, Loss_semigroup: 0.02359\n",
      "Iter 430, Loss: 10.04197, Loss_pinn: 9.38795 Loss_smooth: 0.63194, Loss_semigroup: 0.02209\n",
      "Iter 440, Loss: 9.41829, Loss_pinn: 8.83654 Loss_smooth: 0.56289, Loss_semigroup: 0.01886\n",
      "Iter 450, Loss: 8.85633, Loss_pinn: 8.27753 Loss_smooth: 0.55993, Loss_semigroup: 0.01886\n",
      "Iter 460, Loss: 8.23603, Loss_pinn: 7.68731 Loss_smooth: 0.53139, Loss_semigroup: 0.01733\n",
      "Iter 470, Loss: 7.89226, Loss_pinn: 7.39912 Loss_smooth: 0.47786, Loss_semigroup: 0.01528\n",
      "Iter 480, Loss: 7.41992, Loss_pinn: 6.96466 Loss_smooth: 0.44086, Loss_semigroup: 0.01439\n",
      "Iter 490, Loss: 6.95220, Loss_pinn: 6.55246 Loss_smooth: 0.38691, Loss_semigroup: 0.01283\n",
      "Iter 500, Loss: 6.57812, Loss_pinn: 6.20043 Loss_smooth: 0.36562, Loss_semigroup: 0.01206\n",
      "Iter 510, Loss: 6.23734, Loss_pinn: 5.86160 Loss_smooth: 0.36429, Loss_semigroup: 0.01145\n",
      "Iter 520, Loss: 5.98427, Loss_pinn: 5.56410 Loss_smooth: 0.40798, Loss_semigroup: 0.01219\n",
      "Iter 530, Loss: 5.75686, Loss_pinn: 5.36121 Loss_smooth: 0.38497, Loss_semigroup: 0.01068\n",
      "Iter 540, Loss: 5.58554, Loss_pinn: 5.22574 Loss_smooth: 0.34935, Loss_semigroup: 0.01045\n",
      "Iter 550, Loss: 5.32821, Loss_pinn: 4.99757 Loss_smooth: 0.32074, Loss_semigroup: 0.00990\n",
      "Iter 560, Loss: 5.15923, Loss_pinn: 4.85051 Loss_smooth: 0.29938, Loss_semigroup: 0.00935\n",
      "Iter 570, Loss: 4.93354, Loss_pinn: 4.62309 Loss_smooth: 0.30122, Loss_semigroup: 0.00923\n",
      "Iter 580, Loss: 4.70538, Loss_pinn: 4.38694 Loss_smooth: 0.30882, Loss_semigroup: 0.00962\n",
      "Iter 590, Loss: 4.57264, Loss_pinn: 4.25230 Loss_smooth: 0.31112, Loss_semigroup: 0.00923\n",
      "Iter 600, Loss: 4.34988, Loss_pinn: 4.03705 Loss_smooth: 0.30326, Loss_semigroup: 0.00957\n",
      "Iter 610, Loss: 4.16923, Loss_pinn: 3.83723 Loss_smooth: 0.32241, Loss_semigroup: 0.00959\n",
      "Iter 620, Loss: 3.99522, Loss_pinn: 3.68176 Loss_smooth: 0.30383, Loss_semigroup: 0.00962\n",
      "Iter 630, Loss: 3.82563, Loss_pinn: 3.53636 Loss_smooth: 0.28035, Loss_semigroup: 0.00891\n",
      "Iter 640, Loss: 3.69443, Loss_pinn: 3.42156 Loss_smooth: 0.26433, Loss_semigroup: 0.00854\n",
      "Iter 650, Loss: 3.58784, Loss_pinn: 3.32952 Loss_smooth: 0.25006, Loss_semigroup: 0.00826\n",
      "Iter 660, Loss: 3.50697, Loss_pinn: 3.25510 Loss_smooth: 0.24382, Loss_semigroup: 0.00804\n",
      "Iter 670, Loss: 3.41882, Loss_pinn: 3.18218 Loss_smooth: 0.22890, Loss_semigroup: 0.00773\n",
      "Iter 680, Loss: 3.30150, Loss_pinn: 3.06495 Loss_smooth: 0.22968, Loss_semigroup: 0.00687\n",
      "Iter 690, Loss: 3.22870, Loss_pinn: 3.00243 Loss_smooth: 0.21953, Loss_semigroup: 0.00674\n",
      "Iter 700, Loss: 3.17830, Loss_pinn: 2.95552 Loss_smooth: 0.21606, Loss_semigroup: 0.00671\n",
      "Iter 710, Loss: 3.09313, Loss_pinn: 2.87359 Loss_smooth: 0.21304, Loss_semigroup: 0.00650\n",
      "Iter 720, Loss: 3.03543, Loss_pinn: 2.81260 Loss_smooth: 0.21642, Loss_semigroup: 0.00641\n",
      "Iter 730, Loss: 2.97562, Loss_pinn: 2.76951 Loss_smooth: 0.20006, Loss_semigroup: 0.00605\n",
      "Iter 740, Loss: 2.90768, Loss_pinn: 2.70239 Loss_smooth: 0.19913, Loss_semigroup: 0.00616\n",
      "Iter 750, Loss: 2.83752, Loss_pinn: 2.63614 Loss_smooth: 0.19559, Loss_semigroup: 0.00579\n",
      "Iter 760, Loss: 2.77926, Loss_pinn: 2.57494 Loss_smooth: 0.19882, Loss_semigroup: 0.00549\n",
      "Iter 770, Loss: 2.72422, Loss_pinn: 2.53602 Loss_smooth: 0.18285, Loss_semigroup: 0.00536\n",
      "Iter 780, Loss: 2.68017, Loss_pinn: 2.49421 Loss_smooth: 0.18079, Loss_semigroup: 0.00517\n",
      "Iter 790, Loss: 2.63750, Loss_pinn: 2.45536 Loss_smooth: 0.17707, Loss_semigroup: 0.00507\n",
      "Iter 800, Loss: 2.59359, Loss_pinn: 2.41742 Loss_smooth: 0.17134, Loss_semigroup: 0.00483\n",
      "Iter 810, Loss: 2.54796, Loss_pinn: 2.37990 Loss_smooth: 0.16342, Loss_semigroup: 0.00464\n",
      "Iter 820, Loss: 2.50539, Loss_pinn: 2.34206 Loss_smooth: 0.15876, Loss_semigroup: 0.00456\n",
      "Iter 830, Loss: 2.47143, Loss_pinn: 2.30560 Loss_smooth: 0.16135, Loss_semigroup: 0.00449\n",
      "Iter 840, Loss: 2.44873, Loss_pinn: 2.28033 Loss_smooth: 0.16388, Loss_semigroup: 0.00452\n",
      "Iter 850, Loss: 2.41125, Loss_pinn: 2.24509 Loss_smooth: 0.16153, Loss_semigroup: 0.00463\n",
      "Iter 860, Loss: 2.39559, Loss_pinn: 2.23141 Loss_smooth: 0.15956, Loss_semigroup: 0.00463\n",
      "Iter 870, Loss: 2.36564, Loss_pinn: 2.20841 Loss_smooth: 0.15279, Loss_semigroup: 0.00444\n",
      "Iter 880, Loss: 2.33121, Loss_pinn: 2.17564 Loss_smooth: 0.15129, Loss_semigroup: 0.00428\n",
      "Iter 890, Loss: 2.29907, Loss_pinn: 2.14520 Loss_smooth: 0.14952, Loss_semigroup: 0.00435\n",
      "Iter 900, Loss: 2.26486, Loss_pinn: 2.11530 Loss_smooth: 0.14530, Loss_semigroup: 0.00426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 910, Loss: 2.24990, Loss_pinn: 2.10371 Loss_smooth: 0.14201, Loss_semigroup: 0.00419\n",
      "Iter 920, Loss: 2.23374, Loss_pinn: 2.08447 Loss_smooth: 0.14518, Loss_semigroup: 0.00409\n",
      "Iter 930, Loss: 2.20799, Loss_pinn: 2.06762 Loss_smooth: 0.13630, Loss_semigroup: 0.00406\n",
      "Iter 940, Loss: 2.18804, Loss_pinn: 2.04557 Loss_smooth: 0.13830, Loss_semigroup: 0.00418\n",
      "Iter 950, Loss: 2.16486, Loss_pinn: 2.02391 Loss_smooth: 0.13690, Loss_semigroup: 0.00405\n",
      "Iter 960, Loss: 2.14405, Loss_pinn: 2.00420 Loss_smooth: 0.13582, Loss_semigroup: 0.00403\n",
      "Iter 970, Loss: 2.12817, Loss_pinn: 1.99320 Loss_smooth: 0.13102, Loss_semigroup: 0.00396\n",
      "Iter 980, Loss: 2.11092, Loss_pinn: 1.97756 Loss_smooth: 0.12949, Loss_semigroup: 0.00387\n",
      "Iter 990, Loss: 2.08783, Loss_pinn: 1.95575 Loss_smooth: 0.12813, Loss_semigroup: 0.00395\n",
      "Iter 1000, Loss: 2.07039, Loss_pinn: 1.93335 Loss_smooth: 0.13315, Loss_semigroup: 0.00388\n",
      "Iter 1010, Loss: 2.05360, Loss_pinn: 1.92139 Loss_smooth: 0.12848, Loss_semigroup: 0.00373\n",
      "Iter 1020, Loss: 2.04115, Loss_pinn: 1.91217 Loss_smooth: 0.12521, Loss_semigroup: 0.00377\n",
      "Iter 1030, Loss: 2.03137, Loss_pinn: 1.90125 Loss_smooth: 0.12636, Loss_semigroup: 0.00376\n",
      "Iter 1040, Loss: 2.01745, Loss_pinn: 1.88785 Loss_smooth: 0.12588, Loss_semigroup: 0.00372\n",
      "Iter 1050, Loss: 2.00226, Loss_pinn: 1.87134 Loss_smooth: 0.12727, Loss_semigroup: 0.00365\n",
      "Iter 1060, Loss: 1.99127, Loss_pinn: 1.86270 Loss_smooth: 0.12488, Loss_semigroup: 0.00369\n",
      "Iter 1070, Loss: 1.97644, Loss_pinn: 1.85210 Loss_smooth: 0.12069, Loss_semigroup: 0.00365\n",
      "Iter 1080, Loss: 1.96409, Loss_pinn: 1.83882 Loss_smooth: 0.12170, Loss_semigroup: 0.00357\n",
      "Iter 1090, Loss: 1.95465, Loss_pinn: 1.83131 Loss_smooth: 0.11980, Loss_semigroup: 0.00355\n",
      "Iter 1100, Loss: 1.94528, Loss_pinn: 1.82046 Loss_smooth: 0.12131, Loss_semigroup: 0.00351\n",
      "Iter 1110, Loss: 1.93391, Loss_pinn: 1.81306 Loss_smooth: 0.11741, Loss_semigroup: 0.00344\n",
      "Iter 1120, Loss: 1.92534, Loss_pinn: 1.80594 Loss_smooth: 0.11600, Loss_semigroup: 0.00341\n",
      "Iter 1130, Loss: 1.91612, Loss_pinn: 1.79633 Loss_smooth: 0.11631, Loss_semigroup: 0.00348\n",
      "Iter 1140, Loss: 1.90377, Loss_pinn: 1.78123 Loss_smooth: 0.11888, Loss_semigroup: 0.00365\n",
      "Iter 1150, Loss: 1.89665, Loss_pinn: 1.77550 Loss_smooth: 0.11755, Loss_semigroup: 0.00360\n",
      "Iter 1160, Loss: 1.88662, Loss_pinn: 1.76913 Loss_smooth: 0.11386, Loss_semigroup: 0.00363\n",
      "Iter 1170, Loss: 1.87732, Loss_pinn: 1.76268 Loss_smooth: 0.11105, Loss_semigroup: 0.00359\n",
      "Iter 1180, Loss: 1.86886, Loss_pinn: 1.75811 Loss_smooth: 0.10719, Loss_semigroup: 0.00356\n",
      "Iter 1190, Loss: 1.86129, Loss_pinn: 1.75175 Loss_smooth: 0.10602, Loss_semigroup: 0.00352\n",
      "Iter 1200, Loss: 1.85066, Loss_pinn: 1.74118 Loss_smooth: 0.10599, Loss_semigroup: 0.00349\n",
      "Iter 1210, Loss: 1.84113, Loss_pinn: 1.73320 Loss_smooth: 0.10445, Loss_semigroup: 0.00347\n",
      "Iter 1220, Loss: 1.83189, Loss_pinn: 1.72393 Loss_smooth: 0.10464, Loss_semigroup: 0.00332\n",
      "Iter 1230, Loss: 1.82738, Loss_pinn: 1.71396 Loss_smooth: 0.11018, Loss_semigroup: 0.00324\n",
      "Iter 1240, Loss: 1.81917, Loss_pinn: 1.71246 Loss_smooth: 0.10343, Loss_semigroup: 0.00329\n",
      "Iter 1250, Loss: 1.81308, Loss_pinn: 1.70435 Loss_smooth: 0.10550, Loss_semigroup: 0.00323\n",
      "Iter 1260, Loss: 1.80896, Loss_pinn: 1.70049 Loss_smooth: 0.10528, Loss_semigroup: 0.00320\n",
      "Iter 1270, Loss: 1.79856, Loss_pinn: 1.68781 Loss_smooth: 0.10752, Loss_semigroup: 0.00323\n",
      "Iter 1280, Loss: 1.79527, Loss_pinn: 1.68428 Loss_smooth: 0.10780, Loss_semigroup: 0.00319\n",
      "Iter 1290, Loss: 1.79134, Loss_pinn: 1.68270 Loss_smooth: 0.10549, Loss_semigroup: 0.00315\n",
      "Iter 1300, Loss: 1.78452, Loss_pinn: 1.67326 Loss_smooth: 0.10807, Loss_semigroup: 0.00319\n",
      "Iter 1310, Loss: 1.78061, Loss_pinn: 1.67096 Loss_smooth: 0.10648, Loss_semigroup: 0.00316\n",
      "Iter 1320, Loss: 1.77292, Loss_pinn: 1.66523 Loss_smooth: 0.10456, Loss_semigroup: 0.00313\n",
      "Iter 1330, Loss: 1.76667, Loss_pinn: 1.65943 Loss_smooth: 0.10414, Loss_semigroup: 0.00311\n",
      "Iter 1340, Loss: 1.75934, Loss_pinn: 1.65292 Loss_smooth: 0.10329, Loss_semigroup: 0.00313\n",
      "Iter 1350, Loss: 1.75060, Loss_pinn: 1.64226 Loss_smooth: 0.10521, Loss_semigroup: 0.00313\n",
      "Iter 1360, Loss: 1.74537, Loss_pinn: 1.63709 Loss_smooth: 0.10512, Loss_semigroup: 0.00317\n",
      "Iter 1370, Loss: 1.73838, Loss_pinn: 1.63093 Loss_smooth: 0.10429, Loss_semigroup: 0.00316\n",
      "Iter 1380, Loss: 1.73276, Loss_pinn: 1.62525 Loss_smooth: 0.10432, Loss_semigroup: 0.00319\n",
      "Iter 1390, Loss: 1.72849, Loss_pinn: 1.62139 Loss_smooth: 0.10395, Loss_semigroup: 0.00316\n",
      "Iter 1400, Loss: 1.72148, Loss_pinn: 1.61627 Loss_smooth: 0.10215, Loss_semigroup: 0.00306\n",
      "Iter 1410, Loss: 1.71539, Loss_pinn: 1.61141 Loss_smooth: 0.10097, Loss_semigroup: 0.00301\n",
      "Iter 1420, Loss: 1.70958, Loss_pinn: 1.60605 Loss_smooth: 0.10051, Loss_semigroup: 0.00302\n",
      "Iter 1430, Loss: 1.70373, Loss_pinn: 1.60093 Loss_smooth: 0.09975, Loss_semigroup: 0.00305\n",
      "Iter 1440, Loss: 1.69801, Loss_pinn: 1.59719 Loss_smooth: 0.09775, Loss_semigroup: 0.00307\n",
      "Iter 1450, Loss: 1.69177, Loss_pinn: 1.58831 Loss_smooth: 0.10031, Loss_semigroup: 0.00315\n",
      "Iter 1460, Loss: 1.68639, Loss_pinn: 1.58304 Loss_smooth: 0.10019, Loss_semigroup: 0.00317\n",
      "Iter 1470, Loss: 1.68060, Loss_pinn: 1.57639 Loss_smooth: 0.10101, Loss_semigroup: 0.00319\n",
      "Iter 1480, Loss: 1.67486, Loss_pinn: 1.56997 Loss_smooth: 0.10172, Loss_semigroup: 0.00317\n",
      "Iter 1490, Loss: 1.66983, Loss_pinn: 1.56731 Loss_smooth: 0.09933, Loss_semigroup: 0.00319\n",
      "Iter 1500, Loss: 1.66522, Loss_pinn: 1.56516 Loss_smooth: 0.09695, Loss_semigroup: 0.00310\n",
      "Iter 1510, Loss: 1.65820, Loss_pinn: 1.55592 Loss_smooth: 0.09911, Loss_semigroup: 0.00316\n",
      "Iter 1520, Loss: 1.65088, Loss_pinn: 1.54971 Loss_smooth: 0.09802, Loss_semigroup: 0.00315\n",
      "Iter 1530, Loss: 1.64535, Loss_pinn: 1.54538 Loss_smooth: 0.09688, Loss_semigroup: 0.00309\n",
      "Iter 1540, Loss: 1.64215, Loss_pinn: 1.54356 Loss_smooth: 0.09551, Loss_semigroup: 0.00308\n",
      "Iter 1550, Loss: 1.63818, Loss_pinn: 1.53952 Loss_smooth: 0.09563, Loss_semigroup: 0.00303\n",
      "Iter 1560, Loss: 1.63379, Loss_pinn: 1.53626 Loss_smooth: 0.09445, Loss_semigroup: 0.00308\n",
      "Iter 1570, Loss: 1.62827, Loss_pinn: 1.53335 Loss_smooth: 0.09184, Loss_semigroup: 0.00308\n",
      "Iter 1580, Loss: 1.62422, Loss_pinn: 1.52895 Loss_smooth: 0.09223, Loss_semigroup: 0.00305\n",
      "Iter 1590, Loss: 1.61729, Loss_pinn: 1.52185 Loss_smooth: 0.09236, Loss_semigroup: 0.00308\n",
      "Iter 1600, Loss: 1.61243, Loss_pinn: 1.51602 Loss_smooth: 0.09338, Loss_semigroup: 0.00303\n",
      "Iter 1610, Loss: 1.60724, Loss_pinn: 1.51250 Loss_smooth: 0.09173, Loss_semigroup: 0.00301\n",
      "Iter 1620, Loss: 1.60295, Loss_pinn: 1.50988 Loss_smooth: 0.09007, Loss_semigroup: 0.00301\n",
      "Iter 1630, Loss: 1.59849, Loss_pinn: 1.50685 Loss_smooth: 0.08865, Loss_semigroup: 0.00299\n",
      "Iter 1640, Loss: 1.59432, Loss_pinn: 1.50208 Loss_smooth: 0.08926, Loss_semigroup: 0.00298\n",
      "Iter 1650, Loss: 1.58998, Loss_pinn: 1.49731 Loss_smooth: 0.08965, Loss_semigroup: 0.00302\n",
      "Iter 1660, Loss: 1.58465, Loss_pinn: 1.49193 Loss_smooth: 0.08962, Loss_semigroup: 0.00310\n",
      "Iter 1670, Loss: 1.58144, Loss_pinn: 1.48879 Loss_smooth: 0.08961, Loss_semigroup: 0.00304\n",
      "Iter 1680, Loss: 1.57828, Loss_pinn: 1.48611 Loss_smooth: 0.08916, Loss_semigroup: 0.00301\n",
      "Iter 1690, Loss: 1.57378, Loss_pinn: 1.48146 Loss_smooth: 0.08930, Loss_semigroup: 0.00302\n",
      "Iter 1700, Loss: 1.56966, Loss_pinn: 1.47765 Loss_smooth: 0.08900, Loss_semigroup: 0.00301\n",
      "Iter 1710, Loss: 1.56572, Loss_pinn: 1.47457 Loss_smooth: 0.08815, Loss_semigroup: 0.00300\n",
      "Iter 1720, Loss: 1.56123, Loss_pinn: 1.47014 Loss_smooth: 0.08809, Loss_semigroup: 0.00301\n",
      "Iter 1730, Loss: 1.55700, Loss_pinn: 1.46674 Loss_smooth: 0.08725, Loss_semigroup: 0.00301\n",
      "Iter 1740, Loss: 1.55332, Loss_pinn: 1.46324 Loss_smooth: 0.08707, Loss_semigroup: 0.00302\n",
      "Iter 1750, Loss: 1.54928, Loss_pinn: 1.45907 Loss_smooth: 0.08723, Loss_semigroup: 0.00297\n",
      "Iter 1760, Loss: 1.54525, Loss_pinn: 1.45511 Loss_smooth: 0.08719, Loss_semigroup: 0.00295\n",
      "Iter 1770, Loss: 1.54088, Loss_pinn: 1.45097 Loss_smooth: 0.08696, Loss_semigroup: 0.00295\n",
      "Iter 1780, Loss: 1.53616, Loss_pinn: 1.44787 Loss_smooth: 0.08539, Loss_semigroup: 0.00290\n",
      "Iter 1790, Loss: 1.53314, Loss_pinn: 1.44561 Loss_smooth: 0.08460, Loss_semigroup: 0.00293\n",
      "Iter 1800, Loss: 1.52944, Loss_pinn: 1.44095 Loss_smooth: 0.08554, Loss_semigroup: 0.00295\n",
      "Iter 1810, Loss: 1.52586, Loss_pinn: 1.43689 Loss_smooth: 0.08602, Loss_semigroup: 0.00296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1820, Loss: 1.52310, Loss_pinn: 1.43440 Loss_smooth: 0.08569, Loss_semigroup: 0.00300\n",
      "Iter 1830, Loss: 1.52029, Loss_pinn: 1.43119 Loss_smooth: 0.08612, Loss_semigroup: 0.00298\n",
      "Iter 1840, Loss: 1.51702, Loss_pinn: 1.42858 Loss_smooth: 0.08546, Loss_semigroup: 0.00298\n",
      "Iter 1850, Loss: 1.51301, Loss_pinn: 1.42568 Loss_smooth: 0.08439, Loss_semigroup: 0.00294\n",
      "Iter 1860, Loss: 1.50828, Loss_pinn: 1.42120 Loss_smooth: 0.08417, Loss_semigroup: 0.00291\n",
      "Iter 1870, Loss: 1.50522, Loss_pinn: 1.41834 Loss_smooth: 0.08401, Loss_semigroup: 0.00288\n",
      "Iter 1880, Loss: 1.50144, Loss_pinn: 1.41630 Loss_smooth: 0.08224, Loss_semigroup: 0.00289\n",
      "Iter 1890, Loss: 1.49823, Loss_pinn: 1.41277 Loss_smooth: 0.08257, Loss_semigroup: 0.00289\n",
      "Iter 1900, Loss: 1.49567, Loss_pinn: 1.41071 Loss_smooth: 0.08206, Loss_semigroup: 0.00290\n",
      "Iter 1910, Loss: 1.49312, Loss_pinn: 1.40796 Loss_smooth: 0.08228, Loss_semigroup: 0.00289\n",
      "Iter 1920, Loss: 1.49317, Loss_pinn: 1.40830 Loss_smooth: 0.08196, Loss_semigroup: 0.00291\n",
      "Iter 1930, Loss: 1.48717, Loss_pinn: 1.40178 Loss_smooth: 0.08246, Loss_semigroup: 0.00292\n",
      "Iter 1940, Loss: 1.48319, Loss_pinn: 1.39766 Loss_smooth: 0.08256, Loss_semigroup: 0.00298\n",
      "Iter 1950, Loss: 1.47942, Loss_pinn: 1.39389 Loss_smooth: 0.08253, Loss_semigroup: 0.00300\n",
      "Iter 1960, Loss: 1.47526, Loss_pinn: 1.38901 Loss_smooth: 0.08326, Loss_semigroup: 0.00299\n",
      "Iter 1970, Loss: 1.47114, Loss_pinn: 1.38579 Loss_smooth: 0.08236, Loss_semigroup: 0.00299\n",
      "Iter 1980, Loss: 1.46898, Loss_pinn: 1.38354 Loss_smooth: 0.08238, Loss_semigroup: 0.00306\n",
      "Iter 1990, Loss: 1.46599, Loss_pinn: 1.38059 Loss_smooth: 0.08238, Loss_semigroup: 0.00303\n",
      "Iter 2000, Loss: 1.46175, Loss_pinn: 1.37557 Loss_smooth: 0.08322, Loss_semigroup: 0.00296\n",
      "Iter 2010, Loss: 1.45902, Loss_pinn: 1.37425 Loss_smooth: 0.08181, Loss_semigroup: 0.00296\n",
      "Iter 2020, Loss: 1.45531, Loss_pinn: 1.37000 Loss_smooth: 0.08234, Loss_semigroup: 0.00298\n",
      "Iter 2030, Loss: 1.45094, Loss_pinn: 1.36602 Loss_smooth: 0.08196, Loss_semigroup: 0.00296\n",
      "Iter 2040, Loss: 1.44799, Loss_pinn: 1.36184 Loss_smooth: 0.08316, Loss_semigroup: 0.00299\n",
      "Iter 2050, Loss: 1.44511, Loss_pinn: 1.35887 Loss_smooth: 0.08315, Loss_semigroup: 0.00309\n",
      "Iter 2060, Loss: 1.44220, Loss_pinn: 1.35569 Loss_smooth: 0.08351, Loss_semigroup: 0.00300\n",
      "Iter 2070, Loss: 1.43887, Loss_pinn: 1.35234 Loss_smooth: 0.08361, Loss_semigroup: 0.00292\n",
      "Iter 2080, Loss: 1.43520, Loss_pinn: 1.34794 Loss_smooth: 0.08434, Loss_semigroup: 0.00293\n",
      "Iter 2090, Loss: 1.43294, Loss_pinn: 1.34649 Loss_smooth: 0.08348, Loss_semigroup: 0.00297\n",
      "Iter 2100, Loss: 1.42941, Loss_pinn: 1.34370 Loss_smooth: 0.08279, Loss_semigroup: 0.00292\n",
      "Iter 2110, Loss: 1.42550, Loss_pinn: 1.33962 Loss_smooth: 0.08290, Loss_semigroup: 0.00298\n",
      "Iter 2120, Loss: 1.42177, Loss_pinn: 1.33627 Loss_smooth: 0.08255, Loss_semigroup: 0.00295\n",
      "Iter 2130, Loss: 1.41821, Loss_pinn: 1.33071 Loss_smooth: 0.08446, Loss_semigroup: 0.00305\n",
      "Iter 2140, Loss: 1.41522, Loss_pinn: 1.32748 Loss_smooth: 0.08469, Loss_semigroup: 0.00306\n",
      "Iter 2150, Loss: 1.41275, Loss_pinn: 1.32488 Loss_smooth: 0.08479, Loss_semigroup: 0.00309\n",
      "Iter 2160, Loss: 1.40996, Loss_pinn: 1.32227 Loss_smooth: 0.08460, Loss_semigroup: 0.00309\n",
      "Iter 2170, Loss: 1.40750, Loss_pinn: 1.31949 Loss_smooth: 0.08492, Loss_semigroup: 0.00309\n",
      "Iter 2180, Loss: 1.40486, Loss_pinn: 1.31714 Loss_smooth: 0.08462, Loss_semigroup: 0.00311\n",
      "Iter 2190, Loss: 1.40207, Loss_pinn: 1.31509 Loss_smooth: 0.08392, Loss_semigroup: 0.00306\n",
      "Iter 2200, Loss: 1.40254, Loss_pinn: 1.31191 Loss_smooth: 0.08755, Loss_semigroup: 0.00309\n",
      "Iter 2210, Loss: 1.39735, Loss_pinn: 1.31004 Loss_smooth: 0.08419, Loss_semigroup: 0.00313\n",
      "Iter 2220, Loss: 1.39474, Loss_pinn: 1.30648 Loss_smooth: 0.08518, Loss_semigroup: 0.00308\n",
      "Iter 2230, Loss: 1.39201, Loss_pinn: 1.30350 Loss_smooth: 0.08545, Loss_semigroup: 0.00307\n",
      "Iter 2240, Loss: 1.39004, Loss_pinn: 1.30134 Loss_smooth: 0.08568, Loss_semigroup: 0.00302\n",
      "Iter 2250, Loss: 1.38786, Loss_pinn: 1.29965 Loss_smooth: 0.08515, Loss_semigroup: 0.00306\n",
      "Iter 2260, Loss: 1.38624, Loss_pinn: 1.29739 Loss_smooth: 0.08578, Loss_semigroup: 0.00307\n",
      "Iter 2270, Loss: 1.38359, Loss_pinn: 1.29366 Loss_smooth: 0.08680, Loss_semigroup: 0.00314\n",
      "Iter 2280, Loss: 1.38165, Loss_pinn: 1.29140 Loss_smooth: 0.08711, Loss_semigroup: 0.00314\n",
      "Iter 2290, Loss: 1.37873, Loss_pinn: 1.28937 Loss_smooth: 0.08630, Loss_semigroup: 0.00306\n",
      "Iter 2300, Loss: 1.37644, Loss_pinn: 1.28664 Loss_smooth: 0.08676, Loss_semigroup: 0.00304\n",
      "Iter 2310, Loss: 1.37422, Loss_pinn: 1.28493 Loss_smooth: 0.08625, Loss_semigroup: 0.00304\n",
      "Iter 2320, Loss: 1.37235, Loss_pinn: 1.28314 Loss_smooth: 0.08616, Loss_semigroup: 0.00306\n",
      "Iter 2330, Loss: 1.37022, Loss_pinn: 1.28186 Loss_smooth: 0.08530, Loss_semigroup: 0.00306\n",
      "Iter 2340, Loss: 1.36743, Loss_pinn: 1.28015 Loss_smooth: 0.08425, Loss_semigroup: 0.00304\n",
      "Iter 2350, Loss: 1.36548, Loss_pinn: 1.27837 Loss_smooth: 0.08409, Loss_semigroup: 0.00302\n",
      "Iter 2360, Loss: 1.36347, Loss_pinn: 1.27589 Loss_smooth: 0.08455, Loss_semigroup: 0.00303\n",
      "Iter 2370, Loss: 1.36118, Loss_pinn: 1.27406 Loss_smooth: 0.08411, Loss_semigroup: 0.00301\n",
      "Iter 2380, Loss: 1.35843, Loss_pinn: 1.27161 Loss_smooth: 0.08380, Loss_semigroup: 0.00302\n",
      "Iter 2390, Loss: 1.35616, Loss_pinn: 1.26985 Loss_smooth: 0.08329, Loss_semigroup: 0.00301\n",
      "Iter 2400, Loss: 1.35321, Loss_pinn: 1.26581 Loss_smooth: 0.08437, Loss_semigroup: 0.00303\n",
      "Iter 2410, Loss: 1.35114, Loss_pinn: 1.26421 Loss_smooth: 0.08387, Loss_semigroup: 0.00306\n",
      "Iter 2420, Loss: 1.34947, Loss_pinn: 1.26265 Loss_smooth: 0.08375, Loss_semigroup: 0.00306\n",
      "Iter 2430, Loss: 1.34788, Loss_pinn: 1.25932 Loss_smooth: 0.08547, Loss_semigroup: 0.00309\n",
      "Iter 2440, Loss: 1.34543, Loss_pinn: 1.25760 Loss_smooth: 0.08471, Loss_semigroup: 0.00312\n",
      "Iter 2450, Loss: 1.34275, Loss_pinn: 1.25480 Loss_smooth: 0.08482, Loss_semigroup: 0.00313\n",
      "Iter 2460, Loss: 1.34016, Loss_pinn: 1.25198 Loss_smooth: 0.08500, Loss_semigroup: 0.00318\n",
      "Iter 2470, Loss: 1.33850, Loss_pinn: 1.25024 Loss_smooth: 0.08505, Loss_semigroup: 0.00320\n",
      "Iter 2480, Loss: 1.33655, Loss_pinn: 1.24849 Loss_smooth: 0.08488, Loss_semigroup: 0.00317\n",
      "Iter 2490, Loss: 1.33445, Loss_pinn: 1.24716 Loss_smooth: 0.08415, Loss_semigroup: 0.00314\n",
      "Iter 2500, Loss: 1.33262, Loss_pinn: 1.24553 Loss_smooth: 0.08398, Loss_semigroup: 0.00311\n",
      "Iter 2510, Loss: 1.33026, Loss_pinn: 1.24443 Loss_smooth: 0.08276, Loss_semigroup: 0.00307\n",
      "Iter 2520, Loss: 1.32794, Loss_pinn: 1.24315 Loss_smooth: 0.08177, Loss_semigroup: 0.00302\n",
      "Iter 2530, Loss: 1.32591, Loss_pinn: 1.24038 Loss_smooth: 0.08255, Loss_semigroup: 0.00299\n",
      "Iter 2540, Loss: 1.32441, Loss_pinn: 1.23871 Loss_smooth: 0.08268, Loss_semigroup: 0.00303\n",
      "Iter 2550, Loss: 1.32310, Loss_pinn: 1.23836 Loss_smooth: 0.08171, Loss_semigroup: 0.00302\n",
      "Iter 2560, Loss: 1.32148, Loss_pinn: 1.23637 Loss_smooth: 0.08207, Loss_semigroup: 0.00304\n",
      "Iter 2570, Loss: 1.31931, Loss_pinn: 1.23340 Loss_smooth: 0.08283, Loss_semigroup: 0.00307\n",
      "Iter 2580, Loss: 1.31806, Loss_pinn: 1.23330 Loss_smooth: 0.08168, Loss_semigroup: 0.00307\n",
      "Iter 2590, Loss: 1.31668, Loss_pinn: 1.23154 Loss_smooth: 0.08204, Loss_semigroup: 0.00310\n",
      "Iter 2600, Loss: 1.31468, Loss_pinn: 1.22909 Loss_smooth: 0.08244, Loss_semigroup: 0.00316\n",
      "Iter 2610, Loss: 1.31253, Loss_pinn: 1.22698 Loss_smooth: 0.08241, Loss_semigroup: 0.00314\n",
      "Iter 2620, Loss: 1.31040, Loss_pinn: 1.22542 Loss_smooth: 0.08185, Loss_semigroup: 0.00312\n",
      "Iter 2630, Loss: 1.30897, Loss_pinn: 1.22365 Loss_smooth: 0.08222, Loss_semigroup: 0.00311\n",
      "Iter 2640, Loss: 1.30723, Loss_pinn: 1.22324 Loss_smooth: 0.08090, Loss_semigroup: 0.00309\n",
      "Iter 2650, Loss: 1.30509, Loss_pinn: 1.22075 Loss_smooth: 0.08126, Loss_semigroup: 0.00308\n",
      "Iter 2660, Loss: 1.30332, Loss_pinn: 1.21924 Loss_smooth: 0.08099, Loss_semigroup: 0.00309\n",
      "Iter 2670, Loss: 1.30209, Loss_pinn: 1.21844 Loss_smooth: 0.08060, Loss_semigroup: 0.00305\n",
      "Iter 2680, Loss: 1.30012, Loss_pinn: 1.21642 Loss_smooth: 0.08063, Loss_semigroup: 0.00307\n",
      "Iter 2690, Loss: 1.29902, Loss_pinn: 1.21584 Loss_smooth: 0.08011, Loss_semigroup: 0.00307\n",
      "Iter 2700, Loss: 1.29779, Loss_pinn: 1.21396 Loss_smooth: 0.08075, Loss_semigroup: 0.00309\n",
      "Iter 2710, Loss: 1.29612, Loss_pinn: 1.21236 Loss_smooth: 0.08066, Loss_semigroup: 0.00310\n",
      "Iter 2720, Loss: 1.29479, Loss_pinn: 1.21139 Loss_smooth: 0.08030, Loss_semigroup: 0.00309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2730, Loss: 1.29262, Loss_pinn: 1.20918 Loss_smooth: 0.08034, Loss_semigroup: 0.00309\n",
      "Iter 2740, Loss: 1.29105, Loss_pinn: 1.20744 Loss_smooth: 0.08051, Loss_semigroup: 0.00310\n",
      "Iter 2750, Loss: 1.28971, Loss_pinn: 1.20561 Loss_smooth: 0.08100, Loss_semigroup: 0.00309\n",
      "Iter 2760, Loss: 1.28817, Loss_pinn: 1.20399 Loss_smooth: 0.08107, Loss_semigroup: 0.00310\n",
      "Iter 2770, Loss: 1.28691, Loss_pinn: 1.20246 Loss_smooth: 0.08134, Loss_semigroup: 0.00312\n",
      "Iter 2780, Loss: 1.28567, Loss_pinn: 1.20147 Loss_smooth: 0.08108, Loss_semigroup: 0.00313\n",
      "Iter 2790, Loss: 1.28392, Loss_pinn: 1.19972 Loss_smooth: 0.08106, Loss_semigroup: 0.00314\n",
      "Iter 2800, Loss: 1.28271, Loss_pinn: 1.19851 Loss_smooth: 0.08105, Loss_semigroup: 0.00314\n",
      "Iter 2810, Loss: 1.28058, Loss_pinn: 1.19684 Loss_smooth: 0.08055, Loss_semigroup: 0.00319\n",
      "Iter 2820, Loss: 1.27865, Loss_pinn: 1.19514 Loss_smooth: 0.08032, Loss_semigroup: 0.00319\n",
      "Iter 2830, Loss: 1.27728, Loss_pinn: 1.19352 Loss_smooth: 0.08057, Loss_semigroup: 0.00319\n",
      "Iter 2840, Loss: 1.27598, Loss_pinn: 1.19234 Loss_smooth: 0.08046, Loss_semigroup: 0.00318\n",
      "Iter 2850, Loss: 1.27456, Loss_pinn: 1.19168 Loss_smooth: 0.07971, Loss_semigroup: 0.00318\n",
      "Iter 2860, Loss: 1.27396, Loss_pinn: 1.19058 Loss_smooth: 0.08020, Loss_semigroup: 0.00318\n",
      "Iter 2870, Loss: 1.27297, Loss_pinn: 1.18925 Loss_smooth: 0.08059, Loss_semigroup: 0.00313\n",
      "Iter 2880, Loss: 1.27127, Loss_pinn: 1.18798 Loss_smooth: 0.08014, Loss_semigroup: 0.00315\n",
      "Iter 2890, Loss: 1.26999, Loss_pinn: 1.18659 Loss_smooth: 0.08022, Loss_semigroup: 0.00317\n",
      "Iter 2900, Loss: 1.26850, Loss_pinn: 1.18478 Loss_smooth: 0.08051, Loss_semigroup: 0.00321\n",
      "Iter 2910, Loss: 1.26716, Loss_pinn: 1.18358 Loss_smooth: 0.08035, Loss_semigroup: 0.00323\n",
      "Iter 2920, Loss: 1.26655, Loss_pinn: 1.18292 Loss_smooth: 0.08041, Loss_semigroup: 0.00323\n",
      "Iter 2930, Loss: 1.26508, Loss_pinn: 1.18065 Loss_smooth: 0.08121, Loss_semigroup: 0.00323\n",
      "Iter 2940, Loss: 1.26371, Loss_pinn: 1.17968 Loss_smooth: 0.08076, Loss_semigroup: 0.00327\n",
      "Iter 2950, Loss: 1.26236, Loss_pinn: 1.17841 Loss_smooth: 0.08068, Loss_semigroup: 0.00327\n",
      "Iter 2960, Loss: 1.26085, Loss_pinn: 1.17657 Loss_smooth: 0.08102, Loss_semigroup: 0.00326\n",
      "Iter 2970, Loss: 1.25988, Loss_pinn: 1.17624 Loss_smooth: 0.08039, Loss_semigroup: 0.00325\n",
      "Iter 2980, Loss: 1.25832, Loss_pinn: 1.17570 Loss_smooth: 0.07937, Loss_semigroup: 0.00325\n",
      "Iter 2990, Loss: 1.25714, Loss_pinn: 1.17437 Loss_smooth: 0.07952, Loss_semigroup: 0.00325\n",
      "Iter 3000, Loss: 1.27197, Loss_pinn: 1.17797 Loss_smooth: 0.09068, Loss_semigroup: 0.00332\n",
      "Iter 3010, Loss: 1.25559, Loss_pinn: 1.17287 Loss_smooth: 0.07947, Loss_semigroup: 0.00326\n",
      "Iter 3020, Loss: 1.25447, Loss_pinn: 1.17242 Loss_smooth: 0.07882, Loss_semigroup: 0.00323\n",
      "Iter 3030, Loss: 1.25334, Loss_pinn: 1.17131 Loss_smooth: 0.07882, Loss_semigroup: 0.00322\n",
      "Iter 3040, Loss: 1.25192, Loss_pinn: 1.16929 Loss_smooth: 0.07938, Loss_semigroup: 0.00325\n",
      "Iter 3050, Loss: 1.25074, Loss_pinn: 1.16784 Loss_smooth: 0.07961, Loss_semigroup: 0.00328\n",
      "Iter 3060, Loss: 1.24958, Loss_pinn: 1.16646 Loss_smooth: 0.07983, Loss_semigroup: 0.00328\n",
      "Iter 3070, Loss: 1.24883, Loss_pinn: 1.16554 Loss_smooth: 0.08001, Loss_semigroup: 0.00328\n",
      "Iter 3080, Loss: 1.24781, Loss_pinn: 1.16408 Loss_smooth: 0.08043, Loss_semigroup: 0.00330\n",
      "Iter 3090, Loss: 1.24654, Loss_pinn: 1.16332 Loss_smooth: 0.07993, Loss_semigroup: 0.00328\n",
      "Iter 3100, Loss: 1.24475, Loss_pinn: 1.16177 Loss_smooth: 0.07972, Loss_semigroup: 0.00326\n",
      "Iter 3110, Loss: 1.24339, Loss_pinn: 1.16133 Loss_smooth: 0.07881, Loss_semigroup: 0.00325\n",
      "Iter 3120, Loss: 1.24278, Loss_pinn: 1.16081 Loss_smooth: 0.07873, Loss_semigroup: 0.00324\n",
      "Iter 3130, Loss: 1.24159, Loss_pinn: 1.15933 Loss_smooth: 0.07900, Loss_semigroup: 0.00326\n",
      "Iter 3140, Loss: 1.24055, Loss_pinn: 1.15840 Loss_smooth: 0.07891, Loss_semigroup: 0.00324\n",
      "Iter 3150, Loss: 1.23908, Loss_pinn: 1.15711 Loss_smooth: 0.07875, Loss_semigroup: 0.00322\n",
      "Iter 3160, Loss: 1.23774, Loss_pinn: 1.15577 Loss_smooth: 0.07876, Loss_semigroup: 0.00320\n",
      "Iter 3170, Loss: 1.23660, Loss_pinn: 1.15471 Loss_smooth: 0.07869, Loss_semigroup: 0.00321\n",
      "Iter 3180, Loss: 1.23572, Loss_pinn: 1.15303 Loss_smooth: 0.07949, Loss_semigroup: 0.00320\n",
      "Iter 3190, Loss: 1.23456, Loss_pinn: 1.15169 Loss_smooth: 0.07968, Loss_semigroup: 0.00318\n",
      "Iter 3200, Loss: 1.23378, Loss_pinn: 1.15143 Loss_smooth: 0.07918, Loss_semigroup: 0.00317\n",
      "Iter 3210, Loss: 1.23249, Loss_pinn: 1.14987 Loss_smooth: 0.07945, Loss_semigroup: 0.00317\n",
      "Iter 3220, Loss: 1.23165, Loss_pinn: 1.14903 Loss_smooth: 0.07946, Loss_semigroup: 0.00315\n",
      "Iter 3230, Loss: 1.23053, Loss_pinn: 1.14853 Loss_smooth: 0.07884, Loss_semigroup: 0.00316\n",
      "Iter 3240, Loss: 1.22956, Loss_pinn: 1.14740 Loss_smooth: 0.07900, Loss_semigroup: 0.00316\n",
      "Iter 3250, Loss: 1.22888, Loss_pinn: 1.14634 Loss_smooth: 0.07937, Loss_semigroup: 0.00317\n",
      "Iter 3260, Loss: 1.22792, Loss_pinn: 1.14593 Loss_smooth: 0.07882, Loss_semigroup: 0.00316\n",
      "Iter 3270, Loss: 1.22690, Loss_pinn: 1.14537 Loss_smooth: 0.07835, Loss_semigroup: 0.00318\n",
      "Iter 3280, Loss: 1.22561, Loss_pinn: 1.14402 Loss_smooth: 0.07839, Loss_semigroup: 0.00320\n",
      "Iter 3290, Loss: 1.22438, Loss_pinn: 1.14261 Loss_smooth: 0.07857, Loss_semigroup: 0.00321\n",
      "Iter 3300, Loss: 1.22280, Loss_pinn: 1.14082 Loss_smooth: 0.07874, Loss_semigroup: 0.00324\n",
      "Iter 3310, Loss: 1.22139, Loss_pinn: 1.13962 Loss_smooth: 0.07853, Loss_semigroup: 0.00324\n",
      "Iter 3320, Loss: 1.21994, Loss_pinn: 1.13878 Loss_smooth: 0.07796, Loss_semigroup: 0.00321\n",
      "Iter 3330, Loss: 1.21876, Loss_pinn: 1.13773 Loss_smooth: 0.07783, Loss_semigroup: 0.00320\n",
      "Iter 3340, Loss: 1.21777, Loss_pinn: 1.13705 Loss_smooth: 0.07753, Loss_semigroup: 0.00318\n",
      "Iter 3350, Loss: 1.21681, Loss_pinn: 1.13631 Loss_smooth: 0.07733, Loss_semigroup: 0.00317\n",
      "Iter 3360, Loss: 1.21605, Loss_pinn: 1.13595 Loss_smooth: 0.07693, Loss_semigroup: 0.00317\n",
      "Iter 3370, Loss: 1.21585, Loss_pinn: 1.13496 Loss_smooth: 0.07772, Loss_semigroup: 0.00317\n",
      "Iter 3380, Loss: 1.21405, Loss_pinn: 1.13404 Loss_smooth: 0.07686, Loss_semigroup: 0.00315\n",
      "Iter 3390, Loss: 1.21291, Loss_pinn: 1.13266 Loss_smooth: 0.07710, Loss_semigroup: 0.00314\n",
      "Iter 3400, Loss: 1.21200, Loss_pinn: 1.13219 Loss_smooth: 0.07667, Loss_semigroup: 0.00315\n",
      "Iter 3410, Loss: 1.21132, Loss_pinn: 1.13169 Loss_smooth: 0.07647, Loss_semigroup: 0.00316\n",
      "Iter 3420, Loss: 1.21070, Loss_pinn: 1.13082 Loss_smooth: 0.07672, Loss_semigroup: 0.00315\n",
      "Iter 3430, Loss: 1.20969, Loss_pinn: 1.12968 Loss_smooth: 0.07689, Loss_semigroup: 0.00313\n",
      "Iter 3440, Loss: 1.20836, Loss_pinn: 1.12962 Loss_smooth: 0.07566, Loss_semigroup: 0.00308\n",
      "Iter 3450, Loss: 1.20735, Loss_pinn: 1.12887 Loss_smooth: 0.07541, Loss_semigroup: 0.00306\n",
      "Iter 3460, Loss: 1.20635, Loss_pinn: 1.12819 Loss_smooth: 0.07509, Loss_semigroup: 0.00306\n",
      "Iter 3470, Loss: 1.20523, Loss_pinn: 1.12661 Loss_smooth: 0.07557, Loss_semigroup: 0.00306\n",
      "Iter 3480, Loss: 1.20432, Loss_pinn: 1.12604 Loss_smooth: 0.07523, Loss_semigroup: 0.00305\n",
      "Iter 3490, Loss: 1.20338, Loss_pinn: 1.12521 Loss_smooth: 0.07513, Loss_semigroup: 0.00305\n",
      "Iter 3500, Loss: 1.20212, Loss_pinn: 1.12402 Loss_smooth: 0.07506, Loss_semigroup: 0.00304\n",
      "Iter 3510, Loss: 1.20087, Loss_pinn: 1.12316 Loss_smooth: 0.07467, Loss_semigroup: 0.00303\n",
      "Iter 3520, Loss: 1.19997, Loss_pinn: 1.12233 Loss_smooth: 0.07460, Loss_semigroup: 0.00303\n",
      "Iter 3530, Loss: 1.19893, Loss_pinn: 1.12109 Loss_smooth: 0.07482, Loss_semigroup: 0.00303\n",
      "Iter 3540, Loss: 1.19803, Loss_pinn: 1.12097 Loss_smooth: 0.07402, Loss_semigroup: 0.00303\n",
      "Iter 3550, Loss: 1.19674, Loss_pinn: 1.11918 Loss_smooth: 0.07452, Loss_semigroup: 0.00304\n",
      "Iter 3560, Loss: 1.19582, Loss_pinn: 1.11792 Loss_smooth: 0.07485, Loss_semigroup: 0.00306\n",
      "Iter 3570, Loss: 1.19532, Loss_pinn: 1.11755 Loss_smooth: 0.07471, Loss_semigroup: 0.00306\n",
      "Iter 3580, Loss: 1.19423, Loss_pinn: 1.11560 Loss_smooth: 0.07555, Loss_semigroup: 0.00308\n",
      "Iter 3590, Loss: 1.19326, Loss_pinn: 1.11467 Loss_smooth: 0.07549, Loss_semigroup: 0.00310\n",
      "Iter 3600, Loss: 1.19207, Loss_pinn: 1.11396 Loss_smooth: 0.07500, Loss_semigroup: 0.00310\n",
      "Iter 3610, Loss: 1.19116, Loss_pinn: 1.11290 Loss_smooth: 0.07517, Loss_semigroup: 0.00309\n",
      "Iter 3620, Loss: 1.19044, Loss_pinn: 1.11218 Loss_smooth: 0.07518, Loss_semigroup: 0.00308\n",
      "Iter 3630, Loss: 1.18977, Loss_pinn: 1.11193 Loss_smooth: 0.07475, Loss_semigroup: 0.00309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3640, Loss: 1.18907, Loss_pinn: 1.11156 Loss_smooth: 0.07442, Loss_semigroup: 0.00309\n",
      "Iter 3650, Loss: 1.18829, Loss_pinn: 1.11099 Loss_smooth: 0.07422, Loss_semigroup: 0.00308\n",
      "Iter 3660, Loss: 1.18743, Loss_pinn: 1.11007 Loss_smooth: 0.07430, Loss_semigroup: 0.00306\n",
      "Iter 3670, Loss: 1.18604, Loss_pinn: 1.10922 Loss_smooth: 0.07375, Loss_semigroup: 0.00307\n",
      "Iter 3680, Loss: 1.18521, Loss_pinn: 1.10850 Loss_smooth: 0.07361, Loss_semigroup: 0.00310\n",
      "Iter 3690, Loss: 1.18458, Loss_pinn: 1.10722 Loss_smooth: 0.07424, Loss_semigroup: 0.00311\n",
      "Iter 3700, Loss: 1.18355, Loss_pinn: 1.10666 Loss_smooth: 0.07378, Loss_semigroup: 0.00311\n",
      "Iter 3710, Loss: 1.18270, Loss_pinn: 1.10568 Loss_smooth: 0.07390, Loss_semigroup: 0.00312\n",
      "Iter 3720, Loss: 1.18169, Loss_pinn: 1.10513 Loss_smooth: 0.07344, Loss_semigroup: 0.00312\n",
      "Iter 3730, Loss: 1.18104, Loss_pinn: 1.10430 Loss_smooth: 0.07360, Loss_semigroup: 0.00314\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "Input \u001b[0;32mIn [61]\u001b[0m, in \u001b[0;36mPINN.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/optim/lbfgs.py:425\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_directional_evaluate(closure, x, t, d)\n\u001b[0;32m--> 425\u001b[0m     loss, flat_grad, t, ls_func_evals \u001b[38;5;241m=\u001b[39m \u001b[43m_strong_wolfe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[1;32m    428\u001b[0m opt_cond \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance_grad\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/optim/lbfgs.py:49\u001b[0m, in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     47\u001b[0m g \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mclone(memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcontiguous_format)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# evaluate objective and gradient using initial step\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m f_new, g_new \u001b[38;5;241m=\u001b[39m \u001b[43mobj_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m ls_func_evals \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     51\u001b[0m gtd_new \u001b[38;5;241m=\u001b[39m g_new\u001b[38;5;241m.\u001b[39mdot(d)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/optim/lbfgs.py:423\u001b[0m, in \u001b[0;36mLBFGS.step.<locals>.obj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_directional_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/optim/lbfgs.py:277\u001b[0m, in \u001b[0;36mLBFGS._directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_directional_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure, x, t, d):\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[0;32m--> 277\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    278\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_flat_grad()\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_param(x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [61]\u001b[0m, in \u001b[0;36mPINN.loss_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m y_pred_smooth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_y(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_smooth, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_smooth)\n\u001b[1;32m    105\u001b[0m deriv_pred_below \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_derivative(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_smooth, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_smooth)\n\u001b[0;32m--> 106\u001b[0m deriv_pred_above \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt_smooth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_smooth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m loss_smooth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.0\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t1, t2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(deriv_pred_below, deriv_pred_above):\n",
      "Input \u001b[0;32mIn [61]\u001b[0m, in \u001b[0;36mPINN.net_derivative\u001b[0;34m(self, t, y0)\u001b[0m\n\u001b[1;32m     55\u001b[0m     vec[:,i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# list of derivative tensors\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# the first entry is a tensor with \\partial_t PINN_0(t, y0) for all (t, y0) in the batch,\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# each input (t, y0) corresponds to one row in each tensor\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m derivatives \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     61\u001b[0m     torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(\n\u001b[1;32m     62\u001b[0m         y, t, \n\u001b[1;32m     63\u001b[0m         grad_outputs\u001b[38;5;241m=\u001b[39mvec,\n\u001b[1;32m     64\u001b[0m         retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     65\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m vec \u001b[38;5;129;01min\u001b[39;00m vectors\n\u001b[1;32m     68\u001b[0m ]\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m derivatives\n",
      "Input \u001b[0;32mIn [61]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     55\u001b[0m     vec[:,i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# list of derivative tensors\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# the first entry is a tensor with \\partial_t PINN_0(t, y0) for all (t, y0) in the batch,\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# each input (t, y0) corresponds to one row in each tensor\u001b[39;00m\n\u001b[1;32m     60\u001b[0m derivatives \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 61\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m vec \u001b[38;5;129;01min\u001b[39;00m vectors\n\u001b[1;32m     68\u001b[0m ]\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m derivatives\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:234\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "               \n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "zaYQX3QDouAl",
   "metadata": {
    "id": "zaYQX3QDouAl"
   },
   "outputs": [],
   "source": [
    "with open(\"./model.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7db582b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd52a59",
   "metadata": {
    "id": "3dd52a59"
   },
   "source": [
    "## Predict and Plot the Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ff2236a2",
   "metadata": {
    "id": "ff2236a2"
   },
   "outputs": [],
   "source": [
    "def generate_figure(figsize, xlim, ylim):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.spines[['top', 'right']].set_visible(False)\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_ode_solution(ax, y, index0, index1, *args, **kwargs):\n",
    "    \n",
    "    ax.plot(y[:,index0], y[:,index1], '.-', *args, **kwargs)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2f561ea4",
   "metadata": {
    "id": "2f561ea4"
   },
   "outputs": [],
   "source": [
    "def predict_tc(model, y0, max_t_pred, delta_t):\n",
    "    \"\"\"\n",
    "    detla_t should devide model.max_t to guarantee equidistant steps\n",
    "    \"\"\"\n",
    "    times = np.arange(0, model.T + delta_t, delta_t)[1:]\n",
    "    times = times[:,np.newaxis]\n",
    "    n_resets = int(np.ceil(max_t_pred / model.T))\n",
    "    \n",
    "    trajectory = np.array([y0])\n",
    "    \n",
    "    for _ in range(n_resets):\n",
    "        \n",
    "        y0 = trajectory[-1]\n",
    "        y0 = np.array([y0 for _ in range(len(times))])\n",
    "        segment =  model.predict(times, y0)\n",
    "        trajectory = np.vstack([trajectory, segment])\n",
    "    \n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f2f012bb",
   "metadata": {
    "id": "f2f012bb"
   },
   "outputs": [],
   "source": [
    "# Note that max_t in training is 1\n",
    "y0 = [1., 0., 0., 1., -1., -1, .0, .0, .0, .0, .0, .0]\n",
    "max_t_pred = 10.\n",
    "delta_t = 0.01\n",
    "\n",
    "validation_tc = predict_tc(model, y0, max_t_pred, delta_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21cf58fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "21cf58fc",
    "outputId": "4de0c3b8-7193-4346-acbf-097daf24e50c"
   },
   "outputs": [],
   "source": [
    "fig, ax = generate_figure(figsize=(8,8), xlim=[-7, 7], ylim=[-7, 7])\n",
    "\n",
    "ax = plot_ode_solution(ax, validation_tc, 0, 1, markevery=[0], label=\"Body 1\", color=\"#03468F\")\n",
    "ax = plot_ode_solution(ax, validation_tc, 2, 3, markevery=[0], label=\"Body 2\", color=\"#A51C30\")\n",
    "ax = plot_ode_solution(ax, validation_tc, 4, 5, markevery=[0], label=\"Body 3\", color=\"orange\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"3_body_problem.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
