{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c314520",
   "metadata": {
    "id": "2c314520"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.rcParams['pgf.texsystem'] = 'pdflatex'\n",
    "# matplotlib.rcParams.update({'font.family': 'serif', 'font.size': 10})\n",
    "# matplotlib.rcParams['text.usetex'] = True\n",
    "from matplotlib.lines import Line2D\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "import time\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec36d69",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This jupyter notebook implements the time-consistent physics-informed neural network (tcPINN) idea for the planar three-body problem, restricted to solutions with constant barycenter. Without loss of generality, the domain of intial values can be further reduced by exploiting the roation and scale invariance of the ODE system.\n",
    "\n",
    "Although the training loss is reduced by two orders of magnitude training compared to the naive three-body problem implemenation, the tcPINN is still unable to completely learn the dynamics for this restricted domain of initial values. That is, the PINN loss does not converge to zero.\n",
    "\n",
    "A more comprehensive discussion of the problem is given in $\\texttt{three_body_problem_naive.ipynb}$ and in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6d595",
   "metadata": {
    "id": "dab6d595"
   },
   "source": [
    "Consider three gravitationally interacting identical bodies with positions $r_i(t) \\in \\mathbb{R}^2$. Assuming a gravitational force of $G=1$, the Newtonian equations governing their motion reads\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{d^2}{dt^2} \\begin{pmatrix} r_1(t) \\\\ r_2(t) \\\\ r_3(t) \\end{pmatrix} = \\begin{pmatrix} - \\frac{r_1(t) - r_2(t)}{|r_1(t) - r_2(t)|^3} - \\frac{r_1(t) - r_3(t)}{|r_1(t) - r_3(t)|^3} \\\\ - \\frac{r_2(t) - r_1(t)}{|r_2(t) - r_1(t)|^3} - \\frac{r_2(t) - r_3(t)}{|r_2(t) - r_3(t)|^3} \\\\ - \\frac{r_3(t) - r_1(t)}{|r_3(t) - r_1(t)|^3} - \\frac{r_3(t) - r_2(t)}{|r_3(t) - r_2(t)|^3} \\end{pmatrix}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057dc609",
   "metadata": {
    "id": "057dc609"
   },
   "source": [
    "Notice that the ODE implies\n",
    "\n",
    "\\begin{align*}\n",
    "    \\sum_{i=1}^3 \\frac{d^2}{dt^2} r_i(t) = 0,\n",
    "\\end{align*}\n",
    "\n",
    "which yields\n",
    "\n",
    "\\begin{align*}\n",
    "    \\sum_{i=1}^3 \\frac{d}{dt} r_i(t) = \\overrightarrow{C}_1 = \\sum_{i=1}^3 \\frac{d}{dt} r_i(0),\n",
    "\\end{align*}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align*}\n",
    "    \\sum_{i=1}^3 r_i(t) = \\overrightarrow{C}_1 t + \\overrightarrow{C}_2\n",
    "\\end{align*}\n",
    "\n",
    "with \n",
    "\n",
    "\\begin{align*}\n",
    "    \\overrightarrow{C}_2 = \\sum_{i=1}^3 r_i(0).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beddfd4",
   "metadata": {
    "id": "2beddfd4"
   },
   "source": [
    "That is, the barycenter of the three bodies moves on a straight line. By restricting ourselves to initial conditions with mean location and velocity equal to the origin, we enforce the solution to have a constant barycenter over time, i.e.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\sum_{i=1}^3 r_i(t) = 0.\n",
    "\\end{align*}\n",
    "\n",
    "Both the location and the velocity of the thrid particle can then be obtained by the symmetry of the system:\n",
    "\n",
    "\\begin{align}\n",
    "    r_3(t) &= - (r_1(t) + r_2(t)), \\\\\n",
    "    v_3(t) &= - (v_1(t) + v_2(t))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cfb516",
   "metadata": {
    "id": "d1cfb516"
   },
   "source": [
    "An even stronger assumption on the initial conditions was used in [1] to simplify the training task. Under this assumption, we can plug the formula for $r_3$ into the original ODE system to obtain\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{d^2}{dt^2} \\begin{pmatrix} r_1 \\\\ r_2 \\end{pmatrix} = \\begin{pmatrix} - \\frac{r_1 - r_2}{|r_1 - r_2|^3} - \\frac{2r_1 + r_2}{|2r_1 + r_2|^3} \\\\ - \\frac{r_2 - r_1}{|r_2 - r_1|^3} - \\frac{2r_2 + r_1}{|2r_2 + r_1|^3} \\end{pmatrix}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc79de83",
   "metadata": {
    "id": "cc79de83"
   },
   "source": [
    "This is a second-order ODE system of $4$ equations. By introducing the velocities $v_i(t) = \\frac{d}{dt}r_i(t)$, it can be rewritten as the following first-order ODE system of $8$ equations:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{d}{dt} \\begin{pmatrix} r_1 \\\\ r_2 \\\\ v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ - \\frac{r_1 - r_2}{|r_1 - r_2|^3} - \\frac{2r_1 + r_2}{|2r_1 + r_2|^3} \\\\ - \\frac{r_2 - r_1}{|r_2 - r_1|^3} - \\frac{2r_2 + r_1}{|2r_2 + r_1|^3} \\end{pmatrix}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1809c5",
   "metadata": {
    "id": "0a1809c5"
   },
   "source": [
    "For completeness, all $8$ equations with written-out components are\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{d}{dt} \\begin{pmatrix} r_{11} \\\\ r_{12} \\\\ r_{21} \\\\ r_{22} \\\\ v_{11} \\\\ v_{12} \\\\ v_{21} \\\\ v_{22} \\end{pmatrix} = \\begin{pmatrix} \n",
    "    v_{11} \\\\ v_{12} \\\\ v_{21} \\\\ v_{22} \\\\ \n",
    "    - \\frac{r_{11} - r_{21}}{|r_1 - r_2|^3} - \\frac{2r_{11} + r_{21}}{|2r_1 + r_2|^3} \\\\\n",
    "    - \\frac{r_{12} - r_{22}}{|r_1 - r_2|^3} - \\frac{2r_{12} + r_{22}}{|2r_1 + r_2|^3} \\\\\n",
    "    - \\frac{r_{21} - r_{11}}{|r_2 - r_1|^3} - \\frac{2r_{21} + r_{11}}{|2r_2 + r_1|^3} \\\\\n",
    "    - \\frac{r_{22} - r_{12}}{|r_2 - r_1|^3} - \\frac{2r_{22} + r_{12}}{|2r_2 + r_1|^3}\n",
    "\\end{pmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "With the notation $y = (r_{11}, r_{12}, r_{21}, r_{22}, v_{11}, v_{12}, v_{21}, v_{22})$ used in the implementation, the system reads\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{d}{dt} y = \\begin{pmatrix} \n",
    "    y_4 \\\\ y_5 \\\\ y_6 \\\\ y_7 \\\\ \n",
    "    - \\frac{y_0 - y_2}{((y_0 - y_2)^2 + (y_1 - y_3)^2)^{3/2}} - \\frac{2y_0 + y_2}{((2y_0 + y_2)^2 + (2y_1 + y_3)^2)^{3/2}} \\\\\n",
    "    - \\frac{y_1 - y_3}{((y_0 - y_2)^2 + (y_1 - y_3)^2)^{3/2}} - \\frac{2y_1 + y_3}{((2y_0 + y_2)^2 + (2y_1 + y_3)^2)^{3/2}} \\\\\n",
    "    - \\frac{y_2 - y_0}{((y_0 - y_2)^2 + (y_1 - y_3)^2)^{3/2}} - \\frac{2y_2 + y_0}{((2y_2 + y_0)^2 + (2y_3 + y_1)^2)^{3/2}} \\\\\n",
    "    - \\frac{y_3 - y_1}{((y_0 - y_2)^2 + (y_1 - y_3)^2)^{3/2}} - \\frac{2y_3 + y_1}{((2y_2 + y_0)^2 + (2y_3 + y_1)^2)^{3/2}}\n",
    "\\end{pmatrix}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8c76e2",
   "metadata": {
    "id": "8c8c76e2"
   },
   "source": [
    "Due to rotational and scalar symmetry, we can assume\n",
    "\n",
    "\\begin{align*}\n",
    "    r_1(0) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "without loss of generality.\n",
    "\n",
    "The barycenter of the three particles being equal to the origin of the coordinate system further allows us to assume\n",
    "\n",
    "\\begin{align*}\n",
    "    \\| r_2(0) - \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix} \\| \\leq 1\n",
    "\\end{align*}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align*}\n",
    "    -0.5 &\\leq r_{21}(0) \\\\\n",
    "    0 &\\leq r_{22}(0).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd54600",
   "metadata": {},
   "source": [
    "Notice that the authors of [1] used an even stronger assumption by additionally setting all initial velocities to zero to simplify the training task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca39cb49",
   "metadata": {
    "id": "ca39cb49"
   },
   "outputs": [],
   "source": [
    "# rejection sampling of the intial value of the second body\n",
    "def sample_r2_init(N):\n",
    "    \n",
    "    samples = np.empty((N, 2))\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    while count < N:\n",
    "        \n",
    "        length = np.sqrt(np.random.uniform(0, 1))\n",
    "        angle = np.pi * np.random.uniform(0, 2)\n",
    "        \n",
    "        r21 = length * np.cos(angle)\n",
    "        r22 = length * np.sin(angle)\n",
    "        \n",
    "        cond1 = (r21 + 1)**2 + r22**2 < 1\n",
    "        cond2 = r21 > -.5\n",
    "        cond3 = r22 > 0\n",
    "        \n",
    "        if cond1 and cond2 and cond3:\n",
    "            \n",
    "            samples[count, :] = [r21, r22]\n",
    "            count += 1\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c61bbbc8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "c61bbbc8",
    "outputId": "37e0c365-460f-4139-f2d4-f41f03ad5fac"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# demo of sample_r2_init\n",
    "demo = sample_r2_init(1000)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "plt.scatter(demo[:,0], demo[:,1], s=2)\n",
    "plt.ylim([-1, 1])\n",
    "plt.xlim([-1, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "135abf6b",
   "metadata": {
    "id": "135abf6b"
   },
   "outputs": [],
   "source": [
    "# CUDA support \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92eed791",
   "metadata": {
    "id": "92eed791"
   },
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class DNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.Tanh\n",
    "        \n",
    "        layer_list = list()\n",
    "        \n",
    "        for i in range(self.depth - 1):\n",
    "            \n",
    "            linear_layer = torch.nn.Linear(layers[i], layers[i+1])\n",
    "            torch.nn.init.xavier_normal_(linear_layer.weight, gain=5/3)\n",
    "            torch.nn.init.zeros_(linear_layer.bias.data)\n",
    "            \n",
    "            layer_list.append(\n",
    "                (f\"layer_{i}\", linear_layer)\n",
    "            )\n",
    "            layer_list.append((f\"activation_{i}\", self.activation()))\n",
    "        \n",
    "        last_layer = torch.nn.Linear(layers[-2], layers[-1])\n",
    "        torch.nn.init.xavier_normal_(last_layer.weight)\n",
    "        torch.nn.init.zeros_(last_layer.bias.data)\n",
    "        \n",
    "        layer_list.append(\n",
    "            (f\"layer_{self.depth - 1}\", last_layer)\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x = (t, y0)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "300b459c",
   "metadata": {
    "id": "300b459c"
   },
   "outputs": [],
   "source": [
    "# PINN: physics-informed neural network\n",
    "class TcPINN():\n",
    "\n",
    "    def __init__(self, X_pinn, X_semigroup, X_smooth, layers, T):\n",
    "\n",
    "        # neural network architecture\n",
    "        self.layers = layers\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "        \n",
    "        # semigroup PINN step time\n",
    "        self.T = torch.tensor(T).float().to(device)\n",
    "\n",
    "        # training data\n",
    "        self.t_pinn = torch.tensor(X_pinn[:, :1], requires_grad=True).float().to(device)\n",
    "        self.y_pinn = torch.tensor(X_pinn[:, 1:], requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.s_semigroup = torch.tensor(X_semigroup[:, :1], requires_grad=True).float().to(device)\n",
    "        self.t_semigroup = torch.tensor(X_semigroup[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.y_semigroup = torch.tensor(X_semigroup[:, 2:], requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.t_smooth = torch.tensor(X_smooth[:, :1], requires_grad=True).float().to(device)\n",
    "        self.y_smooth = torch.tensor(X_smooth[:, 1:], requires_grad=True).float().to(device)\n",
    "        \n",
    "        # optimization\n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(), lr=1.0, max_iter=50000, max_eval=50000, \n",
    "            history_size=50, tolerance_grad=1e-5, tolerance_change=np.finfo(float).eps, \n",
    "            line_search_fn=\"strong_wolfe\"\n",
    "        )\n",
    "\n",
    "        self.iter = 0\n",
    "    \n",
    "    \n",
    "    def net_y(self, t, y0):\n",
    "        \n",
    "        # The M(t, y0) = y0 + t N(t, y0) scheme seems to drastically increase the accuracy\n",
    "        # This works perfectly fine with automatic differentiation\n",
    "        y = y0 + t * self.dnn(torch.cat([t, y0], dim=1))\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    \n",
    "    def net_derivative(self, t, y0):\n",
    "        \"\"\"\n",
    "        Pytorch automatic differentiation to compute the derivative of the neural network\n",
    "        \"\"\"\n",
    "        y = self.net_y(t, y0)\n",
    "        \n",
    "        # vectors for the autograd vector Jacobian product \n",
    "        # to compute the derivatives w.r.t. every output dimension\n",
    "        vectors = [torch.zeros_like(y) for _ in range(8)]\n",
    "        \n",
    "        for i, vec in enumerate(vectors):\n",
    "            \n",
    "            vec[:,i] = 1.\n",
    "        \n",
    "        # list of derivative tensors\n",
    "        # the first entry is a tensor with \\partial_t PINN_0(t, y0) for all (t, y0) in the batch,\n",
    "        # each input (t, y0) corresponds to one row in each tensor\n",
    "        derivatives = [\n",
    "            torch.autograd.grad(\n",
    "                y, t, \n",
    "                grad_outputs=vec,\n",
    "                retain_graph=True,\n",
    "                create_graph=True\n",
    "            )[0]\n",
    "            for vec in vectors\n",
    "        ]\n",
    "        \n",
    "        return derivatives\n",
    "    \n",
    "    \n",
    "    def loss_function(self):\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = self.net_y(self.t_pinn, self.y_pinn)\n",
    "        deriv_pred = self.net_derivative(self.t_pinn, self.y_pinn)\n",
    "        \n",
    "        # This is specific to the ODE\n",
    "        loss_pinn0 = torch.mean((deriv_pred[0] - y_pred[:,4:5]) ** 2)\n",
    "        loss_pinn1 = torch.mean((deriv_pred[1] - y_pred[:,5:6]) ** 2)\n",
    "        loss_pinn2 = torch.mean((deriv_pred[2] - y_pred[:,6:7]) ** 2)\n",
    "        loss_pinn3 = torch.mean((deriv_pred[3] - y_pred[:,7:8]) ** 2)\n",
    "        \n",
    "        loss_pinn4 = torch.mean((deriv_pred[4] + (y_pred[:,0:1] - y_pred[:,2:3]) / ((y_pred[:,0:1] - y_pred[:,2:3])**2 + (y_pred[:,1:2] - y_pred[:,3:4])**2)**(3/2) + (2*y_pred[:,0:1] + y_pred[:,2:3]) / ((2*y_pred[:,0:1] + y_pred[:,2:3])**2 + (2*y_pred[:,1:2] + y_pred[:,3:4])**2)**(3/2)) ** 2)\n",
    "        loss_pinn5 = torch.mean((deriv_pred[5] + (y_pred[:,1:2] - y_pred[:,3:4]) / ((y_pred[:,0:1] - y_pred[:,2:3])**2 + (y_pred[:,1:2] - y_pred[:,3:4])**2)**(3/2) + (2*y_pred[:,1:2] + y_pred[:,3:4]) / ((2*y_pred[:,0:1] + y_pred[:,2:3])**2 + (2*y_pred[:,1:2] + y_pred[:,3:4])**2)**(3/2)) ** 2)\n",
    "        loss_pinn6 = torch.mean((deriv_pred[6] + (y_pred[:,2:3] - y_pred[:,0:1]) / ((y_pred[:,0:1] - y_pred[:,2:3])**2 + (y_pred[:,1:2] - y_pred[:,3:4])**2)**(3/2) + (2*y_pred[:,2:3] + y_pred[:,0:1]) / ((2*y_pred[:,2:3] + y_pred[:,0:1])**2 + (2*y_pred[:,3:4] + y_pred[:,1:2])**2)**(3/2)) ** 2)\n",
    "        loss_pinn7 = torch.mean((deriv_pred[7] + (y_pred[:,3:4] - y_pred[:,1:2]) / ((y_pred[:,0:1] - y_pred[:,2:3])**2 + (y_pred[:,1:2] - y_pred[:,3:4])**2)**(3/2) + (2*y_pred[:,3:4] + y_pred[:,1:2]) / ((2*y_pred[:,2:3] + y_pred[:,0:1])**2 + (2*y_pred[:,3:4] + y_pred[:,1:2])**2)**(3/2)) ** 2)\n",
    "        \n",
    "        loss_pinn = loss_pinn0 + loss_pinn1 + loss_pinn2 + loss_pinn3 + loss_pinn4 + loss_pinn5 + loss_pinn6 + loss_pinn7\n",
    "        \n",
    "        # The general semigroup loss for autonomous ODEs\n",
    "        y_pred_tps = self.net_y(self.s_semigroup + self.t_semigroup, self.y_semigroup)\n",
    "        y_pred_s = self.net_y(self.s_semigroup, self.y_semigroup)\n",
    "        y_pred_restart = self.net_y(self.t_semigroup, y_pred_s)\n",
    "        loss_semigroup = torch.mean((y_pred_tps - y_pred_restart) ** 2)\n",
    "        \n",
    "        # The smoothness loss\n",
    "        y_pred_smooth = self.net_y(self.t_smooth, self.y_smooth)\n",
    "        deriv_pred_below = self.net_derivative(self.t_smooth, self.y_smooth)\n",
    "        deriv_pred_above = self.net_derivative(torch.zeros_like(self.t_smooth, requires_grad=True), y_pred_smooth)\n",
    "        \n",
    "        loss_smooth = .0\n",
    "        \n",
    "        for t1, t2 in zip(deriv_pred_below, deriv_pred_above):\n",
    "            \n",
    "            loss_smooth += torch.mean((t1 - t2) ** 2)\n",
    "        \n",
    "        loss = loss_pinn + loss_smooth + loss_semigroup\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        self.iter += 1\n",
    "             \n",
    "        if self.iter % 1 == 0:\n",
    "            print(\n",
    "                f\"Iter {self.iter}, Loss: {loss.item():.5f}, Loss_pinn: {loss_pinn.item():.5f} \" \\\n",
    "                f\"Loss_smooth: {loss_smooth.item():.5f}, Loss_semigroup: {loss_semigroup.item():.5f}\"\n",
    "            )\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        self.dnn.train()\n",
    "        self.optimizer.step(self.loss_function)\n",
    "        \n",
    "        # save model checkpoint to allow further training\n",
    "        state = {\n",
    "                \"state_dict\": self.dnn.state_dict(),\n",
    "                \"optimizer\": self.optimizer.state_dict()\n",
    "            }\n",
    "        torch.save(state, \"./model_center_restricted_ivp\")\n",
    "    \n",
    "    \n",
    "    def predict(self, t, y0):\n",
    "        \n",
    "        t = torch.tensor(t, requires_grad=True).float().to(device)\n",
    "        y0 = torch.tensor(y0, requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.dnn.eval()\n",
    "        y = self.net_y(t, y0)\n",
    "        y = y.detach().cpu().numpy()\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e84c9ec",
   "metadata": {
    "id": "9e84c9ec"
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(model, filename):\n",
    "    \"\"\"\n",
    "    Update the state of model.dnn and model.optimizer\n",
    "    Usecase: Stop and restart the training process\n",
    "\n",
    "    NOTE: When training with a GPU, the loaded models have to be moved to cuda\n",
    "      see https://discuss.pytorch.org/t/loading-a-saved-model-for-continue-training/17244/2\n",
    "    \"\"\"   \n",
    "    if os.path.isfile(filename):\n",
    "        \n",
    "        print(f\"=> loading checkpoint '{filename}'\")\n",
    "        \n",
    "        checkpoint = torch.load(filename)\n",
    "        \n",
    "        model.dnn.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        model.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        model.dnn.train()\n",
    "        \n",
    "        print(f\"=> loaded checkpoint '{filename}'\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"=> no checkpoint found at '{filename}'\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68934f41",
   "metadata": {
    "id": "68934f41"
   },
   "source": [
    "### Setup Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "OV--ARgV5RQ5",
   "metadata": {
    "id": "OV--ARgV5RQ5"
   },
   "outputs": [],
   "source": [
    "layers = [9] + 15 * [128] + [8]\n",
    "\n",
    "\n",
    "T = 1\n",
    "max_v = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5ae371e",
   "metadata": {
    "id": "b5ae371e"
   },
   "outputs": [],
   "source": [
    "# standard PINN loss function training samples\n",
    "N_pinn = 200000\n",
    "N_semigroup = 50000\n",
    "N_smooth = 50000\n",
    "\n",
    "\n",
    "def sample_y(N, max_v):\n",
    "    \n",
    "    r1 = np.tile([1.,0.], (N,1))\n",
    "    r2 = sample_r2_init(N)\n",
    "    v = np.random.uniform(-max_v, max_v, (N, 4))\n",
    "    \n",
    "    return np.hstack([r1, r2, v])\n",
    "\n",
    "\n",
    "t_pinn = np.random.uniform(0, T, (N_pinn, 1))\n",
    "y_pinn = sample_y(N_pinn,  max_v)\n",
    "X_pinn = np.hstack([t_pinn, y_pinn])\n",
    "\n",
    "\n",
    "r1 = np.random.uniform(0, 1, N_semigroup)\n",
    "r2 = np.random.uniform(0, 1, N_semigroup)\n",
    "s_semigroup, t_semigroup = np.sqrt(r1) * (1 - r2), r2 * np.sqrt(r1)\n",
    "s_semigroup, t_semigroup = T * s_semigroup[:, np.newaxis], T * t_semigroup[:, np.newaxis]\n",
    "y_semigroup = sample_y(N_semigroup, max_v)\n",
    "X_semigroup = np.hstack([s_semigroup, t_semigroup, y_semigroup])\n",
    "\n",
    "\n",
    "t_smooth = np.random.uniform(0, T, (N_smooth, 1))\n",
    "y_smooth = sample_y(N_smooth, max_v)\n",
    "X_smooth = np.hstack([t_smooth, y_smooth])\n",
    "\n",
    "\n",
    "with open(\"./X_pinn_center_restricted_ivp.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(X_pinn, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"./X_semigroup_center_restricted_ivp.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(X_semigroup, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"./X_smooth_center_restricted_ivp.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(X_smooth, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e50f9b7",
   "metadata": {
    "id": "5e50f9b7"
   },
   "outputs": [],
   "source": [
    "model = TcPINN(X_pinn, X_semigroup, X_smooth, layers, T)\n",
    "\n",
    "opt_sd = model.optimizer.state_dict()\n",
    "opt_sd ['param_groups'][0]['max_iter'] = 50\n",
    "model.optimizer.load_state_dict(opt_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccc4d4a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ccc4d4a3",
    "outputId": "255245a2-ee03-4ee8-8d6b-d906c3904200",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1, Loss: 2766.70581, Loss_pinn: 2754.48486 Loss_smooth: 12.12974, Loss_semigroup: 0.09129\n",
      "Iter 2, Loss: 4575.29053, Loss_pinn: 4563.13672 Loss_smooth: 12.06243, Loss_semigroup: 0.09131\n",
      "Iter 3, Loss: 1816.97754, Loss_pinn: 1804.76294 Loss_smooth: 12.12324, Loss_semigroup: 0.09129\n",
      "Iter 4, Loss: 1441.47412, Loss_pinn: 1429.25427 Loss_smooth: 12.12857, Loss_semigroup: 0.09130\n",
      "Iter 5, Loss: 1426.77759, Loss_pinn: 1414.55859 Loss_smooth: 12.12773, Loss_semigroup: 0.09129\n",
      "Iter 6, Loss: 1347.17371, Loss_pinn: 1334.96021 Loss_smooth: 12.12216, Loss_semigroup: 0.09125\n",
      "Iter 7, Loss: 1233.59473, Loss_pinn: 1221.39209 Loss_smooth: 12.11142, Loss_semigroup: 0.09118\n",
      "Iter 8, Loss: 1092.73633, Loss_pinn: 1080.54749 Loss_smooth: 12.09783, Loss_semigroup: 0.09112\n",
      "Iter 9, Loss: 902.05908, Loss_pinn: 889.89832 Loss_smooth: 12.06982, Loss_semigroup: 0.09094\n",
      "Iter 10, Loss: 895.09204, Loss_pinn: 883.02637 Loss_smooth: 11.97552, Loss_semigroup: 0.09014\n",
      "Iter 11, Loss: 723.24933, Loss_pinn: 711.14673 Loss_smooth: 12.01212, Loss_semigroup: 0.09045\n",
      "Iter 12, Loss: 638.35809, Loss_pinn: 626.28235 Loss_smooth: 11.98545, Loss_semigroup: 0.09028\n",
      "Iter 13, Loss: 532.14142, Loss_pinn: 520.12109 Loss_smooth: 11.93036, Loss_semigroup: 0.08996\n",
      "Iter 14, Loss: 501.75760, Loss_pinn: 489.76483 Loss_smooth: 11.90298, Loss_semigroup: 0.08978\n",
      "Iter 15, Loss: 462.33371, Loss_pinn: 450.34058 Loss_smooth: 11.90347, Loss_semigroup: 0.08967\n",
      "Iter 16, Loss: 455.28369, Loss_pinn: 443.29266 Loss_smooth: 11.90144, Loss_semigroup: 0.08960\n",
      "Iter 17, Loss: 436.91858, Loss_pinn: 424.90308 Loss_smooth: 11.92593, Loss_semigroup: 0.08956\n",
      "Iter 18, Loss: 431.56244, Loss_pinn: 419.53610 Loss_smooth: 11.93674, Loss_semigroup: 0.08960\n",
      "Iter 19, Loss: 414.24026, Loss_pinn: 402.17319 Loss_smooth: 11.97746, Loss_semigroup: 0.08964\n",
      "Iter 20, Loss: 403.01569, Loss_pinn: 390.92606 Loss_smooth: 11.99994, Loss_semigroup: 0.08968\n",
      "Iter 21, Loss: 369.64633, Loss_pinn: 357.51508 Loss_smooth: 12.04152, Loss_semigroup: 0.08971\n",
      "Iter 22, Loss: 356.28778, Loss_pinn: 344.13702 Loss_smooth: 12.06105, Loss_semigroup: 0.08971\n",
      "Iter 23, Loss: 336.31778, Loss_pinn: 324.18634 Loss_smooth: 12.04172, Loss_semigroup: 0.08974\n",
      "Iter 24, Loss: 317.82156, Loss_pinn: 305.67761 Loss_smooth: 12.05409, Loss_semigroup: 0.08984\n",
      "Iter 25, Loss: 229.42256, Loss_pinn: 217.13315 Loss_smooth: 12.19867, Loss_semigroup: 0.09075\n",
      "Iter 26, Loss: 184.71886, Loss_pinn: 172.32999 Loss_smooth: 12.29759, Loss_semigroup: 0.09127\n",
      "Iter 27, Loss: 183.94699, Loss_pinn: 171.35851 Loss_smooth: 12.49626, Loss_semigroup: 0.09223\n",
      "Iter 28, Loss: 157.56989, Loss_pinn: 145.06088 Loss_smooth: 12.41714, Loss_semigroup: 0.09186\n",
      "Iter 29, Loss: 141.24603, Loss_pinn: 128.64352 Loss_smooth: 12.51006, Loss_semigroup: 0.09244\n",
      "Iter 30, Loss: 134.08769, Loss_pinn: 121.53558 Loss_smooth: 12.45974, Loss_semigroup: 0.09237\n",
      "Iter 31, Loss: 124.75230, Loss_pinn: 112.15366 Loss_smooth: 12.50593, Loss_semigroup: 0.09272\n",
      "Iter 32, Loss: 106.64126, Loss_pinn: 93.89877 Loss_smooth: 12.64866, Loss_semigroup: 0.09383\n",
      "Iter 33, Loss: 94.46275, Loss_pinn: 81.55647 Loss_smooth: 12.81111, Loss_semigroup: 0.09516\n",
      "Iter 34, Loss: 93.01086, Loss_pinn: 80.11833 Loss_smooth: 12.79745, Loss_semigroup: 0.09508\n",
      "Iter 35, Loss: 89.34783, Loss_pinn: 76.40628 Loss_smooth: 12.84606, Loss_semigroup: 0.09550\n",
      "Iter 36, Loss: 85.41232, Loss_pinn: 72.33970 Loss_smooth: 12.97608, Loss_semigroup: 0.09653\n",
      "Iter 37, Loss: 84.62854, Loss_pinn: 71.54726 Loss_smooth: 12.98472, Loss_semigroup: 0.09655\n",
      "Iter 38, Loss: 82.13082, Loss_pinn: 68.97342 Loss_smooth: 13.06047, Loss_semigroup: 0.09693\n",
      "Iter 39, Loss: 79.26447, Loss_pinn: 66.06166 Loss_smooth: 13.10579, Loss_semigroup: 0.09702\n",
      "Iter 40, Loss: 75.84801, Loss_pinn: 62.53690 Loss_smooth: 13.21367, Loss_semigroup: 0.09744\n",
      "Iter 41, Loss: 73.02544, Loss_pinn: 59.64322 Loss_smooth: 13.28440, Loss_semigroup: 0.09782\n",
      "Iter 42, Loss: 65.45891, Loss_pinn: 51.86698 Loss_smooth: 13.49259, Loss_semigroup: 0.09933\n",
      "Iter 43, Loss: 64.41582, Loss_pinn: 50.74866 Loss_smooth: 13.56731, Loss_semigroup: 0.09985\n",
      "Iter 44, Loss: 62.19829, Loss_pinn: 48.41462 Loss_smooth: 13.68307, Loss_semigroup: 0.10060\n",
      "Iter 45, Loss: 176.63123, Loss_pinn: 162.02359 Loss_smooth: 14.50080, Loss_semigroup: 0.10684\n",
      "Iter 46, Loss: 73.39608, Loss_pinn: 59.15150 Loss_smooth: 14.14062, Loss_semigroup: 0.10395\n",
      "Iter 47, Loss: 59.51862, Loss_pinn: 45.50759 Loss_smooth: 13.90883, Loss_semigroup: 0.10219\n",
      "Iter 48, Loss: 55.29504, Loss_pinn: 41.20336 Loss_smooth: 13.98912, Loss_semigroup: 0.10256\n",
      "Iter 49, Loss: 51.64507, Loss_pinn: 37.39692 Loss_smooth: 14.14465, Loss_semigroup: 0.10350\n",
      "Iter 50, Loss: 49.96855, Loss_pinn: 35.79559 Loss_smooth: 14.07014, Loss_semigroup: 0.10281\n",
      "Iter 51, Loss: 49.29688, Loss_pinn: 34.99106 Loss_smooth: 14.20201, Loss_semigroup: 0.10381\n",
      "Iter 52, Loss: 48.10910, Loss_pinn: 33.49982 Loss_smooth: 14.50316, Loss_semigroup: 0.10612\n",
      "Iter 53, Loss: 46.87423, Loss_pinn: 31.93156 Loss_smooth: 14.83394, Loss_semigroup: 0.10873\n",
      "Iter 54, Loss: 48.22590, Loss_pinn: 32.73000 Loss_smooth: 15.38296, Loss_semigroup: 0.11293\n",
      "Iter 55, Loss: 45.90660, Loss_pinn: 30.66711 Loss_smooth: 15.12851, Loss_semigroup: 0.11098\n",
      "Iter 56, Loss: 45.43073, Loss_pinn: 30.36711 Loss_smooth: 14.95407, Loss_semigroup: 0.10953\n",
      "Iter 57, Loss: 45.14754, Loss_pinn: 30.05381 Loss_smooth: 14.98398, Loss_semigroup: 0.10976\n",
      "Iter 58, Loss: 44.42052, Loss_pinn: 29.31352 Loss_smooth: 14.99704, Loss_semigroup: 0.10996\n",
      "Iter 59, Loss: 43.85015, Loss_pinn: 28.77298 Loss_smooth: 14.96731, Loss_semigroup: 0.10985\n",
      "CPU times: user 2h 11min 13s, sys: 1h 3min 3s, total: 3h 14min 16s\n",
      "Wall time: 29min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "               \n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eyGc1RPfPuPp",
   "metadata": {
    "id": "eyGc1RPfPuPp"
   },
   "outputs": [],
   "source": [
    "n_training = 1\n",
    "max_n_training = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e52b6e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e52b6e5",
    "outputId": "45161553-24d3-4302-b4e9-01ff72bbaab5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training run 1\n",
      "=> loading checkpoint 'model_center_restricted_ivp'\n",
      "=> loaded checkpoint 'model_center_restricted_ivp'\n",
      "Iter 1, Loss: 0.48575, Loss_pinn: 0.46548 Loss_smooth: 0.01946, Loss_semigroup: 0.00082\n",
      "Iter 2, Loss: 0.48545, Loss_pinn: 0.46508 Loss_smooth: 0.01955, Loss_semigroup: 0.00082\n",
      "Iter 3, Loss: 0.49294, Loss_pinn: 0.47197 Loss_smooth: 0.02013, Loss_semigroup: 0.00085\n",
      "Iter 4, Loss: 0.48535, Loss_pinn: 0.46515 Loss_smooth: 0.01938, Loss_semigroup: 0.00083\n",
      "Iter 5, Loss: 0.48488, Loss_pinn: 0.46425 Loss_smooth: 0.01979, Loss_semigroup: 0.00084\n",
      "Iter 6, Loss: 0.49021, Loss_pinn: 0.46879 Loss_smooth: 0.02053, Loss_semigroup: 0.00090\n",
      "Iter 7, Loss: 0.48473, Loss_pinn: 0.46415 Loss_smooth: 0.01972, Loss_semigroup: 0.00085\n",
      "Iter 8, Loss: 0.48429, Loss_pinn: 0.46364 Loss_smooth: 0.01980, Loss_semigroup: 0.00085\n",
      "Iter 9, Loss: 0.48377, Loss_pinn: 0.46316 Loss_smooth: 0.01979, Loss_semigroup: 0.00081\n",
      "Iter 10, Loss: 0.48423, Loss_pinn: 0.46382 Loss_smooth: 0.01960, Loss_semigroup: 0.00081\n",
      "Iter 11, Loss: 0.48364, Loss_pinn: 0.46314 Loss_smooth: 0.01969, Loss_semigroup: 0.00081\n",
      "Iter 12, Loss: 0.48335, Loss_pinn: 0.46284 Loss_smooth: 0.01969, Loss_semigroup: 0.00082\n",
      "Iter 13, Loss: 0.48299, Loss_pinn: 0.46249 Loss_smooth: 0.01968, Loss_semigroup: 0.00082\n",
      "Iter 14, Loss: 0.48275, Loss_pinn: 0.46220 Loss_smooth: 0.01973, Loss_semigroup: 0.00082\n",
      "Iter 15, Loss: 0.48241, Loss_pinn: 0.46170 Loss_smooth: 0.01989, Loss_semigroup: 0.00082\n",
      "Iter 16, Loss: 0.49055, Loss_pinn: 0.46669 Loss_smooth: 0.02295, Loss_semigroup: 0.00091\n",
      "Iter 17, Loss: 0.48227, Loss_pinn: 0.46125 Loss_smooth: 0.02017, Loss_semigroup: 0.00084\n",
      "Iter 18, Loss: 0.48194, Loss_pinn: 0.46085 Loss_smooth: 0.02025, Loss_semigroup: 0.00084\n",
      "Iter 19, Loss: 0.48158, Loss_pinn: 0.46025 Loss_smooth: 0.02049, Loss_semigroup: 0.00085\n",
      "Iter 20, Loss: 0.48104, Loss_pinn: 0.45956 Loss_smooth: 0.02062, Loss_semigroup: 0.00085\n",
      "Iter 21, Loss: 0.47994, Loss_pinn: 0.45849 Loss_smooth: 0.02060, Loss_semigroup: 0.00086\n",
      "Iter 22, Loss: 0.47936, Loss_pinn: 0.45793 Loss_smooth: 0.02058, Loss_semigroup: 0.00086\n",
      "Iter 23, Loss: 0.47970, Loss_pinn: 0.45876 Loss_smooth: 0.02010, Loss_semigroup: 0.00084\n",
      "Iter 24, Loss: 0.47894, Loss_pinn: 0.45778 Loss_smooth: 0.02031, Loss_semigroup: 0.00085\n",
      "Iter 25, Loss: 0.47868, Loss_pinn: 0.45773 Loss_smooth: 0.02010, Loss_semigroup: 0.00085\n",
      "Iter 26, Loss: 0.47832, Loss_pinn: 0.45775 Loss_smooth: 0.01974, Loss_semigroup: 0.00083\n",
      "Iter 27, Loss: 0.47776, Loss_pinn: 0.45770 Loss_smooth: 0.01924, Loss_semigroup: 0.00081\n",
      "Iter 28, Loss: 0.47756, Loss_pinn: 0.45775 Loss_smooth: 0.01901, Loss_semigroup: 0.00080\n",
      "Iter 29, Loss: 0.47675, Loss_pinn: 0.45737 Loss_smooth: 0.01860, Loss_semigroup: 0.00077\n",
      "Iter 30, Loss: 0.47607, Loss_pinn: 0.45660 Loss_smooth: 0.01869, Loss_semigroup: 0.00078\n",
      "Iter 31, Loss: 0.47530, Loss_pinn: 0.45580 Loss_smooth: 0.01871, Loss_semigroup: 0.00079\n",
      "Iter 32, Loss: 0.47436, Loss_pinn: 0.45484 Loss_smooth: 0.01875, Loss_semigroup: 0.00076\n",
      "Iter 33, Loss: 0.47374, Loss_pinn: 0.45380 Loss_smooth: 0.01920, Loss_semigroup: 0.00074\n",
      "Iter 34, Loss: 0.47371, Loss_pinn: 0.45376 Loss_smooth: 0.01920, Loss_semigroup: 0.00075\n",
      "Iter 35, Loss: 0.47333, Loss_pinn: 0.45351 Loss_smooth: 0.01908, Loss_semigroup: 0.00074\n",
      "Iter 36, Loss: 0.47295, Loss_pinn: 0.45347 Loss_smooth: 0.01875, Loss_semigroup: 0.00073\n",
      "Iter 37, Loss: 0.47277, Loss_pinn: 0.45337 Loss_smooth: 0.01867, Loss_semigroup: 0.00073\n",
      "Iter 38, Loss: 0.47264, Loss_pinn: 0.45316 Loss_smooth: 0.01876, Loss_semigroup: 0.00072\n",
      "Iter 39, Loss: 0.47252, Loss_pinn: 0.45299 Loss_smooth: 0.01880, Loss_semigroup: 0.00073\n",
      "Iter 40, Loss: 0.47239, Loss_pinn: 0.45270 Loss_smooth: 0.01897, Loss_semigroup: 0.00073\n",
      "Iter 41, Loss: 0.47230, Loss_pinn: 0.45262 Loss_smooth: 0.01895, Loss_semigroup: 0.00072\n",
      "Iter 42, Loss: 0.47217, Loss_pinn: 0.45253 Loss_smooth: 0.01892, Loss_semigroup: 0.00072\n",
      "Iter 43, Loss: 0.47184, Loss_pinn: 0.45210 Loss_smooth: 0.01903, Loss_semigroup: 0.00071\n",
      "Iter 44, Loss: 0.47148, Loss_pinn: 0.45191 Loss_smooth: 0.01887, Loss_semigroup: 0.00070\n",
      "Iter 45, Loss: 0.47088, Loss_pinn: 0.45096 Loss_smooth: 0.01922, Loss_semigroup: 0.00071\n",
      "Iter 46, Loss: 0.47004, Loss_pinn: 0.44984 Loss_smooth: 0.01948, Loss_semigroup: 0.00072\n",
      "Iter 47, Loss: 0.46915, Loss_pinn: 0.44857 Loss_smooth: 0.01985, Loss_semigroup: 0.00073\n",
      "Iter 48, Loss: 0.47370, Loss_pinn: 0.45138 Loss_smooth: 0.02153, Loss_semigroup: 0.00078\n",
      "Iter 49, Loss: 0.46888, Loss_pinn: 0.44799 Loss_smooth: 0.02014, Loss_semigroup: 0.00074\n",
      "Iter 50, Loss: 0.46836, Loss_pinn: 0.44696 Loss_smooth: 0.02065, Loss_semigroup: 0.00076\n",
      "Iter 51, Loss: 0.46770, Loss_pinn: 0.44708 Loss_smooth: 0.01988, Loss_semigroup: 0.00075\n",
      "Iter 52, Loss: 0.46768, Loss_pinn: 0.44707 Loss_smooth: 0.01987, Loss_semigroup: 0.00074\n",
      "Iter 53, Loss: 0.46750, Loss_pinn: 0.44690 Loss_smooth: 0.01986, Loss_semigroup: 0.00075\n",
      "Iter 54, Loss: 0.46717, Loss_pinn: 0.44680 Loss_smooth: 0.01962, Loss_semigroup: 0.00075\n",
      "Iter 55, Loss: 0.46681, Loss_pinn: 0.44633 Loss_smooth: 0.01972, Loss_semigroup: 0.00076\n",
      "Iter 56, Loss: 0.46636, Loss_pinn: 0.44562 Loss_smooth: 0.01992, Loss_semigroup: 0.00082\n",
      "Iter 57, Loss: 0.46595, Loss_pinn: 0.44433 Loss_smooth: 0.02078, Loss_semigroup: 0.00084\n",
      "Iter 58, Loss: 0.46523, Loss_pinn: 0.44374 Loss_smooth: 0.02065, Loss_semigroup: 0.00084\n",
      "Iter 59, Loss: 0.46487, Loss_pinn: 0.44374 Loss_smooth: 0.02029, Loss_semigroup: 0.00084\n",
      "\n",
      "Training run 2\n",
      "=> loading checkpoint 'model_center_restricted_ivp'\n",
      "=> loaded checkpoint 'model_center_restricted_ivp'\n",
      "Iter 1, Loss: 0.46487, Loss_pinn: 0.44374 Loss_smooth: 0.02029, Loss_semigroup: 0.00084\n",
      "Iter 2, Loss: 0.46440, Loss_pinn: 0.44312 Loss_smooth: 0.02043, Loss_semigroup: 0.00085\n",
      "Iter 3, Loss: 0.46430, Loss_pinn: 0.44257 Loss_smooth: 0.02086, Loss_semigroup: 0.00087\n",
      "Iter 4, Loss: 0.46403, Loss_pinn: 0.44256 Loss_smooth: 0.02061, Loss_semigroup: 0.00086\n",
      "Iter 5, Loss: 0.46372, Loss_pinn: 0.44178 Loss_smooth: 0.02105, Loss_semigroup: 0.00088\n",
      "Iter 6, Loss: 0.46346, Loss_pinn: 0.44157 Loss_smooth: 0.02101, Loss_semigroup: 0.00088\n",
      "Iter 7, Loss: 0.46314, Loss_pinn: 0.44147 Loss_smooth: 0.02080, Loss_semigroup: 0.00087\n",
      "Iter 8, Loss: 0.46293, Loss_pinn: 0.44150 Loss_smooth: 0.02057, Loss_semigroup: 0.00086\n",
      "Iter 9, Loss: 0.46245, Loss_pinn: 0.44133 Loss_smooth: 0.02027, Loss_semigroup: 0.00085\n",
      "Iter 10, Loss: 0.47829, Loss_pinn: 0.45678 Loss_smooth: 0.02066, Loss_semigroup: 0.00085\n",
      "Iter 11, Loss: 0.46235, Loss_pinn: 0.44134 Loss_smooth: 0.02017, Loss_semigroup: 0.00085\n",
      "Iter 12, Loss: 0.46196, Loss_pinn: 0.44115 Loss_smooth: 0.01998, Loss_semigroup: 0.00083\n",
      "Iter 13, Loss: 0.46167, Loss_pinn: 0.44086 Loss_smooth: 0.01998, Loss_semigroup: 0.00083\n",
      "Iter 14, Loss: 0.46134, Loss_pinn: 0.44057 Loss_smooth: 0.01993, Loss_semigroup: 0.00083\n",
      "Iter 15, Loss: 0.46091, Loss_pinn: 0.44023 Loss_smooth: 0.01985, Loss_semigroup: 0.00083\n",
      "Iter 16, Loss: 0.46058, Loss_pinn: 0.43996 Loss_smooth: 0.01979, Loss_semigroup: 0.00083\n",
      "Iter 17, Loss: 0.46041, Loss_pinn: 0.44003 Loss_smooth: 0.01956, Loss_semigroup: 0.00082\n",
      "Iter 18, Loss: 0.46025, Loss_pinn: 0.43999 Loss_smooth: 0.01945, Loss_semigroup: 0.00082\n",
      "Iter 19, Loss: 0.45996, Loss_pinn: 0.43997 Loss_smooth: 0.01919, Loss_semigroup: 0.00080\n",
      "Iter 20, Loss: 0.45973, Loss_pinn: 0.44007 Loss_smooth: 0.01888, Loss_semigroup: 0.00078\n",
      "Iter 21, Loss: 0.45961, Loss_pinn: 0.44013 Loss_smooth: 0.01871, Loss_semigroup: 0.00077\n",
      "Iter 22, Loss: 0.45946, Loss_pinn: 0.43997 Loss_smooth: 0.01872, Loss_semigroup: 0.00077\n",
      "Iter 23, Loss: 0.45933, Loss_pinn: 0.43970 Loss_smooth: 0.01887, Loss_semigroup: 0.00077\n",
      "Iter 24, Loss: 0.45924, Loss_pinn: 0.43945 Loss_smooth: 0.01902, Loss_semigroup: 0.00077\n",
      "Iter 25, Loss: 0.45904, Loss_pinn: 0.43900 Loss_smooth: 0.01928, Loss_semigroup: 0.00077\n",
      "Iter 26, Loss: 0.45864, Loss_pinn: 0.43850 Loss_smooth: 0.01936, Loss_semigroup: 0.00078\n",
      "Iter 27, Loss: 0.45825, Loss_pinn: 0.43808 Loss_smooth: 0.01939, Loss_semigroup: 0.00078\n",
      "Iter 28, Loss: 0.45776, Loss_pinn: 0.43805 Loss_smooth: 0.01894, Loss_semigroup: 0.00077\n",
      "Iter 29, Loss: 0.45739, Loss_pinn: 0.43757 Loss_smooth: 0.01903, Loss_semigroup: 0.00078\n",
      "Iter 30, Loss: 0.45716, Loss_pinn: 0.43742 Loss_smooth: 0.01894, Loss_semigroup: 0.00079\n",
      "Iter 31, Loss: 0.45690, Loss_pinn: 0.43707 Loss_smooth: 0.01904, Loss_semigroup: 0.00080\n",
      "Iter 32, Loss: 0.45679, Loss_pinn: 0.43689 Loss_smooth: 0.01910, Loss_semigroup: 0.00080\n",
      "Iter 33, Loss: 0.45663, Loss_pinn: 0.43667 Loss_smooth: 0.01916, Loss_semigroup: 0.00080\n",
      "Iter 34, Loss: 0.45653, Loss_pinn: 0.43659 Loss_smooth: 0.01913, Loss_semigroup: 0.00081\n",
      "Iter 35, Loss: 0.45631, Loss_pinn: 0.43627 Loss_smooth: 0.01922, Loss_semigroup: 0.00082\n",
      "Iter 36, Loss: 0.45611, Loss_pinn: 0.43598 Loss_smooth: 0.01931, Loss_semigroup: 0.00082\n",
      "Iter 37, Loss: 0.45582, Loss_pinn: 0.43561 Loss_smooth: 0.01939, Loss_semigroup: 0.00082\n",
      "Iter 38, Loss: 0.45551, Loss_pinn: 0.43531 Loss_smooth: 0.01938, Loss_semigroup: 0.00082\n",
      "Iter 39, Loss: 0.45525, Loss_pinn: 0.43534 Loss_smooth: 0.01910, Loss_semigroup: 0.00080\n",
      "Iter 40, Loss: 0.45511, Loss_pinn: 0.43538 Loss_smooth: 0.01894, Loss_semigroup: 0.00079\n",
      "Iter 41, Loss: 0.45501, Loss_pinn: 0.43538 Loss_smooth: 0.01884, Loss_semigroup: 0.00079\n",
      "Iter 42, Loss: 0.45490, Loss_pinn: 0.43538 Loss_smooth: 0.01873, Loss_semigroup: 0.00079\n",
      "Iter 43, Loss: 0.45485, Loss_pinn: 0.43535 Loss_smooth: 0.01871, Loss_semigroup: 0.00079\n",
      "Iter 44, Loss: 0.45474, Loss_pinn: 0.43511 Loss_smooth: 0.01884, Loss_semigroup: 0.00080\n",
      "Iter 45, Loss: 0.45468, Loss_pinn: 0.43491 Loss_smooth: 0.01898, Loss_semigroup: 0.00080\n",
      "Iter 46, Loss: 0.45465, Loss_pinn: 0.43465 Loss_smooth: 0.01919, Loss_semigroup: 0.00080\n",
      "Iter 47, Loss: 0.45461, Loss_pinn: 0.43457 Loss_smooth: 0.01923, Loss_semigroup: 0.00080\n",
      "Iter 48, Loss: 0.45454, Loss_pinn: 0.43447 Loss_smooth: 0.01927, Loss_semigroup: 0.00080\n",
      "Iter 49, Loss: 0.45447, Loss_pinn: 0.43451 Loss_smooth: 0.01917, Loss_semigroup: 0.00079\n",
      "Iter 50, Loss: 0.45440, Loss_pinn: 0.43454 Loss_smooth: 0.01908, Loss_semigroup: 0.00078\n",
      "Iter 51, Loss: 0.45431, Loss_pinn: 0.43466 Loss_smooth: 0.01889, Loss_semigroup: 0.00076\n",
      "Iter 52, Loss: 0.45420, Loss_pinn: 0.43476 Loss_smooth: 0.01869, Loss_semigroup: 0.00075\n",
      "Iter 53, Loss: 0.45412, Loss_pinn: 0.43473 Loss_smooth: 0.01864, Loss_semigroup: 0.00075\n",
      "\n",
      "Training run 3\n",
      "=> loading checkpoint 'model_center_restricted_ivp'\n",
      "=> loaded checkpoint 'model_center_restricted_ivp'\n",
      "Iter 1, Loss: 0.45412, Loss_pinn: 0.43473 Loss_smooth: 0.01864, Loss_semigroup: 0.00075\n",
      "Iter 2, Loss: 0.45404, Loss_pinn: 0.43478 Loss_smooth: 0.01851, Loss_semigroup: 0.00075\n",
      "Iter 3, Loss: 0.45386, Loss_pinn: 0.43450 Loss_smooth: 0.01861, Loss_semigroup: 0.00075\n",
      "Iter 4, Loss: 0.45366, Loss_pinn: 0.43431 Loss_smooth: 0.01860, Loss_semigroup: 0.00076\n",
      "Iter 5, Loss: 0.45342, Loss_pinn: 0.43406 Loss_smooth: 0.01860, Loss_semigroup: 0.00076\n",
      "Iter 6, Loss: 0.46605, Loss_pinn: 0.44660 Loss_smooth: 0.01873, Loss_semigroup: 0.00072\n",
      "Iter 7, Loss: 0.45334, Loss_pinn: 0.43415 Loss_smooth: 0.01843, Loss_semigroup: 0.00075\n",
      "Iter 8, Loss: 0.45298, Loss_pinn: 0.43383 Loss_smooth: 0.01840, Loss_semigroup: 0.00075\n",
      "Iter 9, Loss: 0.45250, Loss_pinn: 0.43335 Loss_smooth: 0.01840, Loss_semigroup: 0.00075\n",
      "Iter 10, Loss: 0.45202, Loss_pinn: 0.43295 Loss_smooth: 0.01832, Loss_semigroup: 0.00074\n",
      "Iter 11, Loss: 0.45159, Loss_pinn: 0.43262 Loss_smooth: 0.01821, Loss_semigroup: 0.00076\n",
      "Iter 12, Loss: 0.45111, Loss_pinn: 0.43214 Loss_smooth: 0.01821, Loss_semigroup: 0.00076\n",
      "Iter 13, Loss: 0.45050, Loss_pinn: 0.43150 Loss_smooth: 0.01824, Loss_semigroup: 0.00077\n",
      "Iter 14, Loss: 0.45027, Loss_pinn: 0.43090 Loss_smooth: 0.01860, Loss_semigroup: 0.00077\n",
      "Iter 15, Loss: 0.44997, Loss_pinn: 0.43069 Loss_smooth: 0.01849, Loss_semigroup: 0.00078\n",
      "Iter 16, Loss: 0.44978, Loss_pinn: 0.43059 Loss_smooth: 0.01840, Loss_semigroup: 0.00078\n",
      "Iter 17, Loss: 0.44954, Loss_pinn: 0.43033 Loss_smooth: 0.01844, Loss_semigroup: 0.00078\n",
      "Iter 18, Loss: 0.44931, Loss_pinn: 0.43010 Loss_smooth: 0.01843, Loss_semigroup: 0.00078\n",
      "Iter 19, Loss: 0.44982, Loss_pinn: 0.43001 Loss_smooth: 0.01903, Loss_semigroup: 0.00077\n",
      "Iter 20, Loss: 0.44918, Loss_pinn: 0.42983 Loss_smooth: 0.01857, Loss_semigroup: 0.00078\n",
      "Iter 21, Loss: 0.44891, Loss_pinn: 0.42952 Loss_smooth: 0.01861, Loss_semigroup: 0.00078\n",
      "Iter 22, Loss: 0.44855, Loss_pinn: 0.42897 Loss_smooth: 0.01880, Loss_semigroup: 0.00078\n",
      "Iter 23, Loss: 0.44834, Loss_pinn: 0.42862 Loss_smooth: 0.01895, Loss_semigroup: 0.00077\n",
      "Iter 24, Loss: 0.44897, Loss_pinn: 0.42839 Loss_smooth: 0.01979, Loss_semigroup: 0.00078\n",
      "Iter 25, Loss: 0.44825, Loss_pinn: 0.42837 Loss_smooth: 0.01911, Loss_semigroup: 0.00077\n",
      "Iter 26, Loss: 0.44814, Loss_pinn: 0.42819 Loss_smooth: 0.01918, Loss_semigroup: 0.00077\n",
      "Iter 27, Loss: 0.44803, Loss_pinn: 0.42805 Loss_smooth: 0.01922, Loss_semigroup: 0.00076\n",
      "Iter 28, Loss: 0.44794, Loss_pinn: 0.42773 Loss_smooth: 0.01945, Loss_semigroup: 0.00076\n",
      "Iter 29, Loss: 0.44781, Loss_pinn: 0.42771 Loss_smooth: 0.01934, Loss_semigroup: 0.00076\n",
      "Iter 30, Loss: 0.44755, Loss_pinn: 0.42777 Loss_smooth: 0.01902, Loss_semigroup: 0.00076\n",
      "Iter 31, Loss: 0.44743, Loss_pinn: 0.42781 Loss_smooth: 0.01886, Loss_semigroup: 0.00076\n",
      "Iter 32, Loss: 0.44732, Loss_pinn: 0.42782 Loss_smooth: 0.01875, Loss_semigroup: 0.00075\n",
      "Iter 33, Loss: 0.44715, Loss_pinn: 0.42781 Loss_smooth: 0.01860, Loss_semigroup: 0.00073\n",
      "Iter 34, Loss: 0.44697, Loss_pinn: 0.42784 Loss_smooth: 0.01841, Loss_semigroup: 0.00072\n",
      "Iter 35, Loss: 0.44680, Loss_pinn: 0.42778 Loss_smooth: 0.01831, Loss_semigroup: 0.00071\n",
      "Iter 36, Loss: 0.44667, Loss_pinn: 0.42768 Loss_smooth: 0.01827, Loss_semigroup: 0.00071\n",
      "Iter 37, Loss: 0.44653, Loss_pinn: 0.42742 Loss_smooth: 0.01840, Loss_semigroup: 0.00071\n",
      "Iter 38, Loss: 0.44637, Loss_pinn: 0.42711 Loss_smooth: 0.01854, Loss_semigroup: 0.00072\n",
      "Iter 39, Loss: 0.44625, Loss_pinn: 0.42674 Loss_smooth: 0.01878, Loss_semigroup: 0.00073\n",
      "Iter 40, Loss: 0.44615, Loss_pinn: 0.42662 Loss_smooth: 0.01879, Loss_semigroup: 0.00074\n",
      "Iter 41, Loss: 0.44601, Loss_pinn: 0.42632 Loss_smooth: 0.01895, Loss_semigroup: 0.00074\n",
      "Iter 42, Loss: 0.44587, Loss_pinn: 0.42622 Loss_smooth: 0.01891, Loss_semigroup: 0.00074\n",
      "Iter 43, Loss: 0.44558, Loss_pinn: 0.42609 Loss_smooth: 0.01876, Loss_semigroup: 0.00074\n",
      "Iter 44, Loss: 0.44547, Loss_pinn: 0.42586 Loss_smooth: 0.01887, Loss_semigroup: 0.00074\n",
      "Iter 45, Loss: 0.45298, Loss_pinn: 0.43109 Loss_smooth: 0.02106, Loss_semigroup: 0.00084\n",
      "Iter 46, Loss: 0.44538, Loss_pinn: 0.42580 Loss_smooth: 0.01883, Loss_semigroup: 0.00075\n",
      "Iter 47, Loss: 0.44524, Loss_pinn: 0.42578 Loss_smooth: 0.01870, Loss_semigroup: 0.00076\n",
      "Iter 48, Loss: 0.44511, Loss_pinn: 0.42553 Loss_smooth: 0.01881, Loss_semigroup: 0.00076\n",
      "Iter 49, Loss: 0.44497, Loss_pinn: 0.42535 Loss_smooth: 0.01886, Loss_semigroup: 0.00077\n",
      "Iter 50, Loss: 0.44483, Loss_pinn: 0.42509 Loss_smooth: 0.01897, Loss_semigroup: 0.00077\n",
      "Iter 51, Loss: 0.44838, Loss_pinn: 0.42697 Loss_smooth: 0.02054, Loss_semigroup: 0.00086\n",
      "Iter 52, Loss: 0.44478, Loss_pinn: 0.42491 Loss_smooth: 0.01909, Loss_semigroup: 0.00078\n",
      "Iter 53, Loss: 0.44453, Loss_pinn: 0.42451 Loss_smooth: 0.01922, Loss_semigroup: 0.00080\n",
      "Iter 54, Loss: 0.44417, Loss_pinn: 0.42438 Loss_smooth: 0.01900, Loss_semigroup: 0.00079\n",
      "Iter 55, Loss: 0.44390, Loss_pinn: 0.42434 Loss_smooth: 0.01879, Loss_semigroup: 0.00077\n",
      "Iter 56, Loss: 0.44362, Loss_pinn: 0.42428 Loss_smooth: 0.01858, Loss_semigroup: 0.00076\n",
      "\n",
      "Training run 4\n",
      "=> loading checkpoint 'model_center_restricted_ivp'\n",
      "=> loaded checkpoint 'model_center_restricted_ivp'\n",
      "Iter 1, Loss: 0.44362, Loss_pinn: 0.42428 Loss_smooth: 0.01858, Loss_semigroup: 0.00076\n",
      "Iter 2, Loss: 0.44345, Loss_pinn: 0.42441 Loss_smooth: 0.01831, Loss_semigroup: 0.00074\n",
      "Iter 3, Loss: 0.44330, Loss_pinn: 0.42423 Loss_smooth: 0.01833, Loss_semigroup: 0.00074\n",
      "Iter 4, Loss: 0.44319, Loss_pinn: 0.42412 Loss_smooth: 0.01832, Loss_semigroup: 0.00074\n",
      "Iter 5, Loss: 0.44304, Loss_pinn: 0.42400 Loss_smooth: 0.01829, Loss_semigroup: 0.00075\n",
      "Iter 6, Loss: 0.44291, Loss_pinn: 0.42392 Loss_smooth: 0.01825, Loss_semigroup: 0.00074\n",
      "Iter 7, Loss: 0.44277, Loss_pinn: 0.42385 Loss_smooth: 0.01819, Loss_semigroup: 0.00073\n",
      "Iter 8, Loss: 0.44267, Loss_pinn: 0.42365 Loss_smooth: 0.01829, Loss_semigroup: 0.00073\n",
      "Iter 9, Loss: 0.44259, Loss_pinn: 0.42349 Loss_smooth: 0.01838, Loss_semigroup: 0.00073\n",
      "Iter 10, Loss: 0.44254, Loss_pinn: 0.42333 Loss_smooth: 0.01849, Loss_semigroup: 0.00073\n",
      "Iter 11, Loss: 0.44247, Loss_pinn: 0.42318 Loss_smooth: 0.01855, Loss_semigroup: 0.00073\n",
      "Iter 12, Loss: 0.44235, Loss_pinn: 0.42309 Loss_smooth: 0.01853, Loss_semigroup: 0.00073\n",
      "Iter 13, Loss: 0.44226, Loss_pinn: 0.42316 Loss_smooth: 0.01837, Loss_semigroup: 0.00073\n",
      "Iter 14, Loss: 0.44218, Loss_pinn: 0.42334 Loss_smooth: 0.01811, Loss_semigroup: 0.00073\n",
      "Iter 15, Loss: 0.44210, Loss_pinn: 0.42346 Loss_smooth: 0.01792, Loss_semigroup: 0.00073\n",
      "Iter 16, Loss: 0.44308, Loss_pinn: 0.42588 Loss_smooth: 0.01652, Loss_semigroup: 0.00067\n",
      "Iter 17, Loss: 0.44204, Loss_pinn: 0.42372 Loss_smooth: 0.01760, Loss_semigroup: 0.00072\n",
      "Iter 18, Loss: 0.44187, Loss_pinn: 0.42374 Loss_smooth: 0.01742, Loss_semigroup: 0.00071\n",
      "Iter 19, Loss: 0.44163, Loss_pinn: 0.42358 Loss_smooth: 0.01733, Loss_semigroup: 0.00071\n",
      "Iter 20, Loss: 0.44148, Loss_pinn: 0.42349 Loss_smooth: 0.01728, Loss_semigroup: 0.00072\n",
      "Iter 21, Loss: 0.44133, Loss_pinn: 0.42327 Loss_smooth: 0.01735, Loss_semigroup: 0.00072\n",
      "Iter 22, Loss: 0.44123, Loss_pinn: 0.42293 Loss_smooth: 0.01756, Loss_semigroup: 0.00073\n",
      "Iter 23, Loss: 0.44114, Loss_pinn: 0.42279 Loss_smooth: 0.01762, Loss_semigroup: 0.00073\n",
      "Iter 24, Loss: 0.44107, Loss_pinn: 0.42274 Loss_smooth: 0.01761, Loss_semigroup: 0.00072\n",
      "Iter 25, Loss: 0.44104, Loss_pinn: 0.42266 Loss_smooth: 0.01766, Loss_semigroup: 0.00072\n",
      "Iter 26, Loss: 0.44103, Loss_pinn: 0.42264 Loss_smooth: 0.01767, Loss_semigroup: 0.00072\n",
      "Iter 27, Loss: 0.44099, Loss_pinn: 0.42260 Loss_smooth: 0.01768, Loss_semigroup: 0.00072\n",
      "Iter 28, Loss: 0.44313, Loss_pinn: 0.42374 Loss_smooth: 0.01869, Loss_semigroup: 0.00070\n",
      "Iter 29, Loss: 0.44097, Loss_pinn: 0.42256 Loss_smooth: 0.01769, Loss_semigroup: 0.00072\n",
      "Iter 30, Loss: 0.44090, Loss_pinn: 0.42250 Loss_smooth: 0.01768, Loss_semigroup: 0.00072\n",
      "Iter 31, Loss: 0.44092, Loss_pinn: 0.42207 Loss_smooth: 0.01812, Loss_semigroup: 0.00073\n",
      "Iter 32, Loss: 0.44081, Loss_pinn: 0.42223 Loss_smooth: 0.01785, Loss_semigroup: 0.00073\n",
      "Iter 33, Loss: 0.44063, Loss_pinn: 0.42205 Loss_smooth: 0.01784, Loss_semigroup: 0.00074\n",
      "Iter 34, Loss: 0.44047, Loss_pinn: 0.42185 Loss_smooth: 0.01787, Loss_semigroup: 0.00074\n",
      "Iter 35, Loss: 0.44028, Loss_pinn: 0.42163 Loss_smooth: 0.01790, Loss_semigroup: 0.00075\n",
      "Iter 36, Loss: 0.44008, Loss_pinn: 0.42121 Loss_smooth: 0.01811, Loss_semigroup: 0.00076\n",
      "Iter 37, Loss: 0.43993, Loss_pinn: 0.42085 Loss_smooth: 0.01833, Loss_semigroup: 0.00075\n",
      "Iter 38, Loss: 0.43980, Loss_pinn: 0.42069 Loss_smooth: 0.01837, Loss_semigroup: 0.00074\n",
      "Iter 39, Loss: 0.43965, Loss_pinn: 0.42060 Loss_smooth: 0.01831, Loss_semigroup: 0.00074\n",
      "Iter 40, Loss: 0.43946, Loss_pinn: 0.42055 Loss_smooth: 0.01818, Loss_semigroup: 0.00073\n",
      "Iter 41, Loss: 0.43918, Loss_pinn: 0.42040 Loss_smooth: 0.01806, Loss_semigroup: 0.00072\n",
      "Iter 42, Loss: 0.43895, Loss_pinn: 0.42017 Loss_smooth: 0.01805, Loss_semigroup: 0.00073\n",
      "Iter 43, Loss: 0.43879, Loss_pinn: 0.41994 Loss_smooth: 0.01811, Loss_semigroup: 0.00074\n",
      "Iter 44, Loss: 0.43871, Loss_pinn: 0.41980 Loss_smooth: 0.01816, Loss_semigroup: 0.00075\n",
      "Iter 45, Loss: 0.43863, Loss_pinn: 0.41958 Loss_smooth: 0.01829, Loss_semigroup: 0.00076\n",
      "Iter 46, Loss: 0.43848, Loss_pinn: 0.41971 Loss_smooth: 0.01800, Loss_semigroup: 0.00077\n",
      "Iter 47, Loss: 0.43831, Loss_pinn: 0.41958 Loss_smooth: 0.01798, Loss_semigroup: 0.00076\n",
      "Iter 48, Loss: 0.43811, Loss_pinn: 0.41945 Loss_smooth: 0.01791, Loss_semigroup: 0.00075\n",
      "Iter 49, Loss: 0.43793, Loss_pinn: 0.41913 Loss_smooth: 0.01804, Loss_semigroup: 0.00076\n",
      "Iter 50, Loss: 0.43983, Loss_pinn: 0.41971 Loss_smooth: 0.01937, Loss_semigroup: 0.00075\n",
      "Iter 51, Loss: 0.43783, Loss_pinn: 0.41891 Loss_smooth: 0.01816, Loss_semigroup: 0.00076\n",
      "Iter 52, Loss: 0.43751, Loss_pinn: 0.41859 Loss_smooth: 0.01816, Loss_semigroup: 0.00076\n",
      "Iter 53, Loss: 0.43724, Loss_pinn: 0.41835 Loss_smooth: 0.01813, Loss_semigroup: 0.00076\n",
      "Iter 54, Loss: 0.43698, Loss_pinn: 0.41815 Loss_smooth: 0.01807, Loss_semigroup: 0.00076\n",
      "Iter 55, Loss: 0.43680, Loss_pinn: 0.41801 Loss_smooth: 0.01803, Loss_semigroup: 0.00076\n",
      "\n",
      "Training run 5\n",
      "=> loading checkpoint 'model_center_restricted_ivp'\n",
      "=> loaded checkpoint 'model_center_restricted_ivp'\n",
      "Iter 1, Loss: 0.43680, Loss_pinn: 0.41801 Loss_smooth: 0.01803, Loss_semigroup: 0.00076\n",
      "Iter 2, Loss: 0.43655, Loss_pinn: 0.41777 Loss_smooth: 0.01804, Loss_semigroup: 0.00074\n",
      "Iter 3, Loss: 0.43636, Loss_pinn: 0.41750 Loss_smooth: 0.01812, Loss_semigroup: 0.00074\n",
      "Iter 4, Loss: 0.43612, Loss_pinn: 0.41712 Loss_smooth: 0.01826, Loss_semigroup: 0.00074\n",
      "Iter 5, Loss: 0.43591, Loss_pinn: 0.41681 Loss_smooth: 0.01836, Loss_semigroup: 0.00074\n",
      "Iter 6, Loss: 0.43576, Loss_pinn: 0.41640 Loss_smooth: 0.01860, Loss_semigroup: 0.00076\n",
      "Iter 7, Loss: 0.43566, Loss_pinn: 0.41656 Loss_smooth: 0.01833, Loss_semigroup: 0.00078\n",
      "Iter 8, Loss: 0.43558, Loss_pinn: 0.41650 Loss_smooth: 0.01831, Loss_semigroup: 0.00078\n",
      "Iter 9, Loss: 0.43553, Loss_pinn: 0.41639 Loss_smooth: 0.01836, Loss_semigroup: 0.00078\n",
      "Iter 10, Loss: 0.43548, Loss_pinn: 0.41637 Loss_smooth: 0.01833, Loss_semigroup: 0.00078\n",
      "Iter 11, Loss: 0.43552, Loss_pinn: 0.41643 Loss_smooth: 0.01828, Loss_semigroup: 0.00081\n",
      "Iter 12, Loss: 0.43542, Loss_pinn: 0.41634 Loss_smooth: 0.01829, Loss_semigroup: 0.00079\n",
      "Iter 13, Loss: 0.43532, Loss_pinn: 0.41625 Loss_smooth: 0.01827, Loss_semigroup: 0.00079\n",
      "Iter 14, Loss: 0.43523, Loss_pinn: 0.41620 Loss_smooth: 0.01823, Loss_semigroup: 0.00079\n",
      "Iter 15, Loss: 0.43517, Loss_pinn: 0.41617 Loss_smooth: 0.01821, Loss_semigroup: 0.00079\n",
      "Iter 16, Loss: 0.43512, Loss_pinn: 0.41614 Loss_smooth: 0.01819, Loss_semigroup: 0.00080\n",
      "Iter 17, Loss: 0.43507, Loss_pinn: 0.41605 Loss_smooth: 0.01822, Loss_semigroup: 0.00080\n",
      "Iter 18, Loss: 0.43500, Loss_pinn: 0.41592 Loss_smooth: 0.01828, Loss_semigroup: 0.00080\n",
      "Iter 19, Loss: 0.43489, Loss_pinn: 0.41574 Loss_smooth: 0.01834, Loss_semigroup: 0.00080\n",
      "Iter 20, Loss: 0.43526, Loss_pinn: 0.41590 Loss_smooth: 0.01857, Loss_semigroup: 0.00080\n",
      "Iter 21, Loss: 0.43481, Loss_pinn: 0.41565 Loss_smooth: 0.01836, Loss_semigroup: 0.00080\n",
      "Iter 22, Loss: 0.43465, Loss_pinn: 0.41544 Loss_smooth: 0.01840, Loss_semigroup: 0.00081\n",
      "Iter 23, Loss: 0.43437, Loss_pinn: 0.41526 Loss_smooth: 0.01830, Loss_semigroup: 0.00081\n",
      "Iter 24, Loss: 0.43407, Loss_pinn: 0.41483 Loss_smooth: 0.01844, Loss_semigroup: 0.00081\n",
      "Iter 25, Loss: 0.43377, Loss_pinn: 0.41491 Loss_smooth: 0.01806, Loss_semigroup: 0.00080\n",
      "Iter 26, Loss: 0.43353, Loss_pinn: 0.41463 Loss_smooth: 0.01811, Loss_semigroup: 0.00079\n",
      "Iter 27, Loss: 0.43340, Loss_pinn: 0.41445 Loss_smooth: 0.01817, Loss_semigroup: 0.00079\n",
      "Iter 28, Loss: 0.43330, Loss_pinn: 0.41429 Loss_smooth: 0.01823, Loss_semigroup: 0.00079\n",
      "Iter 29, Loss: 0.43328, Loss_pinn: 0.41402 Loss_smooth: 0.01848, Loss_semigroup: 0.00079\n",
      "Iter 30, Loss: 0.43320, Loss_pinn: 0.41404 Loss_smooth: 0.01838, Loss_semigroup: 0.00079\n",
      "Iter 31, Loss: 0.43315, Loss_pinn: 0.41402 Loss_smooth: 0.01835, Loss_semigroup: 0.00079\n",
      "Iter 32, Loss: 0.43306, Loss_pinn: 0.41399 Loss_smooth: 0.01828, Loss_semigroup: 0.00079\n",
      "Iter 33, Loss: 0.43298, Loss_pinn: 0.41389 Loss_smooth: 0.01830, Loss_semigroup: 0.00079\n",
      "Iter 34, Loss: 0.43317, Loss_pinn: 0.41436 Loss_smooth: 0.01800, Loss_semigroup: 0.00080\n",
      "Iter 35, Loss: 0.43296, Loss_pinn: 0.41395 Loss_smooth: 0.01822, Loss_semigroup: 0.00079\n",
      "Iter 36, Loss: 0.43285, Loss_pinn: 0.41382 Loss_smooth: 0.01823, Loss_semigroup: 0.00079\n",
      "Iter 37, Loss: 0.43275, Loss_pinn: 0.41386 Loss_smooth: 0.01810, Loss_semigroup: 0.00079\n",
      "Iter 38, Loss: 0.43263, Loss_pinn: 0.41383 Loss_smooth: 0.01801, Loss_semigroup: 0.00080\n",
      "Iter 39, Loss: 0.43254, Loss_pinn: 0.41378 Loss_smooth: 0.01796, Loss_semigroup: 0.00080\n",
      "Iter 40, Loss: 0.43246, Loss_pinn: 0.41365 Loss_smooth: 0.01800, Loss_semigroup: 0.00080\n",
      "Iter 41, Loss: 0.43248, Loss_pinn: 0.41371 Loss_smooth: 0.01797, Loss_semigroup: 0.00080\n",
      "Iter 42, Loss: 0.43243, Loss_pinn: 0.41365 Loss_smooth: 0.01798, Loss_semigroup: 0.00080\n",
      "Iter 43, Loss: 0.43237, Loss_pinn: 0.41350 Loss_smooth: 0.01807, Loss_semigroup: 0.00080\n",
      "Iter 44, Loss: 0.43231, Loss_pinn: 0.41334 Loss_smooth: 0.01817, Loss_semigroup: 0.00081\n",
      "Iter 45, Loss: 0.43225, Loss_pinn: 0.41325 Loss_smooth: 0.01819, Loss_semigroup: 0.00081\n",
      "Iter 46, Loss: 0.43217, Loss_pinn: 0.41311 Loss_smooth: 0.01825, Loss_semigroup: 0.00081\n",
      "Iter 47, Loss: 0.43201, Loss_pinn: 0.41296 Loss_smooth: 0.01823, Loss_semigroup: 0.00082\n",
      "Iter 48, Loss: 0.43181, Loss_pinn: 0.41282 Loss_smooth: 0.01817, Loss_semigroup: 0.00082\n",
      "Iter 49, Loss: 0.43159, Loss_pinn: 0.41269 Loss_smooth: 0.01808, Loss_semigroup: 0.00082\n",
      "Iter 50, Loss: 0.43352, Loss_pinn: 0.41528 Loss_smooth: 0.01748, Loss_semigroup: 0.00076\n",
      "Iter 51, Loss: 0.43151, Loss_pinn: 0.41282 Loss_smooth: 0.01789, Loss_semigroup: 0.00081\n",
      "Iter 52, Loss: 0.43135, Loss_pinn: 0.41270 Loss_smooth: 0.01785, Loss_semigroup: 0.00080\n",
      "Iter 53, Loss: 0.43117, Loss_pinn: 0.41282 Loss_smooth: 0.01756, Loss_semigroup: 0.00079\n",
      "Iter 54, Loss: 0.43106, Loss_pinn: 0.41269 Loss_smooth: 0.01758, Loss_semigroup: 0.00078\n",
      "Iter 55, Loss: 0.43094, Loss_pinn: 0.41264 Loss_smooth: 0.01753, Loss_semigroup: 0.00077\n",
      "Iter 56, Loss: 0.43084, Loss_pinn: 0.41253 Loss_smooth: 0.01755, Loss_semigroup: 0.00077\n",
      "\n",
      "Training run 6\n",
      "=> loading checkpoint 'model_center_restricted_ivp'\n",
      "=> loaded checkpoint 'model_center_restricted_ivp'\n",
      "Iter 1, Loss: 0.43084, Loss_pinn: 0.41253 Loss_smooth: 0.01755, Loss_semigroup: 0.00077\n",
      "Iter 2, Loss: 0.43078, Loss_pinn: 0.41236 Loss_smooth: 0.01765, Loss_semigroup: 0.00077\n",
      "Iter 3, Loss: 0.43072, Loss_pinn: 0.41223 Loss_smooth: 0.01772, Loss_semigroup: 0.00078\n",
      "Iter 4, Loss: 0.43067, Loss_pinn: 0.41208 Loss_smooth: 0.01781, Loss_semigroup: 0.00078\n",
      "Iter 5, Loss: 0.43061, Loss_pinn: 0.41202 Loss_smooth: 0.01781, Loss_semigroup: 0.00079\n",
      "Iter 6, Loss: 0.43052, Loss_pinn: 0.41187 Loss_smooth: 0.01786, Loss_semigroup: 0.00079\n",
      "Iter 7, Loss: 0.43039, Loss_pinn: 0.41173 Loss_smooth: 0.01787, Loss_semigroup: 0.00079\n",
      "Iter 8, Loss: 0.43024, Loss_pinn: 0.41169 Loss_smooth: 0.01778, Loss_semigroup: 0.00078\n",
      "Iter 9, Loss: 0.43048, Loss_pinn: 0.41188 Loss_smooth: 0.01781, Loss_semigroup: 0.00078\n",
      "Iter 10, Loss: 0.43018, Loss_pinn: 0.41163 Loss_smooth: 0.01777, Loss_semigroup: 0.00078\n",
      "Iter 11, Loss: 0.43003, Loss_pinn: 0.41156 Loss_smooth: 0.01769, Loss_semigroup: 0.00078\n",
      "Iter 12, Loss: 0.42990, Loss_pinn: 0.41148 Loss_smooth: 0.01764, Loss_semigroup: 0.00078\n",
      "Iter 13, Loss: 0.42981, Loss_pinn: 0.41149 Loss_smooth: 0.01753, Loss_semigroup: 0.00078\n",
      "Iter 14, Loss: 0.42974, Loss_pinn: 0.41136 Loss_smooth: 0.01759, Loss_semigroup: 0.00078\n",
      "Iter 15, Loss: 0.42965, Loss_pinn: 0.41126 Loss_smooth: 0.01760, Loss_semigroup: 0.00079\n",
      "Iter 16, Loss: 0.42960, Loss_pinn: 0.41117 Loss_smooth: 0.01765, Loss_semigroup: 0.00079\n",
      "Iter 17, Loss: 0.42951, Loss_pinn: 0.41100 Loss_smooth: 0.01772, Loss_semigroup: 0.00079\n",
      "Iter 18, Loss: 0.42944, Loss_pinn: 0.41091 Loss_smooth: 0.01773, Loss_semigroup: 0.00079\n",
      "Iter 19, Loss: 0.44083, Loss_pinn: 0.41949 Loss_smooth: 0.02045, Loss_semigroup: 0.00090\n",
      "Iter 20, Loss: 0.42940, Loss_pinn: 0.41088 Loss_smooth: 0.01773, Loss_semigroup: 0.00080\n",
      "Iter 21, Loss: 0.42934, Loss_pinn: 0.41078 Loss_smooth: 0.01776, Loss_semigroup: 0.00080\n",
      "Iter 22, Loss: 0.42923, Loss_pinn: 0.41065 Loss_smooth: 0.01778, Loss_semigroup: 0.00080\n",
      "Iter 23, Loss: 0.42916, Loss_pinn: 0.41052 Loss_smooth: 0.01784, Loss_semigroup: 0.00080\n",
      "Iter 24, Loss: 0.42908, Loss_pinn: 0.41042 Loss_smooth: 0.01786, Loss_semigroup: 0.00081\n",
      "Iter 25, Loss: 0.42903, Loss_pinn: 0.41037 Loss_smooth: 0.01785, Loss_semigroup: 0.00081\n",
      "Iter 26, Loss: 0.42898, Loss_pinn: 0.41032 Loss_smooth: 0.01785, Loss_semigroup: 0.00081\n",
      "Iter 27, Loss: 0.42893, Loss_pinn: 0.41028 Loss_smooth: 0.01783, Loss_semigroup: 0.00081\n",
      "Iter 28, Loss: 0.42886, Loss_pinn: 0.41013 Loss_smooth: 0.01791, Loss_semigroup: 0.00081\n",
      "Iter 29, Loss: 0.42877, Loss_pinn: 0.40994 Loss_smooth: 0.01801, Loss_semigroup: 0.00082\n",
      "Iter 30, Loss: 0.42864, Loss_pinn: 0.40979 Loss_smooth: 0.01804, Loss_semigroup: 0.00082\n",
      "Iter 31, Loss: 0.42851, Loss_pinn: 0.40960 Loss_smooth: 0.01810, Loss_semigroup: 0.00082\n",
      "Iter 32, Loss: 0.42840, Loss_pinn: 0.40946 Loss_smooth: 0.01814, Loss_semigroup: 0.00081\n",
      "Iter 33, Loss: 0.42830, Loss_pinn: 0.40933 Loss_smooth: 0.01816, Loss_semigroup: 0.00081\n",
      "Iter 34, Loss: 0.42825, Loss_pinn: 0.40936 Loss_smooth: 0.01808, Loss_semigroup: 0.00081\n",
      "Iter 35, Loss: 0.42820, Loss_pinn: 0.40928 Loss_smooth: 0.01811, Loss_semigroup: 0.00081\n",
      "Iter 36, Loss: 0.42815, Loss_pinn: 0.40924 Loss_smooth: 0.01810, Loss_semigroup: 0.00082\n",
      "Iter 37, Loss: 0.42810, Loss_pinn: 0.40916 Loss_smooth: 0.01813, Loss_semigroup: 0.00082\n",
      "Iter 38, Loss: 0.42805, Loss_pinn: 0.40909 Loss_smooth: 0.01814, Loss_semigroup: 0.00082\n",
      "Iter 39, Loss: 0.42813, Loss_pinn: 0.40920 Loss_smooth: 0.01811, Loss_semigroup: 0.00082\n",
      "Iter 40, Loss: 0.42801, Loss_pinn: 0.40908 Loss_smooth: 0.01811, Loss_semigroup: 0.00082\n",
      "Iter 41, Loss: 0.42792, Loss_pinn: 0.40894 Loss_smooth: 0.01814, Loss_semigroup: 0.00084\n",
      "Iter 42, Loss: 0.42775, Loss_pinn: 0.40889 Loss_smooth: 0.01803, Loss_semigroup: 0.00083\n",
      "Iter 43, Loss: 0.42759, Loss_pinn: 0.40884 Loss_smooth: 0.01792, Loss_semigroup: 0.00083\n",
      "Iter 44, Loss: 0.42743, Loss_pinn: 0.40876 Loss_smooth: 0.01784, Loss_semigroup: 0.00083\n",
      "Iter 45, Loss: 0.42732, Loss_pinn: 0.40871 Loss_smooth: 0.01779, Loss_semigroup: 0.00083\n",
      "Iter 46, Loss: 0.42721, Loss_pinn: 0.40858 Loss_smooth: 0.01780, Loss_semigroup: 0.00083\n",
      "Iter 47, Loss: 0.42722, Loss_pinn: 0.40883 Loss_smooth: 0.01758, Loss_semigroup: 0.00081\n",
      "Iter 48, Loss: 0.42715, Loss_pinn: 0.40866 Loss_smooth: 0.01768, Loss_semigroup: 0.00082\n",
      "Iter 49, Loss: 0.42705, Loss_pinn: 0.40855 Loss_smooth: 0.01769, Loss_semigroup: 0.00082\n",
      "Iter 50, Loss: 0.42696, Loss_pinn: 0.40848 Loss_smooth: 0.01767, Loss_semigroup: 0.00081\n",
      "Iter 51, Loss: 0.42692, Loss_pinn: 0.40848 Loss_smooth: 0.01764, Loss_semigroup: 0.00080\n",
      "Iter 52, Loss: 0.42687, Loss_pinn: 0.40847 Loss_smooth: 0.01760, Loss_semigroup: 0.00080\n",
      "Iter 53, Loss: 0.42680, Loss_pinn: 0.40842 Loss_smooth: 0.01759, Loss_semigroup: 0.00079\n",
      "Iter 54, Loss: 0.42676, Loss_pinn: 0.40835 Loss_smooth: 0.01761, Loss_semigroup: 0.00079\n",
      "Iter 55, Loss: 0.42696, Loss_pinn: 0.40853 Loss_smooth: 0.01765, Loss_semigroup: 0.00078\n",
      "Iter 56, Loss: 0.42674, Loss_pinn: 0.40835 Loss_smooth: 0.01760, Loss_semigroup: 0.00079\n",
      "\n",
      "Training run 7\n",
      "=> loading checkpoint 'model_center_restricted_ivp'\n",
      "=> loaded checkpoint 'model_center_restricted_ivp'\n",
      "Iter 1, Loss: 0.42674, Loss_pinn: 0.40835 Loss_smooth: 0.01760, Loss_semigroup: 0.00079\n",
      "Iter 2, Loss: 0.42694, Loss_pinn: 0.40826 Loss_smooth: 0.01791, Loss_semigroup: 0.00077\n",
      "Iter 3, Loss: 0.42668, Loss_pinn: 0.40823 Loss_smooth: 0.01767, Loss_semigroup: 0.00079\n",
      "Iter 4, Loss: 0.42660, Loss_pinn: 0.40816 Loss_smooth: 0.01764, Loss_semigroup: 0.00079\n",
      "Iter 5, Loss: 0.42653, Loss_pinn: 0.40804 Loss_smooth: 0.01769, Loss_semigroup: 0.00080\n",
      "Iter 6, Loss: 0.42642, Loss_pinn: 0.40788 Loss_smooth: 0.01773, Loss_semigroup: 0.00080\n",
      "Iter 7, Loss: 0.42642, Loss_pinn: 0.40759 Loss_smooth: 0.01802, Loss_semigroup: 0.00081\n",
      "Iter 8, Loss: 0.42637, Loss_pinn: 0.40770 Loss_smooth: 0.01786, Loss_semigroup: 0.00081\n",
      "Iter 9, Loss: 0.42627, Loss_pinn: 0.40764 Loss_smooth: 0.01782, Loss_semigroup: 0.00081\n",
      "Iter 10, Loss: 0.42621, Loss_pinn: 0.40763 Loss_smooth: 0.01777, Loss_semigroup: 0.00081\n",
      "Iter 11, Loss: 0.42617, Loss_pinn: 0.40770 Loss_smooth: 0.01766, Loss_semigroup: 0.00081\n",
      "Iter 12, Loss: 0.42614, Loss_pinn: 0.40767 Loss_smooth: 0.01766, Loss_semigroup: 0.00081\n",
      "Iter 13, Loss: 0.42612, Loss_pinn: 0.40763 Loss_smooth: 0.01768, Loss_semigroup: 0.00081\n",
      "Iter 14, Loss: 0.42709, Loss_pinn: 0.40801 Loss_smooth: 0.01826, Loss_semigroup: 0.00081\n",
      "Iter 15, Loss: 0.42610, Loss_pinn: 0.40758 Loss_smooth: 0.01771, Loss_semigroup: 0.00081\n",
      "Iter 16, Loss: 0.42607, Loss_pinn: 0.40753 Loss_smooth: 0.01772, Loss_semigroup: 0.00081\n",
      "Iter 17, Loss: 0.42605, Loss_pinn: 0.40748 Loss_smooth: 0.01775, Loss_semigroup: 0.00081\n",
      "Iter 18, Loss: 0.42603, Loss_pinn: 0.40744 Loss_smooth: 0.01778, Loss_semigroup: 0.00081\n",
      "Iter 19, Loss: 0.42601, Loss_pinn: 0.40744 Loss_smooth: 0.01776, Loss_semigroup: 0.00081\n",
      "Iter 20, Loss: 0.42598, Loss_pinn: 0.40743 Loss_smooth: 0.01774, Loss_semigroup: 0.00080\n",
      "Iter 21, Loss: 0.42594, Loss_pinn: 0.40743 Loss_smooth: 0.01771, Loss_semigroup: 0.00080\n",
      "Iter 22, Loss: 0.42590, Loss_pinn: 0.40739 Loss_smooth: 0.01771, Loss_semigroup: 0.00080\n",
      "Iter 23, Loss: 0.42588, Loss_pinn: 0.40739 Loss_smooth: 0.01769, Loss_semigroup: 0.00080\n",
      "Iter 24, Loss: 0.42586, Loss_pinn: 0.40737 Loss_smooth: 0.01769, Loss_semigroup: 0.00080\n",
      "Iter 25, Loss: 0.42583, Loss_pinn: 0.40734 Loss_smooth: 0.01769, Loss_semigroup: 0.00080\n",
      "Iter 26, Loss: 0.42580, Loss_pinn: 0.40733 Loss_smooth: 0.01766, Loss_semigroup: 0.00080\n",
      "Iter 27, Loss: 0.42573, Loss_pinn: 0.40734 Loss_smooth: 0.01759, Loss_semigroup: 0.00080\n",
      "Iter 28, Loss: 0.42563, Loss_pinn: 0.40741 Loss_smooth: 0.01742, Loss_semigroup: 0.00079\n",
      "Iter 29, Loss: 0.42553, Loss_pinn: 0.40749 Loss_smooth: 0.01725, Loss_semigroup: 0.00078\n",
      "Iter 30, Loss: 0.42546, Loss_pinn: 0.40752 Loss_smooth: 0.01717, Loss_semigroup: 0.00077\n",
      "Iter 31, Loss: 0.42540, Loss_pinn: 0.40748 Loss_smooth: 0.01716, Loss_semigroup: 0.00077\n",
      "Iter 32, Loss: 0.42533, Loss_pinn: 0.40735 Loss_smooth: 0.01721, Loss_semigroup: 0.00077\n",
      "Iter 33, Loss: 0.42526, Loss_pinn: 0.40723 Loss_smooth: 0.01726, Loss_semigroup: 0.00076\n",
      "Iter 34, Loss: 0.42515, Loss_pinn: 0.40700 Loss_smooth: 0.01739, Loss_semigroup: 0.00076\n",
      "Iter 35, Loss: 0.42510, Loss_pinn: 0.40697 Loss_smooth: 0.01738, Loss_semigroup: 0.00075\n",
      "Iter 36, Loss: 0.42507, Loss_pinn: 0.40697 Loss_smooth: 0.01734, Loss_semigroup: 0.00075\n",
      "Iter 37, Loss: 0.42506, Loss_pinn: 0.40699 Loss_smooth: 0.01731, Loss_semigroup: 0.00076\n",
      "Iter 38, Loss: 0.42504, Loss_pinn: 0.40700 Loss_smooth: 0.01728, Loss_semigroup: 0.00075\n",
      "Iter 39, Loss: 0.42501, Loss_pinn: 0.40698 Loss_smooth: 0.01727, Loss_semigroup: 0.00075\n",
      "Iter 40, Loss: 0.42498, Loss_pinn: 0.40697 Loss_smooth: 0.01725, Loss_semigroup: 0.00075\n",
      "Iter 41, Loss: 0.42526, Loss_pinn: 0.40720 Loss_smooth: 0.01731, Loss_semigroup: 0.00075\n",
      "Iter 42, Loss: 0.42495, Loss_pinn: 0.40695 Loss_smooth: 0.01726, Loss_semigroup: 0.00075\n",
      "Iter 43, Loss: 0.42491, Loss_pinn: 0.40689 Loss_smooth: 0.01727, Loss_semigroup: 0.00075\n",
      "Iter 44, Loss: 0.42485, Loss_pinn: 0.40678 Loss_smooth: 0.01732, Loss_semigroup: 0.00075\n",
      "Iter 45, Loss: 0.42479, Loss_pinn: 0.40663 Loss_smooth: 0.01740, Loss_semigroup: 0.00076\n",
      "Iter 46, Loss: 0.42476, Loss_pinn: 0.40671 Loss_smooth: 0.01730, Loss_semigroup: 0.00075\n",
      "Iter 47, Loss: 0.42469, Loss_pinn: 0.40659 Loss_smooth: 0.01735, Loss_semigroup: 0.00076\n",
      "Iter 48, Loss: 0.42464, Loss_pinn: 0.40654 Loss_smooth: 0.01735, Loss_semigroup: 0.00075\n",
      "Iter 49, Loss: 0.42457, Loss_pinn: 0.40644 Loss_smooth: 0.01737, Loss_semigroup: 0.00075\n",
      "Iter 50, Loss: 0.42448, Loss_pinn: 0.40639 Loss_smooth: 0.01734, Loss_semigroup: 0.00075\n",
      "Iter 51, Loss: 0.42441, Loss_pinn: 0.40633 Loss_smooth: 0.01734, Loss_semigroup: 0.00074\n",
      "Iter 52, Loss: 0.42438, Loss_pinn: 0.40631 Loss_smooth: 0.01733, Loss_semigroup: 0.00074\n",
      "Iter 53, Loss: 0.42435, Loss_pinn: 0.40628 Loss_smooth: 0.01732, Loss_semigroup: 0.00075\n",
      "Iter 54, Loss: 0.42443, Loss_pinn: 0.40634 Loss_smooth: 0.01735, Loss_semigroup: 0.00074\n",
      "Iter 55, Loss: 0.42431, Loss_pinn: 0.40625 Loss_smooth: 0.01732, Loss_semigroup: 0.00074\n",
      "Iter 56, Loss: 0.42426, Loss_pinn: 0.40615 Loss_smooth: 0.01736, Loss_semigroup: 0.00075\n",
      "\n",
      "Training run 8\n",
      "=> loading checkpoint 'model_center_restricted_ivp'\n",
      "=> loaded checkpoint 'model_center_restricted_ivp'\n",
      "Iter 1, Loss: 0.42426, Loss_pinn: 0.40615 Loss_smooth: 0.01736, Loss_semigroup: 0.00075\n",
      "Iter 2, Loss: 0.42420, Loss_pinn: 0.40604 Loss_smooth: 0.01741, Loss_semigroup: 0.00075\n",
      "Iter 3, Loss: 0.42411, Loss_pinn: 0.40592 Loss_smooth: 0.01744, Loss_semigroup: 0.00075\n",
      "Iter 4, Loss: 0.42398, Loss_pinn: 0.40577 Loss_smooth: 0.01746, Loss_semigroup: 0.00076\n",
      "Iter 5, Loss: 0.42386, Loss_pinn: 0.40563 Loss_smooth: 0.01747, Loss_semigroup: 0.00076\n",
      "Iter 6, Loss: 0.42375, Loss_pinn: 0.40557 Loss_smooth: 0.01742, Loss_semigroup: 0.00075\n",
      "Iter 7, Loss: 0.42366, Loss_pinn: 0.40559 Loss_smooth: 0.01732, Loss_semigroup: 0.00075\n",
      "Iter 8, Loss: 0.42362, Loss_pinn: 0.40559 Loss_smooth: 0.01728, Loss_semigroup: 0.00074\n",
      "Iter 9, Loss: 0.42355, Loss_pinn: 0.40554 Loss_smooth: 0.01726, Loss_semigroup: 0.00074\n",
      "Iter 10, Loss: 0.42349, Loss_pinn: 0.40547 Loss_smooth: 0.01728, Loss_semigroup: 0.00074\n",
      "Iter 11, Loss: 0.42343, Loss_pinn: 0.40531 Loss_smooth: 0.01737, Loss_semigroup: 0.00074\n",
      "Iter 12, Loss: 0.42337, Loss_pinn: 0.40517 Loss_smooth: 0.01746, Loss_semigroup: 0.00075\n",
      "Iter 13, Loss: 0.42329, Loss_pinn: 0.40489 Loss_smooth: 0.01764, Loss_semigroup: 0.00076\n",
      "Iter 14, Loss: 0.42502, Loss_pinn: 0.40563 Loss_smooth: 0.01863, Loss_semigroup: 0.00076\n",
      "Iter 15, Loss: 0.42327, Loss_pinn: 0.40482 Loss_smooth: 0.01769, Loss_semigroup: 0.00076\n",
      "Iter 16, Loss: 0.42320, Loss_pinn: 0.40467 Loss_smooth: 0.01778, Loss_semigroup: 0.00075\n",
      "Iter 17, Loss: 0.42313, Loss_pinn: 0.40475 Loss_smooth: 0.01763, Loss_semigroup: 0.00075\n",
      "Iter 18, Loss: 0.42306, Loss_pinn: 0.40476 Loss_smooth: 0.01755, Loss_semigroup: 0.00075\n",
      "Iter 19, Loss: 0.42300, Loss_pinn: 0.40471 Loss_smooth: 0.01754, Loss_semigroup: 0.00075\n",
      "Iter 20, Loss: 0.42292, Loss_pinn: 0.40459 Loss_smooth: 0.01757, Loss_semigroup: 0.00076\n",
      "Iter 21, Loss: 0.42285, Loss_pinn: 0.40452 Loss_smooth: 0.01757, Loss_semigroup: 0.00076\n",
      "Iter 22, Loss: 0.42278, Loss_pinn: 0.40447 Loss_smooth: 0.01755, Loss_semigroup: 0.00077\n",
      "Iter 23, Loss: 0.42270, Loss_pinn: 0.40444 Loss_smooth: 0.01749, Loss_semigroup: 0.00077\n",
      "Iter 24, Loss: 0.42256, Loss_pinn: 0.40438 Loss_smooth: 0.01741, Loss_semigroup: 0.00077\n",
      "Iter 25, Loss: 0.42246, Loss_pinn: 0.40442 Loss_smooth: 0.01727, Loss_semigroup: 0.00077\n",
      "Iter 26, Loss: 0.42236, Loss_pinn: 0.40442 Loss_smooth: 0.01717, Loss_semigroup: 0.00077\n",
      "Iter 27, Loss: 0.42225, Loss_pinn: 0.40440 Loss_smooth: 0.01709, Loss_semigroup: 0.00076\n",
      "Iter 28, Loss: 0.42216, Loss_pinn: 0.40446 Loss_smooth: 0.01694, Loss_semigroup: 0.00075\n",
      "Iter 29, Loss: 0.42205, Loss_pinn: 0.40439 Loss_smooth: 0.01690, Loss_semigroup: 0.00075\n",
      "Iter 30, Loss: 0.42195, Loss_pinn: 0.40435 Loss_smooth: 0.01684, Loss_semigroup: 0.00076\n",
      "Iter 31, Loss: 0.42221, Loss_pinn: 0.40475 Loss_smooth: 0.01668, Loss_semigroup: 0.00078\n",
      "Iter 32, Loss: 0.42188, Loss_pinn: 0.40436 Loss_smooth: 0.01676, Loss_semigroup: 0.00076\n",
      "Iter 33, Loss: 0.42182, Loss_pinn: 0.40432 Loss_smooth: 0.01674, Loss_semigroup: 0.00076\n",
      "Iter 34, Loss: 0.42170, Loss_pinn: 0.40421 Loss_smooth: 0.01673, Loss_semigroup: 0.00076\n",
      "Iter 35, Loss: 0.42164, Loss_pinn: 0.40412 Loss_smooth: 0.01675, Loss_semigroup: 0.00076\n",
      "Iter 36, Loss: 0.42156, Loss_pinn: 0.40385 Loss_smooth: 0.01694, Loss_semigroup: 0.00077\n",
      "Iter 37, Loss: 0.42150, Loss_pinn: 0.40377 Loss_smooth: 0.01696, Loss_semigroup: 0.00077\n",
      "Iter 38, Loss: 0.42147, Loss_pinn: 0.40369 Loss_smooth: 0.01700, Loss_semigroup: 0.00078\n",
      "Iter 39, Loss: 0.42144, Loss_pinn: 0.40363 Loss_smooth: 0.01702, Loss_semigroup: 0.00079\n",
      "Iter 40, Loss: 0.43806, Loss_pinn: 0.41609 Loss_smooth: 0.02113, Loss_semigroup: 0.00083\n",
      "Iter 41, Loss: 0.42142, Loss_pinn: 0.40354 Loss_smooth: 0.01708, Loss_semigroup: 0.00080\n",
      "Iter 42, Loss: 0.42136, Loss_pinn: 0.40354 Loss_smooth: 0.01703, Loss_semigroup: 0.00080\n",
      "Iter 43, Loss: 0.42127, Loss_pinn: 0.40358 Loss_smooth: 0.01689, Loss_semigroup: 0.00080\n",
      "Iter 44, Loss: 0.42121, Loss_pinn: 0.40358 Loss_smooth: 0.01682, Loss_semigroup: 0.00080\n",
      "Iter 45, Loss: 0.42116, Loss_pinn: 0.40351 Loss_smooth: 0.01684, Loss_semigroup: 0.00081\n",
      "Iter 46, Loss: 0.42112, Loss_pinn: 0.40344 Loss_smooth: 0.01687, Loss_semigroup: 0.00081\n",
      "Iter 47, Loss: 0.42108, Loss_pinn: 0.40339 Loss_smooth: 0.01688, Loss_semigroup: 0.00081\n",
      "Iter 48, Loss: 0.42103, Loss_pinn: 0.40329 Loss_smooth: 0.01693, Loss_semigroup: 0.00081\n",
      "Iter 49, Loss: 0.42101, Loss_pinn: 0.40330 Loss_smooth: 0.01690, Loss_semigroup: 0.00081\n",
      "Iter 50, Loss: 0.42099, Loss_pinn: 0.40328 Loss_smooth: 0.01690, Loss_semigroup: 0.00081\n",
      "Iter 51, Loss: 0.42316, Loss_pinn: 0.40420 Loss_smooth: 0.01812, Loss_semigroup: 0.00084\n",
      "Iter 52, Loss: 0.42098, Loss_pinn: 0.40322 Loss_smooth: 0.01695, Loss_semigroup: 0.00081\n",
      "Iter 53, Loss: 0.42095, Loss_pinn: 0.40317 Loss_smooth: 0.01696, Loss_semigroup: 0.00082\n",
      "Iter 54, Loss: 0.42089, Loss_pinn: 0.40308 Loss_smooth: 0.01699, Loss_semigroup: 0.00082\n",
      "Iter 55, Loss: 0.42083, Loss_pinn: 0.40299 Loss_smooth: 0.01702, Loss_semigroup: 0.00082\n",
      "\n",
      "Training run 9\n",
      "=> loading checkpoint 'model_center_restricted_ivp'\n",
      "=> loaded checkpoint 'model_center_restricted_ivp'\n",
      "Iter 1, Loss: 0.42083, Loss_pinn: 0.40299 Loss_smooth: 0.01702, Loss_semigroup: 0.00082\n",
      "Iter 2, Loss: 0.42079, Loss_pinn: 0.40298 Loss_smooth: 0.01699, Loss_semigroup: 0.00082\n",
      "Iter 3, Loss: 0.42068, Loss_pinn: 0.40281 Loss_smooth: 0.01706, Loss_semigroup: 0.00081\n",
      "Iter 4, Loss: 0.42059, Loss_pinn: 0.40272 Loss_smooth: 0.01706, Loss_semigroup: 0.00081\n",
      "Iter 5, Loss: 0.42039, Loss_pinn: 0.40256 Loss_smooth: 0.01702, Loss_semigroup: 0.00081\n",
      "Iter 6, Loss: 0.42030, Loss_pinn: 0.40248 Loss_smooth: 0.01701, Loss_semigroup: 0.00081\n",
      "Iter 7, Loss: 0.42022, Loss_pinn: 0.40243 Loss_smooth: 0.01698, Loss_semigroup: 0.00081\n",
      "Iter 8, Loss: 0.42018, Loss_pinn: 0.40243 Loss_smooth: 0.01694, Loss_semigroup: 0.00081\n",
      "Iter 9, Loss: 0.42011, Loss_pinn: 0.40242 Loss_smooth: 0.01688, Loss_semigroup: 0.00081\n",
      "Iter 10, Loss: 0.42003, Loss_pinn: 0.40232 Loss_smooth: 0.01691, Loss_semigroup: 0.00080\n",
      "Iter 11, Loss: 0.41994, Loss_pinn: 0.40228 Loss_smooth: 0.01686, Loss_semigroup: 0.00080\n",
      "Iter 12, Loss: 0.41989, Loss_pinn: 0.40223 Loss_smooth: 0.01686, Loss_semigroup: 0.00080\n",
      "Iter 13, Loss: 0.41984, Loss_pinn: 0.40219 Loss_smooth: 0.01685, Loss_semigroup: 0.00080\n",
      "Iter 14, Loss: 0.41980, Loss_pinn: 0.40216 Loss_smooth: 0.01685, Loss_semigroup: 0.00080\n",
      "Iter 15, Loss: 0.41975, Loss_pinn: 0.40213 Loss_smooth: 0.01682, Loss_semigroup: 0.00080\n",
      "Iter 16, Loss: 0.41971, Loss_pinn: 0.40213 Loss_smooth: 0.01678, Loss_semigroup: 0.00079\n",
      "Iter 17, Loss: 0.41966, Loss_pinn: 0.40212 Loss_smooth: 0.01675, Loss_semigroup: 0.00079\n",
      "Iter 18, Loss: 0.41962, Loss_pinn: 0.40208 Loss_smooth: 0.01675, Loss_semigroup: 0.00079\n",
      "Iter 19, Loss: 0.41956, Loss_pinn: 0.40198 Loss_smooth: 0.01679, Loss_semigroup: 0.00079\n",
      "Iter 20, Loss: 0.41951, Loss_pinn: 0.40188 Loss_smooth: 0.01683, Loss_semigroup: 0.00079\n",
      "Iter 21, Loss: 0.42413, Loss_pinn: 0.40407 Loss_smooth: 0.01926, Loss_semigroup: 0.00080\n",
      "Iter 22, Loss: 0.41950, Loss_pinn: 0.40182 Loss_smooth: 0.01689, Loss_semigroup: 0.00079\n",
      "Iter 23, Loss: 0.41943, Loss_pinn: 0.40173 Loss_smooth: 0.01690, Loss_semigroup: 0.00080\n",
      "Iter 24, Loss: 0.41933, Loss_pinn: 0.40163 Loss_smooth: 0.01690, Loss_semigroup: 0.00080\n",
      "Iter 25, Loss: 0.41922, Loss_pinn: 0.40154 Loss_smooth: 0.01687, Loss_semigroup: 0.00081\n",
      "Iter 26, Loss: 0.41916, Loss_pinn: 0.40172 Loss_smooth: 0.01664, Loss_semigroup: 0.00080\n",
      "Iter 27, Loss: 0.41904, Loss_pinn: 0.40150 Loss_smooth: 0.01675, Loss_semigroup: 0.00079\n",
      "Iter 28, Loss: 0.41891, Loss_pinn: 0.40146 Loss_smooth: 0.01665, Loss_semigroup: 0.00079\n",
      "Iter 29, Loss: 0.41876, Loss_pinn: 0.40140 Loss_smooth: 0.01657, Loss_semigroup: 0.00079\n",
      "Iter 30, Loss: 0.41857, Loss_pinn: 0.40132 Loss_smooth: 0.01646, Loss_semigroup: 0.00079\n",
      "Iter 31, Loss: 0.41839, Loss_pinn: 0.40115 Loss_smooth: 0.01645, Loss_semigroup: 0.00079\n",
      "Iter 32, Loss: 0.41823, Loss_pinn: 0.40098 Loss_smooth: 0.01644, Loss_semigroup: 0.00081\n",
      "Iter 33, Loss: 0.41812, Loss_pinn: 0.40075 Loss_smooth: 0.01657, Loss_semigroup: 0.00081\n",
      "Iter 34, Loss: 0.41804, Loss_pinn: 0.40056 Loss_smooth: 0.01667, Loss_semigroup: 0.00081\n",
      "Iter 35, Loss: 0.41799, Loss_pinn: 0.40041 Loss_smooth: 0.01677, Loss_semigroup: 0.00081\n",
      "Iter 36, Loss: 0.41795, Loss_pinn: 0.40034 Loss_smooth: 0.01680, Loss_semigroup: 0.00081\n",
      "Iter 37, Loss: 0.41790, Loss_pinn: 0.40029 Loss_smooth: 0.01680, Loss_semigroup: 0.00081\n",
      "Iter 38, Loss: 0.41790, Loss_pinn: 0.40024 Loss_smooth: 0.01685, Loss_semigroup: 0.00080\n",
      "Iter 39, Loss: 0.41787, Loss_pinn: 0.40024 Loss_smooth: 0.01682, Loss_semigroup: 0.00081\n",
      "Iter 40, Loss: 0.41781, Loss_pinn: 0.40020 Loss_smooth: 0.01680, Loss_semigroup: 0.00081\n",
      "Iter 41, Loss: 0.41769, Loss_pinn: 0.40018 Loss_smooth: 0.01671, Loss_semigroup: 0.00080\n",
      "Iter 42, Loss: 0.41774, Loss_pinn: 0.40032 Loss_smooth: 0.01661, Loss_semigroup: 0.00080\n",
      "Iter 43, Loss: 0.41764, Loss_pinn: 0.40019 Loss_smooth: 0.01665, Loss_semigroup: 0.00080\n",
      "Iter 44, Loss: 0.41755, Loss_pinn: 0.40019 Loss_smooth: 0.01656, Loss_semigroup: 0.00080\n",
      "Iter 45, Loss: 0.41765, Loss_pinn: 0.40018 Loss_smooth: 0.01668, Loss_semigroup: 0.00079\n",
      "Iter 46, Loss: 0.41747, Loss_pinn: 0.40010 Loss_smooth: 0.01658, Loss_semigroup: 0.00080\n",
      "Iter 47, Loss: 0.41733, Loss_pinn: 0.40016 Loss_smooth: 0.01638, Loss_semigroup: 0.00078\n",
      "Iter 48, Loss: 0.41722, Loss_pinn: 0.40013 Loss_smooth: 0.01631, Loss_semigroup: 0.00078\n",
      "Iter 49, Loss: 0.41705, Loss_pinn: 0.40013 Loss_smooth: 0.01615, Loss_semigroup: 0.00078\n",
      "Iter 50, Loss: 0.41695, Loss_pinn: 0.39996 Loss_smooth: 0.01620, Loss_semigroup: 0.00079\n",
      "Iter 51, Loss: 0.41682, Loss_pinn: 0.39974 Loss_smooth: 0.01627, Loss_semigroup: 0.00080\n",
      "Iter 52, Loss: 0.42203, Loss_pinn: 0.40408 Loss_smooth: 0.01717, Loss_semigroup: 0.00079\n",
      "Iter 53, Loss: 0.41679, Loss_pinn: 0.39975 Loss_smooth: 0.01624, Loss_semigroup: 0.00080\n",
      "Iter 54, Loss: 0.41690, Loss_pinn: 0.39941 Loss_smooth: 0.01667, Loss_semigroup: 0.00082\n",
      "Iter 55, Loss: 0.41676, Loss_pinn: 0.39959 Loss_smooth: 0.01636, Loss_semigroup: 0.00081\n",
      "Iter 56, Loss: 0.41666, Loss_pinn: 0.39947 Loss_smooth: 0.01637, Loss_semigroup: 0.00081\n",
      "Iter 57, Loss: 0.41659, Loss_pinn: 0.39942 Loss_smooth: 0.01635, Loss_semigroup: 0.00081\n",
      "\n",
      "Training run 10\n",
      "=> loading checkpoint 'model_center_restricted_ivp'\n",
      "=> loaded checkpoint 'model_center_restricted_ivp'\n",
      "Iter 1, Loss: 0.41659, Loss_pinn: 0.39942 Loss_smooth: 0.01635, Loss_semigroup: 0.00081\n",
      "Iter 2, Loss: 0.41646, Loss_pinn: 0.39933 Loss_smooth: 0.01632, Loss_semigroup: 0.00081\n",
      "Iter 3, Loss: 0.41636, Loss_pinn: 0.39927 Loss_smooth: 0.01629, Loss_semigroup: 0.00080\n",
      "Iter 4, Loss: 0.41622, Loss_pinn: 0.39916 Loss_smooth: 0.01628, Loss_semigroup: 0.00079\n",
      "Iter 5, Loss: 0.41616, Loss_pinn: 0.39908 Loss_smooth: 0.01630, Loss_semigroup: 0.00078\n",
      "Iter 6, Loss: 0.41613, Loss_pinn: 0.39908 Loss_smooth: 0.01628, Loss_semigroup: 0.00078\n",
      "Iter 7, Loss: 0.41611, Loss_pinn: 0.39901 Loss_smooth: 0.01632, Loss_semigroup: 0.00078\n",
      "Iter 8, Loss: 0.41608, Loss_pinn: 0.39896 Loss_smooth: 0.01634, Loss_semigroup: 0.00078\n",
      "Iter 9, Loss: 0.41605, Loss_pinn: 0.39890 Loss_smooth: 0.01637, Loss_semigroup: 0.00078\n",
      "Iter 10, Loss: 0.41600, Loss_pinn: 0.39887 Loss_smooth: 0.01635, Loss_semigroup: 0.00078\n",
      "Iter 11, Loss: 0.41589, Loss_pinn: 0.39883 Loss_smooth: 0.01629, Loss_semigroup: 0.00077\n",
      "Iter 12, Loss: 0.41583, Loss_pinn: 0.39886 Loss_smooth: 0.01620, Loss_semigroup: 0.00077\n",
      "Iter 13, Loss: 0.41579, Loss_pinn: 0.39886 Loss_smooth: 0.01615, Loss_semigroup: 0.00077\n",
      "Iter 14, Loss: 0.41576, Loss_pinn: 0.39888 Loss_smooth: 0.01611, Loss_semigroup: 0.00077\n",
      "Iter 15, Loss: 0.41573, Loss_pinn: 0.39887 Loss_smooth: 0.01609, Loss_semigroup: 0.00077\n",
      "Iter 16, Loss: 0.41572, Loss_pinn: 0.39891 Loss_smooth: 0.01604, Loss_semigroup: 0.00078\n",
      "Iter 17, Loss: 0.41582, Loss_pinn: 0.39875 Loss_smooth: 0.01628, Loss_semigroup: 0.00079\n",
      "Iter 18, Loss: 0.41568, Loss_pinn: 0.39879 Loss_smooth: 0.01610, Loss_semigroup: 0.00078\n",
      "Iter 19, Loss: 0.41565, Loss_pinn: 0.39874 Loss_smooth: 0.01613, Loss_semigroup: 0.00078\n",
      "Iter 20, Loss: 0.41562, Loss_pinn: 0.39868 Loss_smooth: 0.01615, Loss_semigroup: 0.00079\n",
      "Iter 21, Loss: 0.41558, Loss_pinn: 0.39864 Loss_smooth: 0.01615, Loss_semigroup: 0.00079\n",
      "Iter 22, Loss: 0.41552, Loss_pinn: 0.39856 Loss_smooth: 0.01616, Loss_semigroup: 0.00080\n",
      "Iter 23, Loss: 0.41544, Loss_pinn: 0.39848 Loss_smooth: 0.01616, Loss_semigroup: 0.00080\n",
      "Iter 24, Loss: 0.41532, Loss_pinn: 0.39826 Loss_smooth: 0.01626, Loss_semigroup: 0.00081\n",
      "Iter 25, Loss: 0.41518, Loss_pinn: 0.39807 Loss_smooth: 0.01629, Loss_semigroup: 0.00082\n",
      "Iter 26, Loss: 0.41503, Loss_pinn: 0.39792 Loss_smooth: 0.01629, Loss_semigroup: 0.00081\n",
      "Iter 27, Loss: 0.41488, Loss_pinn: 0.39771 Loss_smooth: 0.01637, Loss_semigroup: 0.00080\n",
      "Iter 28, Loss: 0.41480, Loss_pinn: 0.39758 Loss_smooth: 0.01641, Loss_semigroup: 0.00080\n",
      "Iter 29, Loss: 0.41468, Loss_pinn: 0.39736 Loss_smooth: 0.01652, Loss_semigroup: 0.00081\n",
      "Iter 30, Loss: 0.41472, Loss_pinn: 0.39732 Loss_smooth: 0.01660, Loss_semigroup: 0.00080\n",
      "Iter 31, Loss: 0.41461, Loss_pinn: 0.39727 Loss_smooth: 0.01653, Loss_semigroup: 0.00080\n",
      "Iter 32, Loss: 0.41451, Loss_pinn: 0.39714 Loss_smooth: 0.01656, Loss_semigroup: 0.00081\n",
      "Iter 33, Loss: 0.41444, Loss_pinn: 0.39705 Loss_smooth: 0.01658, Loss_semigroup: 0.00081\n",
      "Iter 34, Loss: 0.41441, Loss_pinn: 0.39700 Loss_smooth: 0.01660, Loss_semigroup: 0.00081\n",
      "Iter 35, Loss: 0.41436, Loss_pinn: 0.39691 Loss_smooth: 0.01664, Loss_semigroup: 0.00081\n",
      "Iter 36, Loss: 0.41429, Loss_pinn: 0.39675 Loss_smooth: 0.01673, Loss_semigroup: 0.00081\n",
      "Iter 37, Loss: 0.41426, Loss_pinn: 0.39663 Loss_smooth: 0.01683, Loss_semigroup: 0.00080\n",
      "Iter 38, Loss: 0.41422, Loss_pinn: 0.39659 Loss_smooth: 0.01683, Loss_semigroup: 0.00080\n",
      "Iter 39, Loss: 0.41418, Loss_pinn: 0.39655 Loss_smooth: 0.01683, Loss_semigroup: 0.00081\n",
      "Iter 40, Loss: 0.41414, Loss_pinn: 0.39651 Loss_smooth: 0.01682, Loss_semigroup: 0.00081\n",
      "Iter 41, Loss: 0.41423, Loss_pinn: 0.39656 Loss_smooth: 0.01687, Loss_semigroup: 0.00081\n",
      "Iter 42, Loss: 0.41414, Loss_pinn: 0.39650 Loss_smooth: 0.01683, Loss_semigroup: 0.00081\n",
      "Iter 43, Loss: 0.41408, Loss_pinn: 0.39650 Loss_smooth: 0.01676, Loss_semigroup: 0.00081\n",
      "Iter 44, Loss: 0.41402, Loss_pinn: 0.39645 Loss_smooth: 0.01676, Loss_semigroup: 0.00081\n",
      "Iter 45, Loss: 0.41372, Loss_pinn: 0.39615 Loss_smooth: 0.01676, Loss_semigroup: 0.00082\n",
      "Iter 46, Loss: 0.41351, Loss_pinn: 0.39586 Loss_smooth: 0.01683, Loss_semigroup: 0.00082\n",
      "Iter 47, Loss: 0.41329, Loss_pinn: 0.39537 Loss_smooth: 0.01710, Loss_semigroup: 0.00082\n",
      "Iter 48, Loss: 0.41323, Loss_pinn: 0.39518 Loss_smooth: 0.01722, Loss_semigroup: 0.00083\n",
      "Iter 49, Loss: 0.41312, Loss_pinn: 0.39512 Loss_smooth: 0.01716, Loss_semigroup: 0.00084\n",
      "Iter 50, Loss: 0.41303, Loss_pinn: 0.39507 Loss_smooth: 0.01712, Loss_semigroup: 0.00084\n",
      "Iter 51, Loss: 0.41287, Loss_pinn: 0.39495 Loss_smooth: 0.01708, Loss_semigroup: 0.00084\n",
      "Iter 52, Loss: 0.41332, Loss_pinn: 0.39509 Loss_smooth: 0.01738, Loss_semigroup: 0.00084\n",
      "Iter 53, Loss: 0.41280, Loss_pinn: 0.39485 Loss_smooth: 0.01711, Loss_semigroup: 0.00084\n",
      "Iter 54, Loss: 0.41262, Loss_pinn: 0.39469 Loss_smooth: 0.01707, Loss_semigroup: 0.00086\n",
      "Iter 55, Loss: 0.41252, Loss_pinn: 0.39443 Loss_smooth: 0.01722, Loss_semigroup: 0.00087\n",
      "\n",
      "Training run 11\n",
      "=> loading checkpoint 'model_center_restricted_ivp'\n",
      "=> loaded checkpoint 'model_center_restricted_ivp'\n",
      "Iter 1, Loss: 0.41252, Loss_pinn: 0.39443 Loss_smooth: 0.01722, Loss_semigroup: 0.00087\n",
      "Iter 2, Loss: 0.41243, Loss_pinn: 0.39420 Loss_smooth: 0.01735, Loss_semigroup: 0.00088\n",
      "Iter 3, Loss: 0.41233, Loss_pinn: 0.39386 Loss_smooth: 0.01759, Loss_semigroup: 0.00089\n",
      "Iter 4, Loss: 0.41228, Loss_pinn: 0.39369 Loss_smooth: 0.01769, Loss_semigroup: 0.00089\n",
      "Iter 5, Loss: 0.41221, Loss_pinn: 0.39353 Loss_smooth: 0.01778, Loss_semigroup: 0.00090\n",
      "Iter 6, Loss: 0.41216, Loss_pinn: 0.39344 Loss_smooth: 0.01781, Loss_semigroup: 0.00090\n",
      "Iter 7, Loss: 0.41210, Loss_pinn: 0.39339 Loss_smooth: 0.01781, Loss_semigroup: 0.00090\n",
      "Iter 8, Loss: 0.41205, Loss_pinn: 0.39340 Loss_smooth: 0.01775, Loss_semigroup: 0.00090\n",
      "Iter 9, Loss: 0.41189, Loss_pinn: 0.39339 Loss_smooth: 0.01761, Loss_semigroup: 0.00088\n",
      "Iter 10, Loss: 0.41209, Loss_pinn: 0.39360 Loss_smooth: 0.01761, Loss_semigroup: 0.00087\n",
      "Iter 11, Loss: 0.41184, Loss_pinn: 0.39338 Loss_smooth: 0.01758, Loss_semigroup: 0.00088\n",
      "Iter 12, Loss: 0.41173, Loss_pinn: 0.39319 Loss_smooth: 0.01766, Loss_semigroup: 0.00089\n",
      "Iter 13, Loss: 0.41158, Loss_pinn: 0.39317 Loss_smooth: 0.01752, Loss_semigroup: 0.00089\n",
      "Iter 14, Loss: 0.41149, Loss_pinn: 0.39301 Loss_smooth: 0.01759, Loss_semigroup: 0.00089\n",
      "Iter 15, Loss: 0.41137, Loss_pinn: 0.39283 Loss_smooth: 0.01763, Loss_semigroup: 0.00090\n",
      "Iter 16, Loss: 0.41124, Loss_pinn: 0.39272 Loss_smooth: 0.01761, Loss_semigroup: 0.00091\n",
      "Iter 17, Loss: 0.41112, Loss_pinn: 0.39260 Loss_smooth: 0.01760, Loss_semigroup: 0.00091\n",
      "Iter 18, Loss: 0.41163, Loss_pinn: 0.39302 Loss_smooth: 0.01770, Loss_semigroup: 0.00090\n",
      "Iter 19, Loss: 0.41108, Loss_pinn: 0.39260 Loss_smooth: 0.01757, Loss_semigroup: 0.00091\n",
      "Iter 20, Loss: 0.41100, Loss_pinn: 0.39253 Loss_smooth: 0.01757, Loss_semigroup: 0.00090\n",
      "Iter 21, Loss: 0.41092, Loss_pinn: 0.39239 Loss_smooth: 0.01764, Loss_semigroup: 0.00089\n",
      "Iter 22, Loss: 0.41085, Loss_pinn: 0.39231 Loss_smooth: 0.01765, Loss_semigroup: 0.00088\n",
      "Iter 23, Loss: 0.41079, Loss_pinn: 0.39220 Loss_smooth: 0.01771, Loss_semigroup: 0.00089\n",
      "Iter 24, Loss: 0.41069, Loss_pinn: 0.39203 Loss_smooth: 0.01776, Loss_semigroup: 0.00090\n",
      "Iter 25, Loss: 0.41062, Loss_pinn: 0.39191 Loss_smooth: 0.01781, Loss_semigroup: 0.00091\n",
      "Iter 26, Loss: 0.41057, Loss_pinn: 0.39186 Loss_smooth: 0.01779, Loss_semigroup: 0.00091\n",
      "Iter 27, Loss: 0.41050, Loss_pinn: 0.39185 Loss_smooth: 0.01774, Loss_semigroup: 0.00091\n",
      "Iter 28, Loss: 0.41045, Loss_pinn: 0.39180 Loss_smooth: 0.01774, Loss_semigroup: 0.00091\n",
      "Iter 29, Loss: 0.41041, Loss_pinn: 0.39180 Loss_smooth: 0.01771, Loss_semigroup: 0.00091\n",
      "Iter 30, Loss: 0.41037, Loss_pinn: 0.39175 Loss_smooth: 0.01772, Loss_semigroup: 0.00090\n",
      "Iter 31, Loss: 0.41032, Loss_pinn: 0.39174 Loss_smooth: 0.01768, Loss_semigroup: 0.00090\n",
      "Iter 32, Loss: 0.41029, Loss_pinn: 0.39177 Loss_smooth: 0.01762, Loss_semigroup: 0.00090\n",
      "Iter 33, Loss: 0.41025, Loss_pinn: 0.39184 Loss_smooth: 0.01751, Loss_semigroup: 0.00089\n",
      "Iter 34, Loss: 0.41021, Loss_pinn: 0.39191 Loss_smooth: 0.01741, Loss_semigroup: 0.00089\n",
      "Iter 35, Loss: 0.41016, Loss_pinn: 0.39194 Loss_smooth: 0.01733, Loss_semigroup: 0.00089\n",
      "Iter 36, Loss: 0.41192, Loss_pinn: 0.39293 Loss_smooth: 0.01808, Loss_semigroup: 0.00091\n",
      "Iter 37, Loss: 0.41016, Loss_pinn: 0.39194 Loss_smooth: 0.01732, Loss_semigroup: 0.00089\n",
      "Iter 38, Loss: 0.41011, Loss_pinn: 0.39192 Loss_smooth: 0.01730, Loss_semigroup: 0.00089\n",
      "Iter 39, Loss: 0.41005, Loss_pinn: 0.39186 Loss_smooth: 0.01730, Loss_semigroup: 0.00089\n",
      "Iter 40, Loss: 0.41046, Loss_pinn: 0.39221 Loss_smooth: 0.01739, Loss_semigroup: 0.00086\n",
      "Iter 41, Loss: 0.41002, Loss_pinn: 0.39185 Loss_smooth: 0.01729, Loss_semigroup: 0.00088\n",
      "Iter 42, Loss: 0.41007, Loss_pinn: 0.39166 Loss_smooth: 0.01751, Loss_semigroup: 0.00090\n",
      "Iter 43, Loss: 0.40996, Loss_pinn: 0.39173 Loss_smooth: 0.01735, Loss_semigroup: 0.00089\n",
      "Iter 44, Loss: 0.40993, Loss_pinn: 0.39172 Loss_smooth: 0.01732, Loss_semigroup: 0.00089\n",
      "Iter 45, Loss: 0.40990, Loss_pinn: 0.39163 Loss_smooth: 0.01738, Loss_semigroup: 0.00089\n",
      "Iter 46, Loss: 0.40985, Loss_pinn: 0.39153 Loss_smooth: 0.01743, Loss_semigroup: 0.00089\n",
      "Iter 47, Loss: 0.40977, Loss_pinn: 0.39137 Loss_smooth: 0.01751, Loss_semigroup: 0.00089\n",
      "Iter 48, Loss: 0.40966, Loss_pinn: 0.39122 Loss_smooth: 0.01756, Loss_semigroup: 0.00088\n",
      "Iter 49, Loss: 0.40960, Loss_pinn: 0.39117 Loss_smooth: 0.01756, Loss_semigroup: 0.00088\n",
      "Iter 50, Loss: 0.40956, Loss_pinn: 0.39122 Loss_smooth: 0.01747, Loss_semigroup: 0.00088\n",
      "Iter 51, Loss: 0.40951, Loss_pinn: 0.39117 Loss_smooth: 0.01747, Loss_semigroup: 0.00087\n",
      "Iter 52, Loss: 0.40948, Loss_pinn: 0.39116 Loss_smooth: 0.01745, Loss_semigroup: 0.00088\n",
      "Iter 53, Loss: 0.40944, Loss_pinn: 0.39115 Loss_smooth: 0.01741, Loss_semigroup: 0.00088\n",
      "Iter 54, Loss: 0.40940, Loss_pinn: 0.39116 Loss_smooth: 0.01736, Loss_semigroup: 0.00088\n",
      "Iter 55, Loss: 0.40934, Loss_pinn: 0.39114 Loss_smooth: 0.01732, Loss_semigroup: 0.00088\n",
      "Iter 56, Loss: 0.41177, Loss_pinn: 0.39275 Loss_smooth: 0.01816, Loss_semigroup: 0.00087\n",
      "Iter 57, Loss: 0.40933, Loss_pinn: 0.39112 Loss_smooth: 0.01733, Loss_semigroup: 0.00088\n",
      "\n",
      "Training run 12\n",
      "=> loading checkpoint 'model_center_restricted_ivp'\n",
      "=> loaded checkpoint 'model_center_restricted_ivp'\n",
      "Iter 1, Loss: 0.40933, Loss_pinn: 0.39112 Loss_smooth: 0.01733, Loss_semigroup: 0.00088\n",
      "Iter 2, Loss: 0.40928, Loss_pinn: 0.39113 Loss_smooth: 0.01727, Loss_semigroup: 0.00087\n",
      "Iter 3, Loss: 0.40918, Loss_pinn: 0.39107 Loss_smooth: 0.01724, Loss_semigroup: 0.00087\n",
      "Iter 4, Loss: 0.40909, Loss_pinn: 0.39094 Loss_smooth: 0.01728, Loss_semigroup: 0.00087\n",
      "Iter 5, Loss: 0.40899, Loss_pinn: 0.39075 Loss_smooth: 0.01736, Loss_semigroup: 0.00087\n",
      "Iter 6, Loss: 0.40892, Loss_pinn: 0.39060 Loss_smooth: 0.01744, Loss_semigroup: 0.00087\n",
      "Iter 7, Loss: 0.40884, Loss_pinn: 0.39053 Loss_smooth: 0.01743, Loss_semigroup: 0.00087\n",
      "Iter 8, Loss: 0.40877, Loss_pinn: 0.39040 Loss_smooth: 0.01750, Loss_semigroup: 0.00087\n",
      "Iter 9, Loss: 0.40870, Loss_pinn: 0.39041 Loss_smooth: 0.01743, Loss_semigroup: 0.00087\n",
      "Iter 10, Loss: 0.40868, Loss_pinn: 0.39026 Loss_smooth: 0.01755, Loss_semigroup: 0.00088\n",
      "Iter 11, Loss: 0.40861, Loss_pinn: 0.39044 Loss_smooth: 0.01732, Loss_semigroup: 0.00086\n",
      "Iter 12, Loss: 0.40857, Loss_pinn: 0.39030 Loss_smooth: 0.01741, Loss_semigroup: 0.00087\n",
      "Iter 13, Loss: 0.40854, Loss_pinn: 0.39021 Loss_smooth: 0.01746, Loss_semigroup: 0.00087\n",
      "Iter 14, Loss: 0.40857, Loss_pinn: 0.39006 Loss_smooth: 0.01765, Loss_semigroup: 0.00086\n",
      "Iter 15, Loss: 0.40851, Loss_pinn: 0.39012 Loss_smooth: 0.01753, Loss_semigroup: 0.00086\n",
      "Iter 16, Loss: 0.40848, Loss_pinn: 0.39006 Loss_smooth: 0.01755, Loss_semigroup: 0.00086\n",
      "Iter 17, Loss: 0.40842, Loss_pinn: 0.39002 Loss_smooth: 0.01755, Loss_semigroup: 0.00086\n",
      "Iter 18, Loss: 0.40835, Loss_pinn: 0.38997 Loss_smooth: 0.01752, Loss_semigroup: 0.00085\n",
      "Iter 19, Loss: 0.40829, Loss_pinn: 0.38997 Loss_smooth: 0.01747, Loss_semigroup: 0.00086\n",
      "Iter 20, Loss: 0.40823, Loss_pinn: 0.38993 Loss_smooth: 0.01744, Loss_semigroup: 0.00085\n",
      "Iter 21, Loss: 0.40818, Loss_pinn: 0.38990 Loss_smooth: 0.01743, Loss_semigroup: 0.00085\n",
      "Iter 22, Loss: 0.40815, Loss_pinn: 0.38990 Loss_smooth: 0.01740, Loss_semigroup: 0.00085\n",
      "Iter 23, Loss: 0.40811, Loss_pinn: 0.38990 Loss_smooth: 0.01736, Loss_semigroup: 0.00084\n",
      "Iter 24, Loss: 0.40806, Loss_pinn: 0.38990 Loss_smooth: 0.01732, Loss_semigroup: 0.00084\n",
      "Iter 25, Loss: 0.40800, Loss_pinn: 0.38991 Loss_smooth: 0.01726, Loss_semigroup: 0.00084\n",
      "Iter 26, Loss: 0.40880, Loss_pinn: 0.39107 Loss_smooth: 0.01693, Loss_semigroup: 0.00080\n",
      "Iter 27, Loss: 0.40799, Loss_pinn: 0.38995 Loss_smooth: 0.01720, Loss_semigroup: 0.00083\n",
      "Iter 28, Loss: 0.40796, Loss_pinn: 0.39018 Loss_smooth: 0.01696, Loss_semigroup: 0.00082\n",
      "Iter 29, Loss: 0.40781, Loss_pinn: 0.39002 Loss_smooth: 0.01696, Loss_semigroup: 0.00083\n",
      "Iter 30, Loss: 0.40776, Loss_pinn: 0.39000 Loss_smooth: 0.01694, Loss_semigroup: 0.00083\n",
      "Iter 31, Loss: 0.40769, Loss_pinn: 0.38997 Loss_smooth: 0.01689, Loss_semigroup: 0.00083\n",
      "Iter 32, Loss: 0.40762, Loss_pinn: 0.38996 Loss_smooth: 0.01683, Loss_semigroup: 0.00083\n",
      "Iter 33, Loss: 0.40757, Loss_pinn: 0.38992 Loss_smooth: 0.01682, Loss_semigroup: 0.00083\n",
      "Iter 34, Loss: 0.40803, Loss_pinn: 0.39041 Loss_smooth: 0.01681, Loss_semigroup: 0.00081\n",
      "Iter 35, Loss: 0.40748, Loss_pinn: 0.38990 Loss_smooth: 0.01676, Loss_semigroup: 0.00082\n",
      "Iter 36, Loss: 0.40737, Loss_pinn: 0.38977 Loss_smooth: 0.01678, Loss_semigroup: 0.00082\n",
      "Iter 37, Loss: 0.40710, Loss_pinn: 0.38930 Loss_smooth: 0.01698, Loss_semigroup: 0.00082\n",
      "Iter 38, Loss: 0.40745, Loss_pinn: 0.38953 Loss_smooth: 0.01710, Loss_semigroup: 0.00082\n",
      "Iter 39, Loss: 0.40708, Loss_pinn: 0.38929 Loss_smooth: 0.01697, Loss_semigroup: 0.00082\n",
      "Iter 40, Loss: 0.40701, Loss_pinn: 0.38918 Loss_smooth: 0.01700, Loss_semigroup: 0.00083\n",
      "Iter 41, Loss: 0.40693, Loss_pinn: 0.38908 Loss_smooth: 0.01702, Loss_semigroup: 0.00083\n",
      "Iter 42, Loss: 0.40687, Loss_pinn: 0.38893 Loss_smooth: 0.01711, Loss_semigroup: 0.00083\n",
      "Iter 43, Loss: 0.40695, Loss_pinn: 0.38894 Loss_smooth: 0.01720, Loss_semigroup: 0.00081\n",
      "Iter 44, Loss: 0.40684, Loss_pinn: 0.38890 Loss_smooth: 0.01712, Loss_semigroup: 0.00082\n",
      "Iter 45, Loss: 0.40678, Loss_pinn: 0.38879 Loss_smooth: 0.01716, Loss_semigroup: 0.00082\n",
      "Iter 46, Loss: 0.40669, Loss_pinn: 0.38861 Loss_smooth: 0.01726, Loss_semigroup: 0.00082\n",
      "Iter 47, Loss: 0.40772, Loss_pinn: 0.38909 Loss_smooth: 0.01778, Loss_semigroup: 0.00085\n",
      "Iter 48, Loss: 0.40668, Loss_pinn: 0.38858 Loss_smooth: 0.01727, Loss_semigroup: 0.00083\n",
      "Iter 49, Loss: 0.40659, Loss_pinn: 0.38828 Loss_smooth: 0.01748, Loss_semigroup: 0.00083\n",
      "Iter 50, Loss: 0.40647, Loss_pinn: 0.38816 Loss_smooth: 0.01748, Loss_semigroup: 0.00083\n",
      "Iter 51, Loss: 0.40641, Loss_pinn: 0.38807 Loss_smooth: 0.01751, Loss_semigroup: 0.00083\n",
      "Iter 52, Loss: 0.40635, Loss_pinn: 0.38808 Loss_smooth: 0.01743, Loss_semigroup: 0.00083\n",
      "Iter 53, Loss: 0.40631, Loss_pinn: 0.38811 Loss_smooth: 0.01737, Loss_semigroup: 0.00083\n",
      "Iter 54, Loss: 0.40628, Loss_pinn: 0.38816 Loss_smooth: 0.01730, Loss_semigroup: 0.00083\n",
      "Iter 55, Loss: 0.40626, Loss_pinn: 0.38815 Loss_smooth: 0.01729, Loss_semigroup: 0.00082\n",
      "Iter 56, Loss: 0.40624, Loss_pinn: 0.38817 Loss_smooth: 0.01725, Loss_semigroup: 0.00082\n",
      "Iter 57, Loss: 0.40624, Loss_pinn: 0.38817 Loss_smooth: 0.01726, Loss_semigroup: 0.00081\n",
      "\n",
      "Training run 13\n",
      "=> loading checkpoint 'model_center_restricted_ivp'\n",
      "=> loaded checkpoint 'model_center_restricted_ivp'\n",
      "Iter 1, Loss: 0.40624, Loss_pinn: 0.38817 Loss_smooth: 0.01726, Loss_semigroup: 0.00081\n",
      "Iter 2, Loss: 0.40620, Loss_pinn: 0.38818 Loss_smooth: 0.01721, Loss_semigroup: 0.00081\n",
      "Iter 3, Loss: 0.40618, Loss_pinn: 0.38817 Loss_smooth: 0.01720, Loss_semigroup: 0.00081\n",
      "Iter 4, Loss: 0.40615, Loss_pinn: 0.38815 Loss_smooth: 0.01718, Loss_semigroup: 0.00081\n",
      "Iter 5, Loss: 0.40610, Loss_pinn: 0.38809 Loss_smooth: 0.01720, Loss_semigroup: 0.00081\n",
      "Iter 6, Loss: 0.40613, Loss_pinn: 0.38797 Loss_smooth: 0.01735, Loss_semigroup: 0.00081\n",
      "Iter 7, Loss: 0.40607, Loss_pinn: 0.38802 Loss_smooth: 0.01725, Loss_semigroup: 0.00081\n",
      "Iter 8, Loss: 0.40626, Loss_pinn: 0.38801 Loss_smooth: 0.01745, Loss_semigroup: 0.00080\n",
      "Iter 9, Loss: 0.40603, Loss_pinn: 0.38793 Loss_smooth: 0.01729, Loss_semigroup: 0.00081\n",
      "Iter 10, Loss: 0.40597, Loss_pinn: 0.38787 Loss_smooth: 0.01729, Loss_semigroup: 0.00081\n",
      "Iter 11, Loss: 0.40592, Loss_pinn: 0.38782 Loss_smooth: 0.01729, Loss_semigroup: 0.00081\n",
      "Iter 12, Loss: 0.40583, Loss_pinn: 0.38778 Loss_smooth: 0.01724, Loss_semigroup: 0.00081\n",
      "Iter 13, Loss: 0.40581, Loss_pinn: 0.38785 Loss_smooth: 0.01716, Loss_semigroup: 0.00080\n",
      "Iter 14, Loss: 0.40570, Loss_pinn: 0.38769 Loss_smooth: 0.01721, Loss_semigroup: 0.00081\n",
      "Iter 15, Loss: 0.40567, Loss_pinn: 0.38761 Loss_smooth: 0.01724, Loss_semigroup: 0.00081\n",
      "Iter 16, Loss: 0.40598, Loss_pinn: 0.38779 Loss_smooth: 0.01737, Loss_semigroup: 0.00081\n",
      "Iter 17, Loss: 0.40565, Loss_pinn: 0.38759 Loss_smooth: 0.01725, Loss_semigroup: 0.00081\n",
      "Iter 18, Loss: 0.40564, Loss_pinn: 0.38762 Loss_smooth: 0.01721, Loss_semigroup: 0.00081\n",
      "Iter 19, Loss: 0.40561, Loss_pinn: 0.38756 Loss_smooth: 0.01725, Loss_semigroup: 0.00081\n",
      "Iter 20, Loss: 0.40561, Loss_pinn: 0.38758 Loss_smooth: 0.01722, Loss_semigroup: 0.00081\n",
      "Iter 21, Loss: 0.40559, Loss_pinn: 0.38763 Loss_smooth: 0.01716, Loss_semigroup: 0.00080\n",
      "Iter 22, Loss: 0.40558, Loss_pinn: 0.38764 Loss_smooth: 0.01715, Loss_semigroup: 0.00080\n",
      "Iter 23, Loss: 0.40556, Loss_pinn: 0.38764 Loss_smooth: 0.01712, Loss_semigroup: 0.00079\n",
      "Iter 24, Loss: 0.40564, Loss_pinn: 0.38737 Loss_smooth: 0.01747, Loss_semigroup: 0.00080\n",
      "Iter 25, Loss: 0.40555, Loss_pinn: 0.38758 Loss_smooth: 0.01719, Loss_semigroup: 0.00079\n",
      "Iter 26, Loss: 0.40554, Loss_pinn: 0.38754 Loss_smooth: 0.01720, Loss_semigroup: 0.00079\n",
      "Iter 27, Loss: 0.40552, Loss_pinn: 0.38748 Loss_smooth: 0.01725, Loss_semigroup: 0.00080\n",
      "Iter 28, Loss: 0.40551, Loss_pinn: 0.38740 Loss_smooth: 0.01730, Loss_semigroup: 0.00080\n",
      "Iter 29, Loss: 0.40549, Loss_pinn: 0.38735 Loss_smooth: 0.01734, Loss_semigroup: 0.00080\n",
      "Iter 30, Loss: 0.40712, Loss_pinn: 0.38767 Loss_smooth: 0.01864, Loss_semigroup: 0.00081\n",
      "Iter 31, Loss: 0.40548, Loss_pinn: 0.38731 Loss_smooth: 0.01737, Loss_semigroup: 0.00080\n",
      "Iter 32, Loss: 0.40546, Loss_pinn: 0.38730 Loss_smooth: 0.01736, Loss_semigroup: 0.00080\n",
      "Iter 33, Loss: 0.40544, Loss_pinn: 0.38732 Loss_smooth: 0.01733, Loss_semigroup: 0.00079\n",
      "Iter 34, Loss: 0.40543, Loss_pinn: 0.38732 Loss_smooth: 0.01731, Loss_semigroup: 0.00079\n",
      "Iter 35, Loss: 0.40541, Loss_pinn: 0.38731 Loss_smooth: 0.01731, Loss_semigroup: 0.00079\n",
      "Iter 36, Loss: 0.40538, Loss_pinn: 0.38723 Loss_smooth: 0.01736, Loss_semigroup: 0.00079\n",
      "Iter 37, Loss: 0.40534, Loss_pinn: 0.38710 Loss_smooth: 0.01745, Loss_semigroup: 0.00079\n",
      "Iter 38, Loss: 0.40527, Loss_pinn: 0.38686 Loss_smooth: 0.01762, Loss_semigroup: 0.00079\n",
      "Iter 39, Loss: 0.40602, Loss_pinn: 0.38679 Loss_smooth: 0.01840, Loss_semigroup: 0.00083\n",
      "Iter 40, Loss: 0.40522, Loss_pinn: 0.38669 Loss_smooth: 0.01773, Loss_semigroup: 0.00080\n",
      "Iter 41, Loss: 0.40516, Loss_pinn: 0.38656 Loss_smooth: 0.01779, Loss_semigroup: 0.00081\n",
      "Iter 42, Loss: 0.40510, Loss_pinn: 0.38652 Loss_smooth: 0.01776, Loss_semigroup: 0.00082\n",
      "Iter 43, Loss: 0.40504, Loss_pinn: 0.38647 Loss_smooth: 0.01776, Loss_semigroup: 0.00082\n",
      "Iter 44, Loss: 0.40498, Loss_pinn: 0.38647 Loss_smooth: 0.01769, Loss_semigroup: 0.00082\n",
      "Iter 45, Loss: 0.40483, Loss_pinn: 0.38650 Loss_smooth: 0.01751, Loss_semigroup: 0.00082\n",
      "Iter 46, Loss: 0.40476, Loss_pinn: 0.38644 Loss_smooth: 0.01750, Loss_semigroup: 0.00082\n",
      "Iter 47, Loss: 0.40466, Loss_pinn: 0.38640 Loss_smooth: 0.01744, Loss_semigroup: 0.00081\n",
      "Iter 48, Loss: 0.40460, Loss_pinn: 0.38638 Loss_smooth: 0.01741, Loss_semigroup: 0.00081\n",
      "Iter 49, Loss: 0.40452, Loss_pinn: 0.38617 Loss_smooth: 0.01755, Loss_semigroup: 0.00081\n",
      "Iter 50, Loss: 0.40447, Loss_pinn: 0.38598 Loss_smooth: 0.01768, Loss_semigroup: 0.00081\n",
      "Iter 51, Loss: 0.40439, Loss_pinn: 0.38579 Loss_smooth: 0.01780, Loss_semigroup: 0.00081\n",
      "Iter 52, Loss: 0.40439, Loss_pinn: 0.38564 Loss_smooth: 0.01795, Loss_semigroup: 0.00080\n",
      "Iter 53, Loss: 0.40436, Loss_pinn: 0.38569 Loss_smooth: 0.01786, Loss_semigroup: 0.00081\n",
      "Iter 54, Loss: 0.40434, Loss_pinn: 0.38570 Loss_smooth: 0.01784, Loss_semigroup: 0.00081\n",
      "Iter 55, Loss: 0.40431, Loss_pinn: 0.38572 Loss_smooth: 0.01779, Loss_semigroup: 0.00080\n",
      "Iter 56, Loss: 0.40430, Loss_pinn: 0.38573 Loss_smooth: 0.01777, Loss_semigroup: 0.00080\n",
      "Iter 57, Loss: 0.40428, Loss_pinn: 0.38577 Loss_smooth: 0.01771, Loss_semigroup: 0.00080\n",
      "Iter 58, Loss: 0.40467, Loss_pinn: 0.38633 Loss_smooth: 0.01752, Loss_semigroup: 0.00082\n",
      "Iter 59, Loss: 0.40428, Loss_pinn: 0.38580 Loss_smooth: 0.01768, Loss_semigroup: 0.00080\n",
      "\n",
      "Training run 14\n",
      "=> loading checkpoint 'model_center_restricted_ivp'\n",
      "=> loaded checkpoint 'model_center_restricted_ivp'\n",
      "Iter 1, Loss: 0.40428, Loss_pinn: 0.38580 Loss_smooth: 0.01768, Loss_semigroup: 0.00080\n",
      "Iter 2, Loss: 0.40425, Loss_pinn: 0.38573 Loss_smooth: 0.01772, Loss_semigroup: 0.00080\n",
      "Iter 3, Loss: 0.40422, Loss_pinn: 0.38564 Loss_smooth: 0.01777, Loss_semigroup: 0.00080\n",
      "Iter 4, Loss: 0.40418, Loss_pinn: 0.38557 Loss_smooth: 0.01781, Loss_semigroup: 0.00081\n",
      "Iter 5, Loss: 0.40413, Loss_pinn: 0.38541 Loss_smooth: 0.01791, Loss_semigroup: 0.00081\n",
      "Iter 6, Loss: 0.43053, Loss_pinn: 0.40269 Loss_smooth: 0.02695, Loss_semigroup: 0.00090\n",
      "Iter 7, Loss: 0.40409, Loss_pinn: 0.38530 Loss_smooth: 0.01797, Loss_semigroup: 0.00081\n",
      "Iter 8, Loss: 0.40484, Loss_pinn: 0.38538 Loss_smooth: 0.01866, Loss_semigroup: 0.00080\n",
      "Iter 9, Loss: 0.40399, Loss_pinn: 0.38509 Loss_smooth: 0.01809, Loss_semigroup: 0.00081\n",
      "Iter 10, Loss: 0.40385, Loss_pinn: 0.38511 Loss_smooth: 0.01793, Loss_semigroup: 0.00081\n",
      "Iter 11, Loss: 0.40364, Loss_pinn: 0.38521 Loss_smooth: 0.01763, Loss_semigroup: 0.00080\n",
      "Iter 12, Loss: 0.40348, Loss_pinn: 0.38511 Loss_smooth: 0.01757, Loss_semigroup: 0.00080\n",
      "Iter 13, Loss: 0.40348, Loss_pinn: 0.38486 Loss_smooth: 0.01781, Loss_semigroup: 0.00080\n",
      "Iter 14, Loss: 0.40341, Loss_pinn: 0.38494 Loss_smooth: 0.01766, Loss_semigroup: 0.00080\n",
      "Iter 15, Loss: 0.40334, Loss_pinn: 0.38487 Loss_smooth: 0.01765, Loss_semigroup: 0.00081\n",
      "Iter 16, Loss: 0.40329, Loss_pinn: 0.38479 Loss_smooth: 0.01769, Loss_semigroup: 0.00081\n",
      "Iter 17, Loss: 0.40325, Loss_pinn: 0.38473 Loss_smooth: 0.01770, Loss_semigroup: 0.00081\n",
      "Iter 18, Loss: 0.40359, Loss_pinn: 0.38477 Loss_smooth: 0.01798, Loss_semigroup: 0.00084\n",
      "Iter 19, Loss: 0.40321, Loss_pinn: 0.38465 Loss_smooth: 0.01774, Loss_semigroup: 0.00082\n",
      "Iter 20, Loss: 0.40317, Loss_pinn: 0.38470 Loss_smooth: 0.01766, Loss_semigroup: 0.00081\n",
      "Iter 21, Loss: 0.40312, Loss_pinn: 0.38476 Loss_smooth: 0.01756, Loss_semigroup: 0.00081\n",
      "Iter 22, Loss: 0.40373, Loss_pinn: 0.38542 Loss_smooth: 0.01750, Loss_semigroup: 0.00081\n",
      "Iter 23, Loss: 0.40311, Loss_pinn: 0.38477 Loss_smooth: 0.01753, Loss_semigroup: 0.00081\n",
      "Iter 24, Loss: 0.40306, Loss_pinn: 0.38481 Loss_smooth: 0.01744, Loss_semigroup: 0.00080\n",
      "Iter 25, Loss: 0.40306, Loss_pinn: 0.38488 Loss_smooth: 0.01738, Loss_semigroup: 0.00080\n",
      "Iter 26, Loss: 0.40302, Loss_pinn: 0.38481 Loss_smooth: 0.01740, Loss_semigroup: 0.00080\n",
      "Iter 27, Loss: 0.40292, Loss_pinn: 0.38470 Loss_smooth: 0.01741, Loss_semigroup: 0.00081\n",
      "Iter 28, Loss: 0.40267, Loss_pinn: 0.38432 Loss_smooth: 0.01751, Loss_semigroup: 0.00083\n",
      "Iter 29, Loss: 0.40249, Loss_pinn: 0.38398 Loss_smooth: 0.01767, Loss_semigroup: 0.00085\n",
      "Iter 30, Loss: 0.40232, Loss_pinn: 0.38362 Loss_smooth: 0.01783, Loss_semigroup: 0.00087\n",
      "Iter 31, Loss: 0.40239, Loss_pinn: 0.38327 Loss_smooth: 0.01824, Loss_semigroup: 0.00088\n",
      "Iter 32, Loss: 0.40222, Loss_pinn: 0.38339 Loss_smooth: 0.01795, Loss_semigroup: 0.00088\n",
      "Iter 33, Loss: 0.40215, Loss_pinn: 0.38339 Loss_smooth: 0.01788, Loss_semigroup: 0.00087\n",
      "Iter 34, Loss: 0.40198, Loss_pinn: 0.38339 Loss_smooth: 0.01772, Loss_semigroup: 0.00087\n",
      "Iter 35, Loss: 0.40189, Loss_pinn: 0.38329 Loss_smooth: 0.01772, Loss_semigroup: 0.00088\n",
      "Iter 36, Loss: 0.40180, Loss_pinn: 0.38312 Loss_smooth: 0.01779, Loss_semigroup: 0.00089\n",
      "Iter 37, Loss: 0.40203, Loss_pinn: 0.38321 Loss_smooth: 0.01792, Loss_semigroup: 0.00090\n",
      "Iter 38, Loss: 0.40178, Loss_pinn: 0.38309 Loss_smooth: 0.01780, Loss_semigroup: 0.00089\n",
      "Iter 39, Loss: 0.40168, Loss_pinn: 0.38286 Loss_smooth: 0.01792, Loss_semigroup: 0.00090\n",
      "Iter 40, Loss: 0.40184, Loss_pinn: 0.38222 Loss_smooth: 0.01870, Loss_semigroup: 0.00092\n",
      "Iter 41, Loss: 0.40157, Loss_pinn: 0.38245 Loss_smooth: 0.01821, Loss_semigroup: 0.00091\n",
      "Iter 42, Loss: 0.40331, Loss_pinn: 0.38243 Loss_smooth: 0.01997, Loss_semigroup: 0.00090\n",
      "Iter 43, Loss: 0.40146, Loss_pinn: 0.38219 Loss_smooth: 0.01836, Loss_semigroup: 0.00091\n",
      "Iter 44, Loss: 0.40134, Loss_pinn: 0.38202 Loss_smooth: 0.01841, Loss_semigroup: 0.00090\n",
      "Iter 45, Loss: 0.40116, Loss_pinn: 0.38192 Loss_smooth: 0.01835, Loss_semigroup: 0.00090\n",
      "Iter 46, Loss: 0.40099, Loss_pinn: 0.38171 Loss_smooth: 0.01839, Loss_semigroup: 0.00089\n",
      "Iter 47, Loss: 0.40085, Loss_pinn: 0.38163 Loss_smooth: 0.01833, Loss_semigroup: 0.00088\n",
      "Iter 48, Loss: 0.40067, Loss_pinn: 0.38146 Loss_smooth: 0.01833, Loss_semigroup: 0.00088\n",
      "Iter 49, Loss: 0.40063, Loss_pinn: 0.38162 Loss_smooth: 0.01813, Loss_semigroup: 0.00088\n",
      "Iter 50, Loss: 0.40050, Loss_pinn: 0.38128 Loss_smooth: 0.01834, Loss_semigroup: 0.00089\n",
      "Iter 51, Loss: 0.40046, Loss_pinn: 0.38126 Loss_smooth: 0.01830, Loss_semigroup: 0.00089\n",
      "Iter 52, Loss: 0.40037, Loss_pinn: 0.38116 Loss_smooth: 0.01832, Loss_semigroup: 0.00089\n",
      "Iter 53, Loss: 0.40029, Loss_pinn: 0.38103 Loss_smooth: 0.01837, Loss_semigroup: 0.00089\n",
      "Iter 54, Loss: 0.40032, Loss_pinn: 0.38121 Loss_smooth: 0.01822, Loss_semigroup: 0.00088\n",
      "Iter 55, Loss: 0.40026, Loss_pinn: 0.38108 Loss_smooth: 0.01830, Loss_semigroup: 0.00089\n",
      "Iter 56, Loss: 0.40080, Loss_pinn: 0.38142 Loss_smooth: 0.01853, Loss_semigroup: 0.00085\n",
      "Iter 57, Loss: 0.40019, Loss_pinn: 0.38101 Loss_smooth: 0.01831, Loss_semigroup: 0.00088\n",
      "Iter 58, Loss: 0.40014, Loss_pinn: 0.38100 Loss_smooth: 0.01826, Loss_semigroup: 0.00088\n",
      "Iter 59, Loss: 0.40004, Loss_pinn: 0.38102 Loss_smooth: 0.01815, Loss_semigroup: 0.00087\n",
      "Iter 60, Loss: 0.39992, Loss_pinn: 0.38113 Loss_smooth: 0.01793, Loss_semigroup: 0.00086\n",
      "Iter 61, Loss: 0.41739, Loss_pinn: 0.39054 Loss_smooth: 0.02584, Loss_semigroup: 0.00101\n",
      "Iter 62, Loss: 0.40000, Loss_pinn: 0.38093 Loss_smooth: 0.01818, Loss_semigroup: 0.00089\n",
      "Iter 63, Loss: 0.39988, Loss_pinn: 0.38102 Loss_smooth: 0.01800, Loss_semigroup: 0.00087\n",
      "Iter 64, Loss: 0.39976, Loss_pinn: 0.38092 Loss_smooth: 0.01797, Loss_semigroup: 0.00087\n",
      "Iter 65, Loss: 0.39962, Loss_pinn: 0.38090 Loss_smooth: 0.01784, Loss_semigroup: 0.00087\n"
     ]
    }
   ],
   "source": [
    "# NOTE: When training with a GPU, the loaded models have to be moved to cuda\n",
    "# see https://discuss.pytorch.org/t/loading-a-saved-model-for-continue-training/17244/2\n",
    "\n",
    "with open(\"./X_pinn_center_restricted_ivp.pkl\", \"rb\") as f:\n",
    "    X_pinn = pickle.load(f)\n",
    "\n",
    "with open(\"./X_semigroup_center_restricted_ivp.pkl\", \"rb\") as f:\n",
    "    X_semigroup = pickle.load(f)\n",
    "\n",
    "with open(\"./X_smooth_center_restricted_ivp.pkl\", \"rb\") as f:\n",
    "    X_smooth = pickle.load(f)\n",
    "\n",
    "while n_training < max_n_training:\n",
    "\n",
    "    print(f\"\\nTraining run {n_training}\")\n",
    "    model = TcPINN(X_pinn, X_semigroup, X_smooth, layers, T)\n",
    "    model = load_checkpoint(model, \"model_center_restricted_ivp\")\n",
    "\n",
    "    opt_sd = model.optimizer.state_dict()\n",
    "    opt_sd['param_groups'][0]['max_iter'] = 50\n",
    "    opt_sd['param_groups'][0]['lr'] = 1\n",
    "    model.optimizer.load_state_dict(opt_sd)\n",
    "    model.train()\n",
    "    n_training +=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd52a59",
   "metadata": {
    "id": "3dd52a59"
   },
   "source": [
    "## Predict and Plot the Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff2236a2",
   "metadata": {
    "id": "ff2236a2"
   },
   "outputs": [],
   "source": [
    "def generate_figure(figsize, xlim, ylim):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.spines[['top', 'right']].set_visible(False)\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_ode_solution(ax, y, index0, index1, *args, **kwargs):\n",
    "    \n",
    "    ax.plot(y[:,index0], y[:,index1], '.-', *args, **kwargs)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f561ea4",
   "metadata": {
    "id": "2f561ea4"
   },
   "outputs": [],
   "source": [
    "def predict_tc(model, y0, max_t_pred, delta_t):\n",
    "    \"\"\"\n",
    "    detla_t should devide model.max_t to guarantee equidistant steps\n",
    "    \"\"\"\n",
    "    times = np.arange(0, model.T + delta_t, delta_t)[1:]\n",
    "    times = times[:,np.newaxis]\n",
    "    n_resets = int(np.ceil(max_t_pred / model.T))\n",
    "    \n",
    "    trajectory = np.array([y0])\n",
    "    \n",
    "    for _ in range(n_resets):\n",
    "        \n",
    "        y0 = trajectory[-1]\n",
    "        y0 = np.array([y0 for _ in range(len(times))])\n",
    "        segment =  model.predict(times, y0)\n",
    "        trajectory = np.vstack([trajectory, segment])\n",
    "    \n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2f012bb",
   "metadata": {
    "id": "f2f012bb"
   },
   "outputs": [],
   "source": [
    "# Note that max_t in training is 1\n",
    "y0 = [1., 0., -0.4, 0.1, .0, .0, .0, .0]\n",
    "max_t_pred = 1.\n",
    "delta_t = 0.01\n",
    "\n",
    "validation_tc = predict_tc(model, y0, max_t_pred, delta_t)\n",
    "body_three = - (validation_tc[:,0:2] + validation_tc[:,2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21cf58fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "21cf58fc",
    "outputId": "6d5dbe9a-87cd-4c2f-cbc4-c8dfb2eaffcc",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = generate_figure(figsize=(8,8), xlim=[-7, 7], ylim=[-7, 7])\n",
    "\n",
    "ax = plot_ode_solution(ax, validation_tc, 0, 1, markevery=[0], label=\"Body 1\", color=\"#03468F\")\n",
    "ax = plot_ode_solution(ax, validation_tc, 2, 3, markevery=[0], label=\"Body 2\", color=\"#A51C30\")\n",
    "ax = plot_ode_solution(ax, body_three, 0, 1, markevery=[0], label=\"Body 3\", color=\"orange\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"3_body_problem.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3383e64",
   "metadata": {
    "id": "d3383e64"
   },
   "source": [
    "### Reference\n",
    "\n",
    "[1] Breen, Philip G., et al. \"Newton versus the machine: solving the chaotic three-body problem using deep neural networks.\" Monthly Notices of the Royal Astronomical Society 494.2 (2020): 2465-2470."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
