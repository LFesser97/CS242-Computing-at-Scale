{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c314520",
   "metadata": {
    "id": "2c314520"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.rcParams['pgf.texsystem'] = 'pdflatex'\n",
    "# matplotlib.rcParams.update({'font.family': 'serif', 'font.size': 10})\n",
    "# matplotlib.rcParams['text.usetex'] = True\n",
    "from matplotlib.lines import Line2D\n",
    "import pickle\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "import time\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77fff5d",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This jupyter notebook implements the time-consistent physics-informed neural network (tcPINN) idea for the planar three-body problem, restricted to solutions with constant barycenter. We have observed that during training, the tcPINN is still unable to completely learn the dynamics for this restricted domain of initial values. That is, the PINN loss does not converge to zero.\n",
    "\n",
    "A more comprehensive discussion of the problem is given in $\\texttt{three_body_problem_naive.ipynb}$ and in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6d595",
   "metadata": {
    "id": "dab6d595"
   },
   "source": [
    "Consider three gravitationally interacting identical bodies with positions $r_i(t) \\in \\mathbb{R}^2$. Assuming a gravitational force of $G=1$, the Newtonian equations governing their motion reads\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{d^2}{dt^2} \\begin{pmatrix} r_1(t) \\\\ r_2(t) \\\\ r_3(t) \\end{pmatrix} = \\begin{pmatrix} - \\frac{r_1(t) - r_2(t)}{|r_1(t) - r_2(t)|^3} - \\frac{r_1(t) - r_3(t)}{|r_1(t) - r_3(t)|^3} \\\\ - \\frac{r_2(t) - r_1(t)}{|r_2(t) - r_1(t)|^3} - \\frac{r_2(t) - r_3(t)}{|r_2(t) - r_3(t)|^3} \\\\ - \\frac{r_3(t) - r_1(t)}{|r_3(t) - r_1(t)|^3} - \\frac{r_3(t) - r_2(t)}{|r_3(t) - r_2(t)|^3} \\end{pmatrix}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057dc609",
   "metadata": {
    "id": "057dc609"
   },
   "source": [
    "Notice that the ODE implies\n",
    "\n",
    "\\begin{align*}\n",
    "    \\sum_{i=1}^3 \\frac{d^2}{dt^2} r_i(t) = 0,\n",
    "\\end{align*}\n",
    "\n",
    "which yields\n",
    "\n",
    "\\begin{align*}\n",
    "    \\sum_{i=1}^3 \\frac{d}{dt} r_i(t) = \\overrightarrow{C}_1 = \\sum_{i=1}^3 \\frac{d}{dt} r_i(0),\n",
    "\\end{align*}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align*}\n",
    "    \\sum_{i=1}^3 r_i(t) = \\overrightarrow{C}_1 t + \\overrightarrow{C}_2\n",
    "\\end{align*}\n",
    "\n",
    "with \n",
    "\n",
    "\\begin{align*}\n",
    "    \\overrightarrow{C}_2 = \\sum_{i=1}^3 r_i(0).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beddfd4",
   "metadata": {
    "id": "2beddfd4"
   },
   "source": [
    "That is, the barycenter of the three bodies moves on a straight line. By restricting ourselves to initial conditions with mean location and velocity equal to the origin, we enforce the solution to have a constant barycenter over time, i.e.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\sum_{i=1}^3 r_i(t) = 0.\n",
    "\\end{align*}\n",
    "\n",
    "Both the location and the velocity of the thrid body can therefore be obtained by exploiting the symmetry of the system:\n",
    "\n",
    "\\begin{align}\n",
    "    r_3(t) &= - (r_1(t) + r_2(t)), \\\\\n",
    "    v_3(t) &= - (v_1(t) + v_2(t))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cfb516",
   "metadata": {
    "id": "d1cfb516"
   },
   "source": [
    "Notice that an even stronger assumption on the initial conditions was used in [1] to simplify the training task. Under our symmetry assumption, we can plug the formula for $r_3$ into the original ODE system to obtain\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{d^2}{dt^2} \\begin{pmatrix} r_1 \\\\ r_2 \\end{pmatrix} = \\begin{pmatrix} - \\frac{r_1 - r_2}{|r_1 - r_2|^3} - \\frac{2r_1 + r_2}{|2r_1 + r_2|^3} \\\\ - \\frac{r_2 - r_1}{|r_2 - r_1|^3} - \\frac{2r_2 + r_1}{|2r_2 + r_1|^3} \\end{pmatrix}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc79de83",
   "metadata": {
    "id": "cc79de83"
   },
   "source": [
    "This is a second-order ODE system of $4$ equations. By introducing the velocities $v_i(t) = \\frac{d}{dt}r_i(t)$, it can be rewritten as the following first-order ODE system of $8$ equations:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{d}{dt} \\begin{pmatrix} r_1 \\\\ r_2 \\\\ v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ - \\frac{r_1 - r_2}{|r_1 - r_2|^3} - \\frac{2r_1 + r_2}{|2r_1 + r_2|^3} \\\\ - \\frac{r_2 - r_1}{|r_2 - r_1|^3} - \\frac{2r_2 + r_1}{|2r_2 + r_1|^3} \\end{pmatrix}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1809c5",
   "metadata": {
    "id": "0a1809c5"
   },
   "source": [
    "For completeness, all $8$ equations with written-out components are\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{d}{dt} \\begin{pmatrix} r_{11} \\\\ r_{12} \\\\ r_{21} \\\\ r_{22} \\\\ v_{11} \\\\ v_{12} \\\\ v_{21} \\\\ v_{22} \\end{pmatrix} = \\begin{pmatrix} \n",
    "    v_{11} \\\\ v_{12} \\\\ v_{21} \\\\ v_{22} \\\\ \n",
    "    - \\frac{r_{11} - r_{21}}{|r_1 - r_2|^3} - \\frac{2r_{11} + r_{21}}{|2r_1 + r_2|^3} \\\\\n",
    "    - \\frac{r_{12} - r_{22}}{|r_1 - r_2|^3} - \\frac{2r_{12} + r_{22}}{|2r_1 + r_2|^3} \\\\\n",
    "    - \\frac{r_{21} - r_{11}}{|r_2 - r_1|^3} - \\frac{2r_{21} + r_{11}}{|2r_2 + r_1|^3} \\\\\n",
    "    - \\frac{r_{22} - r_{12}}{|r_2 - r_1|^3} - \\frac{2r_{22} + r_{12}}{|2r_2 + r_1|^3}\n",
    "\\end{pmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "With the notation $y = (r_{11}, r_{12}, r_{21}, r_{22}, v_{11}, v_{12}, v_{21}, v_{22})$ used in the implementation, the system reads\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{d}{dt} y = \\begin{pmatrix} \n",
    "    y_4 \\\\ y_5 \\\\ y_6 \\\\ y_7 \\\\ \n",
    "    - \\frac{y_0 - y_2}{((y_0 - y_2)^2 + (y_1 - y_3)^2)^{3/2}} - \\frac{2y_0 + y_2}{((2y_0 + y_2)^2 + (2y_1 + y_3)^2)^{3/2}} \\\\\n",
    "    - \\frac{y_1 - y_3}{((y_0 - y_2)^2 + (y_1 - y_3)^2)^{3/2}} - \\frac{2y_1 + y_3}{((2y_0 + y_2)^2 + (2y_1 + y_3)^2)^{3/2}} \\\\\n",
    "    - \\frac{y_2 - y_0}{((y_0 - y_2)^2 + (y_1 - y_3)^2)^{3/2}} - \\frac{2y_2 + y_0}{((2y_2 + y_0)^2 + (2y_3 + y_1)^2)^{3/2}} \\\\\n",
    "    - \\frac{y_3 - y_1}{((y_0 - y_2)^2 + (y_1 - y_3)^2)^{3/2}} - \\frac{2y_3 + y_1}{((2y_2 + y_0)^2 + (2y_3 + y_1)^2)^{3/2}}\n",
    "\\end{pmatrix}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b698ba",
   "metadata": {},
   "source": [
    "Notice that the authors of [1] used an even stronger assumption by additionally setting all initial velocities to zero to simplify the training task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "135abf6b",
   "metadata": {
    "id": "135abf6b"
   },
   "outputs": [],
   "source": [
    "# CUDA support \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92eed791",
   "metadata": {
    "id": "92eed791"
   },
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class DNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.Tanh\n",
    "        \n",
    "        layer_list = list()\n",
    "        \n",
    "        for i in range(self.depth - 1):\n",
    "            \n",
    "            linear_layer = torch.nn.Linear(layers[i], layers[i+1])\n",
    "            torch.nn.init.xavier_normal_(linear_layer.weight, gain=5/3)\n",
    "            torch.nn.init.zeros_(linear_layer.bias.data)\n",
    "            \n",
    "            layer_list.append(\n",
    "                (f\"layer_{i}\", linear_layer)\n",
    "            )\n",
    "            layer_list.append((f\"activation_{i}\", self.activation()))\n",
    "        \n",
    "        last_layer = torch.nn.Linear(layers[-2], layers[-1])\n",
    "        torch.nn.init.xavier_normal_(last_layer.weight)\n",
    "        torch.nn.init.zeros_(last_layer.bias.data)\n",
    "        \n",
    "        layer_list.append(\n",
    "            (f\"layer_{self.depth - 1}\", last_layer)\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x = (t, y0)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "300b459c",
   "metadata": {
    "id": "300b459c"
   },
   "outputs": [],
   "source": [
    "# PINN: physics-informed neural network\n",
    "class TcPINN():\n",
    "\n",
    "    def __init__(self, X_pinn, X_semigroup, X_smooth, layers, T):\n",
    "\n",
    "        # neural network architecture\n",
    "        self.layers = layers\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "        \n",
    "        # semigroup PINN step time\n",
    "        self.T = torch.tensor(T).float().to(device)\n",
    "\n",
    "        # training data\n",
    "        self.t_pinn = torch.tensor(X_pinn[:, :1], requires_grad=True).float().to(device)\n",
    "        self.y_pinn = torch.tensor(X_pinn[:, 1:], requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.s_semigroup = torch.tensor(X_semigroup[:, :1], requires_grad=True).float().to(device)\n",
    "        self.t_semigroup = torch.tensor(X_semigroup[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.y_semigroup = torch.tensor(X_semigroup[:, 2:], requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.t_smooth = torch.tensor(X_smooth[:, :1], requires_grad=True).float().to(device)\n",
    "        self.y_smooth = torch.tensor(X_smooth[:, 1:], requires_grad=True).float().to(device)\n",
    "        \n",
    "        # optimization\n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(), lr=1.0, max_iter=50000, max_eval=50000, \n",
    "            history_size=50, tolerance_grad=1e-5, tolerance_change=np.finfo(float).eps, \n",
    "            line_search_fn=\"strong_wolfe\"\n",
    "        )\n",
    "\n",
    "        self.iter = 0\n",
    "    \n",
    "    \n",
    "    def net_y(self, t, y0):\n",
    "        \n",
    "        # The M(t, y0) = y0 + t N(t, y0) scheme seems to drastically increase the accuracy\n",
    "        # This works perfectly fine with automatic differentiation\n",
    "        y = y0 + t * self.dnn(torch.cat([t, y0], dim=1))\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    \n",
    "    def net_derivative(self, t, y0):\n",
    "        \"\"\"\n",
    "        Pytorch automatic differentiation to compute the derivative of the neural network\n",
    "        \"\"\"\n",
    "        y = self.net_y(t, y0)\n",
    "        \n",
    "        # vectors for the autograd vector Jacobian product \n",
    "        # to compute the derivatives w.r.t. every output dimension\n",
    "        vectors = [torch.zeros_like(y) for _ in range(8)]\n",
    "        \n",
    "        for i, vec in enumerate(vectors):\n",
    "            \n",
    "            vec[:,i] = 1.\n",
    "        \n",
    "        # list of derivative tensors\n",
    "        # the first entry is a tensor with \\partial_t PINN_0(t, y0) for all (t, y0) in the batch,\n",
    "        # each input (t, y0) corresponds to one row in each tensor\n",
    "        derivatives = [\n",
    "            torch.autograd.grad(\n",
    "                y, t, \n",
    "                grad_outputs=vec,\n",
    "                retain_graph=True,\n",
    "                create_graph=True\n",
    "            )[0]\n",
    "            for vec in vectors\n",
    "        ]\n",
    "        \n",
    "        return derivatives\n",
    "    \n",
    "    \n",
    "    def loss_function(self):\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = self.net_y(self.t_pinn, self.y_pinn)\n",
    "        deriv_pred = self.net_derivative(self.t_pinn, self.y_pinn)\n",
    "        \n",
    "        # This is specific to the ODE\n",
    "        loss_pinn0 = torch.mean((deriv_pred[0] - y_pred[:,4:5]) ** 2)\n",
    "        loss_pinn1 = torch.mean((deriv_pred[1] - y_pred[:,5:6]) ** 2)\n",
    "        loss_pinn2 = torch.mean((deriv_pred[2] - y_pred[:,6:7]) ** 2)\n",
    "        loss_pinn3 = torch.mean((deriv_pred[3] - y_pred[:,7:8]) ** 2)\n",
    "        \n",
    "        loss_pinn4 = torch.mean((deriv_pred[4] + (y_pred[:,0:1] - y_pred[:,2:3]) / ((y_pred[:,0:1] - y_pred[:,2:3])**2 + (y_pred[:,1:2] - y_pred[:,3:4])**2)**(3/2) + (2*y_pred[:,0:1] + y_pred[:,2:3]) / ((2*y_pred[:,0:1] + y_pred[:,2:3])**2 + (2*y_pred[:,1:2] + y_pred[:,3:4])**2)**(3/2)) ** 2)\n",
    "        loss_pinn5 = torch.mean((deriv_pred[5] + (y_pred[:,1:2] - y_pred[:,3:4]) / ((y_pred[:,0:1] - y_pred[:,2:3])**2 + (y_pred[:,1:2] - y_pred[:,3:4])**2)**(3/2) + (2*y_pred[:,1:2] + y_pred[:,3:4]) / ((2*y_pred[:,0:1] + y_pred[:,2:3])**2 + (2*y_pred[:,1:2] + y_pred[:,3:4])**2)**(3/2)) ** 2)\n",
    "        loss_pinn6 = torch.mean((deriv_pred[6] + (y_pred[:,2:3] - y_pred[:,0:1]) / ((y_pred[:,0:1] - y_pred[:,2:3])**2 + (y_pred[:,1:2] - y_pred[:,3:4])**2)**(3/2) + (2*y_pred[:,2:3] + y_pred[:,0:1]) / ((2*y_pred[:,2:3] + y_pred[:,0:1])**2 + (2*y_pred[:,3:4] + y_pred[:,1:2])**2)**(3/2)) ** 2)\n",
    "        loss_pinn7 = torch.mean((deriv_pred[7] + (y_pred[:,3:4] - y_pred[:,1:2]) / ((y_pred[:,0:1] - y_pred[:,2:3])**2 + (y_pred[:,1:2] - y_pred[:,3:4])**2)**(3/2) + (2*y_pred[:,3:4] + y_pred[:,1:2]) / ((2*y_pred[:,2:3] + y_pred[:,0:1])**2 + (2*y_pred[:,3:4] + y_pred[:,1:2])**2)**(3/2)) ** 2)\n",
    "    \n",
    "\n",
    "        loss_pinn = loss_pinn0 + loss_pinn1 + loss_pinn2 + loss_pinn3 + loss_pinn4 + loss_pinn5 + loss_pinn6 + loss_pinn7\n",
    "        \n",
    "        # The general semigroup loss for autonomous ODEs\n",
    "        y_pred_tps = self.net_y(self.s_semigroup + self.t_semigroup, self.y_semigroup)\n",
    "        y_pred_s = self.net_y(self.s_semigroup, self.y_semigroup)\n",
    "        y_pred_restart = self.net_y(self.t_semigroup, y_pred_s)\n",
    "        loss_semigroup = torch.mean((y_pred_tps - y_pred_restart) ** 2)\n",
    "        \n",
    "        # The general smoothness loss\n",
    "        y_pred_smooth = self.net_y(self.t_smooth, self.y_smooth)\n",
    "        deriv_pred_below = self.net_derivative(self.t_smooth, self.y_smooth)\n",
    "        deriv_pred_above = self.net_derivative(torch.zeros_like(self.t_smooth, requires_grad=True), y_pred_smooth)\n",
    "        \n",
    "        loss_smooth = .0\n",
    "        \n",
    "        for t1, t2 in zip(deriv_pred_below, deriv_pred_above):\n",
    "            \n",
    "            loss_smooth += torch.mean((t1 - t2) ** 2)\n",
    "        \n",
    "        loss = loss_pinn + loss_smooth + loss_semigroup\n",
    "        \n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "        \n",
    "        if self.iter % 1 == 0:\n",
    "            print(\n",
    "                f\"Iter {self.iter}, Loss: {loss.item():.5f}, Loss_pinn: {loss_pinn.item():.5f} \" \\\n",
    "                f\"Loss_smooth: {loss_smooth.item():.5f}, Loss_semigroup: {loss_semigroup.item():.5f}\"\n",
    "            )\n",
    "        \n",
    "        if self.iter % 100 == 0:\n",
    "            \n",
    "            with open(f\"./model.pkl\", \"wb\") as handle:\n",
    "                pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        self.dnn.train()\n",
    "        self.optimizer.step(self.loss_function)\n",
    "    \n",
    "    \n",
    "    def predict(self, t, y0):\n",
    "        \n",
    "        t = torch.tensor(t, requires_grad=True).float().to(device)\n",
    "        y0 = torch.tensor(y0, requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.dnn.eval()\n",
    "        y = self.net_y(t, y0)\n",
    "        y = y.detach().cpu().numpy()\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68934f41",
   "metadata": {
    "id": "68934f41"
   },
   "source": [
    "### Setup Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5ae371e",
   "metadata": {
    "id": "b5ae371e"
   },
   "outputs": [],
   "source": [
    "layers = [9] + 15 * [128] + [8]\n",
    "\n",
    "\n",
    "T = 1\n",
    "max_r = 1.\n",
    "max_v = 0.1\n",
    "\n",
    "# standard PINN loss function training samples\n",
    "N_pinn = 200000\n",
    "N_semigroup = 50000\n",
    "N_smooth = 50000\n",
    "\n",
    "\n",
    "def sample_y(max_r, max_v, N):\n",
    "\n",
    "    r = np.random.uniform(-max_r, max_r, (N, 4))\n",
    "    v = np.random.uniform(-max_v, max_v, (N, 4))\n",
    "    \n",
    "    return np.hstack([r, v])\n",
    "\n",
    "\n",
    "t_pinn = np.random.uniform(0, T, (N_pinn, 1))\n",
    "y_pinn = sample_y(max_r, max_v, N_pinn)\n",
    "X_pinn = np.hstack([t_pinn, y_pinn])\n",
    "\n",
    "\n",
    "r1 = np.random.uniform(0, 1, N_semigroup)\n",
    "r2 = np.random.uniform(0, 1, N_semigroup)\n",
    "s_semigroup, t_semigroup = np.sqrt(r1) * (1 - r2), r2 * np.sqrt(r1)\n",
    "s_semigroup, t_semigroup = T * s_semigroup[:, np.newaxis], T * t_semigroup[:, np.newaxis]\n",
    "y_semigroup = sample_y(max_r, max_v, N_semigroup)\n",
    "X_semigroup = np.hstack([s_semigroup, t_semigroup, y_semigroup])\n",
    "\n",
    "\n",
    "t_smooth = np.random.uniform(0, T, (N_smooth, 1))\n",
    "y_smooth = sample_y(max_r, max_v, N_smooth)\n",
    "X_smooth = np.hstack([t_smooth, y_smooth])\n",
    "\n",
    "\n",
    "with open(\"./X_pinn_center_barycenter.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(X_pinn, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"./X_semigroup_center_barycenter.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(X_semigroup, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"./X_smooth_center_barycenter.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(X_smooth, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e50f9b7",
   "metadata": {
    "id": "5e50f9b7"
   },
   "outputs": [],
   "source": [
    "model = TcPINN(X_pinn, X_semigroup, X_smooth, layers, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeb83529",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 602
    },
    "id": "eeb83529",
    "outputId": "6b5f3689-79b9-4f5b-dc87-ea17574a4ca8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "               \n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zaYQX3QDouAl",
   "metadata": {
    "id": "zaYQX3QDouAl"
   },
   "outputs": [],
   "source": [
    "with open(\"./model.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7db582b4",
   "metadata": {
    "id": "7db582b4"
   },
   "outputs": [],
   "source": [
    "with open(\"./model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd52a59",
   "metadata": {
    "id": "3dd52a59"
   },
   "source": [
    "## Predict and Plot the Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff2236a2",
   "metadata": {
    "id": "ff2236a2"
   },
   "outputs": [],
   "source": [
    "def generate_figure(figsize, xlim, ylim):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.spines[['top', 'right']].set_visible(False)\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_ode_solution(ax, y, index0, index1, *args, **kwargs):\n",
    "    \n",
    "    ax.plot(y[:,index0], y[:,index1], '.-', *args, **kwargs)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f561ea4",
   "metadata": {
    "id": "2f561ea4"
   },
   "outputs": [],
   "source": [
    "def predict_tc(model, y0, max_t_pred, delta_t):\n",
    "    \"\"\"\n",
    "    detla_t should devide model.max_t to guarantee equidistant steps\n",
    "    \"\"\"\n",
    "    times = np.arange(0, model.T + delta_t, delta_t)[1:]\n",
    "    times = times[:,np.newaxis]\n",
    "    n_resets = int(np.ceil(max_t_pred / model.T))\n",
    "    \n",
    "    trajectory = np.array([y0])\n",
    "    \n",
    "    for _ in range(n_resets):\n",
    "        \n",
    "        y0 = trajectory[-1]\n",
    "        y0 = np.array([y0 for _ in range(len(times))])\n",
    "        segment =  model.predict(times, y0)\n",
    "        trajectory = np.vstack([trajectory, segment])\n",
    "    \n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2f012bb",
   "metadata": {
    "id": "f2f012bb"
   },
   "outputs": [],
   "source": [
    "# Note that max_t in training is 1\n",
    "y0 = [1., 0., 0., 1., .0, .0, .0, .0]\n",
    "max_t_pred = 10.\n",
    "delta_t = 0.01\n",
    "\n",
    "validation_tc = predict_tc(model, y0, max_t_pred, delta_t)\n",
    "\n",
    "# this is the key difference: the position of body three can be calculated from body one and two\n",
    "body_three = - (validation_tc[:,0:2] + validation_tc[:,2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21cf58fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "21cf58fc",
    "outputId": "8cdc9634-3739-4ffd-ec18-c1ac64a8d958",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = generate_figure(figsize=(8,8), xlim=[-7, 7], ylim=[-7, 7])\n",
    "\n",
    "ax = plot_ode_solution(ax, validation_tc, 0, 1, markevery=[0], label=\"Body 1\", color=\"#03468F\")\n",
    "ax = plot_ode_solution(ax, validation_tc, 2, 3, markevery=[0], label=\"Body 2\", color=\"#A51C30\")\n",
    "ax = plot_ode_solution(ax, body_three, 0, 1, markevery=[0], label=\"Body 3\", color=\"orange\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"3_body_problem.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3383e64",
   "metadata": {
    "id": "d3383e64"
   },
   "source": [
    "### Reference\n",
    "\n",
    "[1] Breen, Philip G., et al. \"Newton versus the machine: solving the chaotic three-body problem using deep neural networks.\" Monthly Notices of the Royal Astronomical Society 494.2 (2020): 2465-2470."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
