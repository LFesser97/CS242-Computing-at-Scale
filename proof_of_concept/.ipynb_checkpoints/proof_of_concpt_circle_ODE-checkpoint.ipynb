{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c314520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['pgf.texsystem'] = 'pdflatex'\n",
    "matplotlib.rcParams.update({'font.family': 'serif', 'font.size': 10})\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "import time\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc3ced6",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This jupyter notebook provides a baseline implemenation of time-consistent PINNs for a simple system of ODEs of size two. The code is structured as follows:\n",
    "\n",
    "1) A DNN class to define an MLP of arbitrary layer size with Tanh activation. In the tcPINN implementation, the inputs of the DNN will correspond to the time and the initial state (t, $y_0$).\n",
    "\n",
    "2) A TcPINN class implementing the time-consistent physics-informed neural network. The input of the network are the time and the initial state (t, $y_0$) of the ODE solution. The output is $y_0 + t \\cdot N(t, y_0)$, where $N$ is a user defined DNN, and approximates the solution of the ODE at time $t$ with initial state $y_0$. This class includes the definition of the PINN-loss, and the semgigroup- and smoothness-loss functions - the two key contributions of our project.\n",
    "\n",
    "3) An example of how to generate the training data points and train a tcPINN.\n",
    "\n",
    "4) A visualization of the predicted time-consistent ODE solution beyond the maximum time in the training dataset.\n",
    "\n",
    "5) An evaluation of the tcPINN measuring the average Euclidean distance of points on the predicted solution to the (a priori known) true solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6d595",
   "metadata": {},
   "source": [
    "We consider the linear ODE\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{d}{dt} \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix}(t) = \\begin{pmatrix} -y_2 \\\\ y_1 \\end{pmatrix} (t).\n",
    "\\end{align*}\n",
    "\n",
    "For a given initial state $y_0 = (y_0^1, y_0^2) \\in \\mathbb{R}^2$, the solution $(y(t))_{t\\geq0}$ flows in counter clockwise circles and is given by\n",
    "\n",
    "\\begin{align*}\n",
    "    y(t) = \\begin{pmatrix} y_0^1 \\cos(t) - y_0^2 \\sin(t) \\\\ y_0^2 \\cos(t) + y_0^1 \\sin(t) \\end{pmatrix}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74e11fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution_simple_ODE(t, y0):\n",
    "    \n",
    "    y1t = y0[0] * np.cos(t) - y0[1] * np.sin(t)\n",
    "    y2t = y0[1] * np.cos(t) + y0[0] * np.sin(t)\n",
    "\n",
    "    return np.array([y1t, y2t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "135abf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA support \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92eed791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class DNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.Tanh\n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x = (t, y0)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300b459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PINN: physics-informed neural network\n",
    "class TcPINN():\n",
    "\n",
    "    def __init__(self, X_pinn, X_semigroup, X_smooth, layers, T):\n",
    "\n",
    "        # neural network architecture\n",
    "        self.layers = layers\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "        \n",
    "        # semigroup PINN step time\n",
    "        self.T = torch.tensor(T).float().to(device)\n",
    "\n",
    "        # training data\n",
    "        self.t_pinn = torch.tensor(X_pinn[:, :1], requires_grad=True).float().to(device)\n",
    "        self.y_pinn = torch.tensor(X_pinn[:, 1:], requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.s_semigroup = torch.tensor(X_semigroup[:, :1], requires_grad=True).float().to(device)\n",
    "        self.t_semigroup = torch.tensor(X_semigroup[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.y_semigroup = torch.tensor(X_semigroup[:, 2:], requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.t_smooth = torch.tensor(X_smooth[:, :1], requires_grad=True).float().to(device)\n",
    "        self.y_smooth = torch.tensor(X_smooth[:, 1:], requires_grad=True).float().to(device)\n",
    "        \n",
    "        # optimization\n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(), lr=1.0, max_iter=50000, max_eval=50000, \n",
    "            history_size=50, tolerance_grad=1e-5, tolerance_change=np.finfo(float).eps, \n",
    "            line_search_fn=\"strong_wolfe\"\n",
    "        )\n",
    "\n",
    "        self.iter = 0\n",
    "    \n",
    "    \n",
    "    def net_y(self, t, y0):\n",
    "        \n",
    "        # The M(t, y0) = y0 + t N(t, y0) scheme seems to drastically increase the accuracy\n",
    "        # This works perfectly fine with automatic differentiation\n",
    "        y = y0 + t * self.dnn(torch.cat([t, y0], dim=1))\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    \n",
    "    def net_derivative(self, t, y0):\n",
    "        \"\"\"\n",
    "        Pytorch automatic differentiation to compute the derivative of the neural network\n",
    "        \"\"\"\n",
    "        y = self.net_y(t, y0)\n",
    "        \n",
    "        # vectors for the autograd vector Jacobian product \n",
    "        # to compute the derivatives w.r.t. every output dimension\n",
    "        vectors = [torch.zeros_like(y) for i in range(2)]\n",
    "        \n",
    "        for i, vec in enumerate(vectors):\n",
    "            \n",
    "            vec[:,i] = 1.\n",
    "        \n",
    "        # list of derivative tensors\n",
    "        # the first entry is a tensor with \\partial_t PINN(t, y0) for all (t, y0) in the batch,\n",
    "        # each input (t, y0) corresponds to one row in each tensor\n",
    "        derivatives = [\n",
    "            torch.autograd.grad(\n",
    "                y, t, \n",
    "                grad_outputs=vec,\n",
    "                retain_graph=True,\n",
    "                create_graph=True\n",
    "            )[0]\n",
    "            for vec in vectors\n",
    "        ]\n",
    "        \n",
    "        return derivatives\n",
    "    \n",
    "    \n",
    "    def loss_function(self):\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = self.net_y(self.t_pinn, self.y_pinn)\n",
    "        deriv_pred = self.net_derivative(self.t_pinn, self.y_pinn)\n",
    "        \n",
    "        # This is specific to the ODE\n",
    "        loss_pinn1 = torch.mean((deriv_pred[0] + y_pred[:,1:2]) ** 2)\n",
    "        loss_pinn2 = torch.mean((deriv_pred[1] - y_pred[:,0:1]) ** 2)\n",
    "        loss_pinn = loss_pinn1 + loss_pinn2\n",
    "        \n",
    "        # The general semigroup loss for autonomous ODEs\n",
    "        y_pred_tps = self.net_y(self.s_semigroup + self.t_semigroup, self.y_semigroup)\n",
    "        y_pred_s = self.net_y(self.s_semigroup, self.y_semigroup)\n",
    "        y_pred_restart = self.net_y(self.t_semigroup, y_pred_s)\n",
    "        loss_semigroup = torch.mean((y_pred_tps - y_pred_restart) ** 2)\n",
    "        \n",
    "        # The general smoothness loss\n",
    "        y_pred_smooth = self.net_y(self.t_smooth, self.y_smooth)\n",
    "        deriv_pred_below = self.net_derivative(self.t_smooth, self.y_smooth)\n",
    "        deriv_pred_above = self.net_derivative(torch.zeros_like(self.t_smooth, requires_grad=True), y_pred_smooth)\n",
    "        \n",
    "        loss_smooth = .0\n",
    "        \n",
    "        for t1, t2 in zip(deriv_pred_below, deriv_pred_above):\n",
    "            \n",
    "            loss_smooth += torch.mean((t1 - t2) ** 2)\n",
    "        \n",
    "        loss = loss_pinn + loss_smooth + loss_semigroup\n",
    "        \n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "        \n",
    "        if self.iter % 100 == 0:\n",
    "            print(\n",
    "                f\"Iter {self.iter}, Loss: {loss.item():.5f}, Loss_pinn: {loss_pinn.item():.5f} \" \\\n",
    "                f\"Loss_smooth: {loss_smooth.item():.5f}, Loss_semigroup: {loss_semigroup.item():.5f}\"\n",
    "            )\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        self.dnn.train()\n",
    "        self.optimizer.step(self.loss_function)\n",
    "    \n",
    "    \n",
    "    def predict(self, t, y0):\n",
    "        \n",
    "        t = torch.tensor(t, requires_grad=True).float().to(device)\n",
    "        y0 = torch.tensor(y0, requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.dnn.eval()\n",
    "        y = self.net_y(t, y0)\n",
    "        y = y.detach().cpu().numpy()\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68934f41",
   "metadata": {},
   "source": [
    "### Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5ae371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [3, 32, 32, 32, 32,32, 2]\n",
    "\n",
    "T = 1\n",
    "max_y0 = 5\n",
    "\n",
    "# standard PINN loss function training samples\n",
    "N_pinn = 20000\n",
    "N_semigroup = 20000\n",
    "N_smooth = 20000\n",
    "\n",
    "\n",
    "t_pinn = np.random.uniform(0, T, (N_pinn, 1))\n",
    "y_pinn = np.random.uniform(0, max_y0, (N_pinn, 2))\n",
    "X_pinn = np.hstack([t_pinn, y_pinn])\n",
    "\n",
    "\n",
    "r1 = np.random.uniform(0, 1, N_semigroup)\n",
    "r2 = np.random.uniform(0, 1, N_semigroup)\n",
    "s_semigroup, t_semigroup = np.sqrt(r1) * (1 - r2), r2 * np.sqrt(r1)\n",
    "s_semigroup, t_semigroup = T * s_semigroup[:, np.newaxis], T * t_semigroup[:, np.newaxis]\n",
    "y_semigroup = np.random.uniform(0, max_y0, (N_semigroup, 2))\n",
    "X_semigroup = np.hstack([s_semigroup, t_semigroup, y_semigroup])\n",
    "\n",
    "\n",
    "t_smooth = np.random.uniform(0, T, (N_smooth, 1))\n",
    "y_smooth = np.random.uniform(0, max_y0, (N_smooth, 2))\n",
    "X_smooth = np.hstack([t_smooth, y_smooth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e50f9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TcPINN(X_pinn, X_semigroup, X_smooth, layers, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb83529",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100, Loss: 0.00512, Loss_pinn: 0.00261 Loss_smooth: 0.00240, Loss_semigroup: 0.00010\n",
      "Iter 200, Loss: 0.00088, Loss_pinn: 0.00046 Loss_smooth: 0.00040, Loss_semigroup: 0.00003\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "               \n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd52a59",
   "metadata": {},
   "source": [
    "## Predict and Plot the Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2236a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_figure(figsize, xlim, ylim):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.spines[['top', 'right']].set_visible(False)\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_ode_solution(ax, y, *args, **kwargs):\n",
    "    \n",
    "    ax.plot(y[:,0], y[:,1], '.-', *args, **kwargs)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f561ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_standard(model, y0, max_t_pred, delta_t):\n",
    "    \n",
    "    times = np.linspace(0, max_t_pred, int(max_t_pred / delta_t) + 1)\n",
    "    times = times[:,np.newaxis]\n",
    "    \n",
    "    y0 = np.array([y0 for _ in range(len(times))])\n",
    "    trajectory =  model.predict(times, y0)\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "\n",
    "def predict_dac(model, y0, max_t_pred, delta_t):\n",
    "    \"\"\"\n",
    "    detla_t should devide model.max_t to guarantee equidistant steps\n",
    "    \"\"\"\n",
    "    times = np.arange(0, model.T + delta_t, delta_t)[1:]\n",
    "    times = times[:,np.newaxis]\n",
    "    n_resets = int(np.ceil(max_t_pred / model.T))\n",
    "    \n",
    "    trajectory = np.array([y0])\n",
    "    \n",
    "    for _ in range(n_resets):\n",
    "        \n",
    "        y0 = trajectory[-1]\n",
    "        y0 = np.array([y0 for _ in range(len(times))])\n",
    "        segment =  model.predict(times, y0)\n",
    "        trajectory = np.vstack([trajectory, segment])\n",
    "    \n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f012bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that max_t in training is 1\n",
    "y0 = [1., 0.]\n",
    "max_t_pred = 7.\n",
    "delta_t = 0.05\n",
    "\n",
    "validation_dac = predict_dac(model, y0, max_t_pred, delta_t)\n",
    "\n",
    "# demonstrating that neural networks don't generalize beyond their training data\n",
    "validation_standard = predict_standard(model, y0, max_t_pred, delta_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef80127",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_solution = np.array([\n",
    "    solution_simple_ODE(t, y0) \n",
    "    for t in np.linspace(0, max_t_pred, int(max_t_pred / delta_t) + 1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cf58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = generate_figure(figsize=(8,8), xlim=[-1.5, 1.5], ylim=[-1.5, 1.5])\n",
    "\n",
    "ax = plot_ode_solution(ax, validation_standard, label=\"Standard approach\", color=\"#03468F\")\n",
    "ax = plot_ode_solution(ax, validation_dac, label=\"tcPINN\", color=\"#A51C30\")\n",
    "ax = plot_ode_solution(ax, true_solution, label=\"true solution\", color=\"orange\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"proof_of_concept.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45781df",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2eb24e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c102e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall:\n",
    "# layers = [3, 32, 32, 32, 32,32, 2]\n",
    "# T = 1\n",
    "# max_y0 = 5\n",
    "# N_pinn = 20000\n",
    "# N_semigroup = 20000\n",
    "# N_smooth = 20000\n",
    "\n",
    "N_evaluate = 1000\n",
    "\n",
    "y0s = np.random.uniform(0, max_y0, (N_evaluate,2))\n",
    "\n",
    "max_t_pred = 3.\n",
    "delta_t = 0.01\n",
    "distances = []\n",
    "\n",
    "\n",
    "for y0 in y0s:\n",
    "    \n",
    "    dac_solution = predict_dac(model, y0, max_t_pred, delta_t)\n",
    "    true_solution = np.array([\n",
    "        solution_simple_ODE(t, y0) \n",
    "        for t in np.linspace(0, max_t_pred, int(max_t_pred / delta_t) + 1)\n",
    "    ])\n",
    "    distances.append(np.sqrt(np.sum((dac_solution - true_solution) ** 2, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb4f5713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Euclidean distance: 8.987E-2, Std: 2.708E-1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean Euclidean distance: {Decimal(np.mean(distances)):.3E}, Std: {Decimal(np.std(distances)):.3E}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
