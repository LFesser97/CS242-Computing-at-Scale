{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c314520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['pgf.texsystem'] = 'pdflatex'\n",
    "matplotlib.rcParams.update({'font.family': 'serif', 'font.size': 10})\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "import time\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6ba0ff",
   "metadata": {},
   "source": [
    "Start with a PINN so solve a single initial value problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6d595",
   "metadata": {},
   "source": [
    "As a proof of concept of divide and conquer phyiscs informed neural networks, we consider the ODE\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{d}{dt} y(t) = y(t).\n",
    "\\end{align*}\n",
    "\n",
    "For a given initial state $y_0 \\in \\mathbb{R}$, the solution $(y(t))_{t\\geq0}$ is given by\n",
    "\n",
    "\\begin{align*}\n",
    "    y(t) = e^t y_0.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74e11fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_field(y):\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "# the true solution\n",
    "def solution_simple_ODE(t, y0):\n",
    "    \n",
    "    return np.exp(t) * y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "135abf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA support \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "92eed791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class DNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.Tanh\n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x = (t, y0)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "300b459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PINN: physics-informed neural network\n",
    "class PINN():\n",
    "\n",
    "    def __init__(self, t_pinn, layers, max_t):\n",
    "        \n",
    "        self.vector_field = vector_field\n",
    "        self.layers = layers\n",
    "\n",
    "        # max training time of the semigroup PINN\n",
    "        self.max_t = torch.tensor(max_t).float().to(device)\n",
    "\n",
    "        # training data\n",
    "        self.t_pinn = torch.tensor(t_pinn, requires_grad=True).float().to(device)\n",
    "        \n",
    "        # neural network architecture\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "        \n",
    "        #self.optimizer = torch.optim.Adam(self.dnn.parameters(), lr=1.)\n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(), lr=1.0, max_iter=50000, max_eval=50000, \n",
    "            history_size=50, tolerance_grad=1e-5, tolerance_change=1.0 * np.finfo(float).eps, \n",
    "            line_search_fn=\"strong_wolfe\"\n",
    "        )\n",
    "\n",
    "        self.iter = 0\n",
    "    \n",
    "    \n",
    "    def net_y(self, t):\n",
    "        \n",
    "        #y = y0 + t * self.dnn(torch.cat([t, y0], dim=1))\n",
    "        y = self.dnn(t)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    \n",
    "    def net_f(self, t):\n",
    "        \"\"\"\n",
    "        Pytorch automatic differentiation to compute the derivative of the neural network\n",
    "        \"\"\"\n",
    "        y = self.net_y(t)\n",
    "        \n",
    "        y_t = torch.autograd.grad(\n",
    "            y, t, \n",
    "            grad_outputs=torch.ones_like(y),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        # the example ODE loss\n",
    "        f = y_t - y\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    \n",
    "    def loss_function(self):\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        f_pred = self.net_f(self.t_pinn)\n",
    "        loss_pinn = torch.mean(f_pred ** 2)\n",
    "        \n",
    "        # initial value y(0) = 1\n",
    "        loss_init = (self.net_y(torch.tensor([[.0]])) - torch.tensor([[1.]])) ** 2\n",
    "        \n",
    "        loss = loss_pinn + loss_init\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        self.dnn.train()\n",
    "                \n",
    "        # Backward and optimize\n",
    "        self.optimizer.step(self.loss_function)\n",
    "\n",
    "            \n",
    "    def predict(self, times):\n",
    "        \n",
    "        t = torch.tensor(times, requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        y = self.net_y(t)\n",
    "        y = y.detach().cpu().numpy()\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68934f41",
   "metadata": {},
   "source": [
    "### Setup data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b5ae371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [1, 20, 20, 20, 20, 1]\n",
    "\n",
    "max_t = 4\n",
    "\n",
    "\n",
    "# standard PINN loss function training samples\n",
    "N_pinn = 10000\n",
    "\n",
    "t_pinn = np.random.uniform(0, max_t, (N_pinn,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6d83191e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_pinn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "5e50f9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ADD THE DATASETS OF THE OTHER LOSS FUNCTIONS\n",
    "model = PINN(t_pinn, layers, max_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "eeb83529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2, Loss: 0.97548, Loss_pinn: 0.00095\n",
      "Iter 4, Loss: 0.43906, Loss_pinn: 0.20872\n",
      "Iter 6, Loss: 0.36025, Loss_pinn: 0.14778\n",
      "Iter 8, Loss: 0.33759, Loss_pinn: 0.22048\n",
      "Iter 10, Loss: 0.33554, Loss_pinn: 0.22341\n",
      "Iter 12, Loss: 0.33066, Loss_pinn: 0.22861\n",
      "Iter 14, Loss: 0.33011, Loss_pinn: 0.22096\n",
      "Iter 16, Loss: 0.33010, Loss_pinn: 0.22135\n",
      "Iter 18, Loss: 0.33009, Loss_pinn: 0.22052\n",
      "Iter 20, Loss: 0.33005, Loss_pinn: 0.21957\n",
      "Iter 22, Loss: 0.32999, Loss_pinn: 0.22090\n",
      "Iter 24, Loss: 0.32999, Loss_pinn: 0.22120\n",
      "Iter 26, Loss: 0.32999, Loss_pinn: 0.22104\n",
      "Iter 28, Loss: 0.32995, Loss_pinn: 0.22210\n",
      "Iter 30, Loss: 0.33010, Loss_pinn: 0.22490\n",
      "Iter 32, Loss: 0.32982, Loss_pinn: 0.22088\n",
      "Iter 34, Loss: 0.32968, Loss_pinn: 0.22019\n",
      "Iter 36, Loss: 0.32963, Loss_pinn: 0.22260\n",
      "Iter 38, Loss: 0.32959, Loss_pinn: 0.22235\n",
      "Iter 40, Loss: 0.32980, Loss_pinn: 0.22741\n",
      "Iter 42, Loss: 0.32918, Loss_pinn: 0.22931\n",
      "Iter 44, Loss: 0.32901, Loss_pinn: 0.21445\n",
      "Iter 46, Loss: 0.32931, Loss_pinn: 0.21611\n",
      "Iter 48, Loss: 0.32880, Loss_pinn: 0.22091\n",
      "Iter 50, Loss: 0.32879, Loss_pinn: 0.22026\n",
      "Iter 52, Loss: 0.32874, Loss_pinn: 0.22035\n",
      "Iter 54, Loss: 0.32870, Loss_pinn: 0.22142\n",
      "Iter 56, Loss: 0.32926, Loss_pinn: 0.20726\n",
      "Iter 58, Loss: 0.32815, Loss_pinn: 0.21478\n",
      "Iter 60, Loss: 0.32890, Loss_pinn: 0.20888\n",
      "Iter 62, Loss: 0.33009, Loss_pinn: 0.23424\n",
      "Iter 64, Loss: 0.32719, Loss_pinn: 0.21765\n",
      "Iter 66, Loss: 0.32715, Loss_pinn: 0.21888\n",
      "Iter 68, Loss: 0.32714, Loss_pinn: 0.21849\n",
      "Iter 70, Loss: 0.32712, Loss_pinn: 0.21994\n",
      "Iter 72, Loss: 0.32704, Loss_pinn: 0.21693\n",
      "Iter 74, Loss: 0.32682, Loss_pinn: 0.21093\n",
      "Iter 76, Loss: 0.32662, Loss_pinn: 0.21621\n",
      "Iter 78, Loss: 0.32616, Loss_pinn: 0.21867\n",
      "Iter 80, Loss: 0.32588, Loss_pinn: 0.22142\n",
      "Iter 82, Loss: 0.32568, Loss_pinn: 0.21790\n",
      "Iter 84, Loss: 0.32549, Loss_pinn: 0.21768\n",
      "Iter 86, Loss: 0.32551, Loss_pinn: 0.20981\n",
      "Iter 88, Loss: 0.32505, Loss_pinn: 0.21760\n",
      "Iter 90, Loss: 0.32490, Loss_pinn: 0.21736\n",
      "Iter 92, Loss: 0.32482, Loss_pinn: 0.21649\n",
      "Iter 94, Loss: 0.32466, Loss_pinn: 0.21730\n",
      "Iter 96, Loss: 0.32411, Loss_pinn: 0.21645\n",
      "Iter 98, Loss: 0.32283, Loss_pinn: 0.20934\n",
      "Iter 100, Loss: 0.32227, Loss_pinn: 0.21034\n",
      "Iter 102, Loss: 0.32167, Loss_pinn: 0.21877\n",
      "Iter 104, Loss: 0.32090, Loss_pinn: 0.20993\n",
      "Iter 106, Loss: 0.32065, Loss_pinn: 0.21368\n",
      "Iter 108, Loss: 0.32051, Loss_pinn: 0.21229\n",
      "Iter 110, Loss: 0.32033, Loss_pinn: 0.21481\n",
      "Iter 112, Loss: 0.31976, Loss_pinn: 0.21191\n",
      "Iter 114, Loss: 0.31935, Loss_pinn: 0.21141\n",
      "Iter 116, Loss: 0.31867, Loss_pinn: 0.21809\n",
      "Iter 118, Loss: 0.31822, Loss_pinn: 0.21692\n",
      "Iter 120, Loss: 0.31901, Loss_pinn: 0.22562\n",
      "Iter 122, Loss: 0.31793, Loss_pinn: 0.21741\n",
      "Iter 124, Loss: 0.31690, Loss_pinn: 0.21114\n",
      "Iter 126, Loss: 0.31620, Loss_pinn: 0.20235\n",
      "Iter 128, Loss: 0.34290, Loss_pinn: 0.19224\n",
      "Iter 130, Loss: 0.31375, Loss_pinn: 0.21247\n",
      "Iter 132, Loss: 0.31309, Loss_pinn: 0.20724\n",
      "Iter 134, Loss: 0.31265, Loss_pinn: 0.21779\n",
      "Iter 136, Loss: 0.31236, Loss_pinn: 0.21149\n",
      "Iter 138, Loss: 0.31218, Loss_pinn: 0.21255\n",
      "Iter 140, Loss: 0.31208, Loss_pinn: 0.21218\n",
      "Iter 142, Loss: 0.31177, Loss_pinn: 0.21099\n",
      "Iter 144, Loss: 0.31116, Loss_pinn: 0.20609\n",
      "Iter 146, Loss: 0.30945, Loss_pinn: 0.21035\n",
      "Iter 148, Loss: 0.30830, Loss_pinn: 0.19973\n",
      "Iter 150, Loss: 0.30793, Loss_pinn: 0.21050\n",
      "Iter 152, Loss: 0.35456, Loss_pinn: 0.11203\n",
      "Iter 154, Loss: 0.31267, Loss_pinn: 0.23329\n",
      "Iter 156, Loss: 0.31590, Loss_pinn: 0.22724\n",
      "Iter 158, Loss: 0.30456, Loss_pinn: 0.20933\n",
      "Iter 160, Loss: 0.30324, Loss_pinn: 0.18481\n",
      "Iter 162, Loss: 0.29956, Loss_pinn: 0.19037\n",
      "Iter 164, Loss: 0.29808, Loss_pinn: 0.19920\n",
      "Iter 166, Loss: 0.29767, Loss_pinn: 0.19563\n",
      "Iter 168, Loss: 0.29784, Loss_pinn: 0.18559\n",
      "Iter 170, Loss: 0.29649, Loss_pinn: 0.19696\n",
      "Iter 172, Loss: 0.29532, Loss_pinn: 0.20348\n",
      "Iter 174, Loss: 0.29474, Loss_pinn: 0.19687\n",
      "Iter 176, Loss: 0.29340, Loss_pinn: 0.20824\n",
      "Iter 178, Loss: 0.29427, Loss_pinn: 0.20282\n",
      "Iter 180, Loss: 0.28669, Loss_pinn: 0.19059\n",
      "Iter 182, Loss: 0.28571, Loss_pinn: 0.18875\n",
      "Iter 184, Loss: 0.35828, Loss_pinn: 0.34864\n",
      "Iter 186, Loss: 0.31139, Loss_pinn: 0.19168\n",
      "Iter 188, Loss: 0.28504, Loss_pinn: 0.17829\n",
      "Iter 190, Loss: 0.27780, Loss_pinn: 0.19396\n",
      "Iter 192, Loss: 0.27748, Loss_pinn: 0.19305\n",
      "Iter 194, Loss: 0.27742, Loss_pinn: 0.19243\n",
      "Iter 196, Loss: 0.27732, Loss_pinn: 0.19295\n",
      "Iter 198, Loss: 0.27724, Loss_pinn: 0.19279\n",
      "Iter 200, Loss: 0.27718, Loss_pinn: 0.19297\n",
      "Iter 202, Loss: 0.27691, Loss_pinn: 0.19189\n",
      "Iter 204, Loss: 0.27677, Loss_pinn: 0.19112\n",
      "Iter 206, Loss: 0.27662, Loss_pinn: 0.19062\n",
      "Iter 208, Loss: 0.27588, Loss_pinn: 0.19052\n",
      "Iter 210, Loss: 0.27345, Loss_pinn: 0.18786\n",
      "Iter 212, Loss: 0.27183, Loss_pinn: 0.18523\n",
      "Iter 214, Loss: 0.27012, Loss_pinn: 0.17938\n",
      "Iter 216, Loss: 0.26970, Loss_pinn: 0.18430\n",
      "Iter 218, Loss: 0.26878, Loss_pinn: 0.18820\n",
      "Iter 220, Loss: 0.26845, Loss_pinn: 0.18658\n",
      "Iter 222, Loss: 0.26821, Loss_pinn: 0.18826\n",
      "Iter 224, Loss: 0.26732, Loss_pinn: 0.18585\n",
      "Iter 226, Loss: 0.27659, Loss_pinn: 0.22131\n",
      "Iter 228, Loss: 0.26678, Loss_pinn: 0.18326\n",
      "Iter 230, Loss: 0.26657, Loss_pinn: 0.18444\n",
      "Iter 232, Loss: 0.26649, Loss_pinn: 0.18537\n",
      "Iter 234, Loss: 0.26635, Loss_pinn: 0.18159\n",
      "Iter 236, Loss: 0.26626, Loss_pinn: 0.18223\n",
      "Iter 238, Loss: 0.26623, Loss_pinn: 0.18234\n",
      "Iter 240, Loss: 0.26617, Loss_pinn: 0.18162\n",
      "Iter 242, Loss: 0.26614, Loss_pinn: 0.18171\n",
      "Iter 244, Loss: 0.26612, Loss_pinn: 0.18122\n",
      "Iter 246, Loss: 0.26610, Loss_pinn: 0.18187\n",
      "Iter 248, Loss: 0.26612, Loss_pinn: 0.18638\n",
      "Iter 250, Loss: 0.26604, Loss_pinn: 0.18311\n",
      "Iter 252, Loss: 0.26604, Loss_pinn: 0.18250\n",
      "Iter 254, Loss: 0.26596, Loss_pinn: 0.18068\n",
      "Iter 256, Loss: 0.26561, Loss_pinn: 0.17858\n",
      "Iter 258, Loss: 0.33753, Loss_pinn: 0.26622\n",
      "Iter 260, Loss: 0.26517, Loss_pinn: 0.17524\n",
      "Iter 262, Loss: 0.26555, Loss_pinn: 0.15952\n",
      "Iter 264, Loss: 0.26475, Loss_pinn: 0.17952\n",
      "Iter 266, Loss: 0.26322, Loss_pinn: 0.17964\n",
      "Iter 268, Loss: 0.26234, Loss_pinn: 0.17767\n",
      "Iter 270, Loss: 0.26026, Loss_pinn: 0.17363\n",
      "Iter 272, Loss: 0.25807, Loss_pinn: 0.17087\n",
      "Iter 274, Loss: 0.25759, Loss_pinn: 0.16497\n",
      "Iter 276, Loss: 0.25540, Loss_pinn: 0.15681\n",
      "Iter 278, Loss: 0.25548, Loss_pinn: 0.18708\n",
      "Iter 280, Loss: 0.25139, Loss_pinn: 0.16367\n",
      "Iter 282, Loss: 0.24957, Loss_pinn: 0.17930\n",
      "Iter 284, Loss: 0.24634, Loss_pinn: 0.16163\n",
      "Iter 286, Loss: 0.24370, Loss_pinn: 0.15818\n",
      "Iter 288, Loss: 0.24063, Loss_pinn: 0.16905\n",
      "Iter 290, Loss: 0.24003, Loss_pinn: 0.16495\n",
      "Iter 292, Loss: 0.23763, Loss_pinn: 0.16082\n",
      "Iter 294, Loss: 0.23625, Loss_pinn: 0.16549\n",
      "Iter 296, Loss: 2.68956, Loss_pinn: 1.85223\n",
      "Iter 298, Loss: 0.22936, Loss_pinn: 0.14944\n",
      "Iter 300, Loss: 0.21958, Loss_pinn: 0.14913\n",
      "Iter 302, Loss: 0.21663, Loss_pinn: 0.14687\n",
      "Iter 304, Loss: 0.22273, Loss_pinn: 0.11990\n",
      "Iter 306, Loss: 0.21409, Loss_pinn: 0.14468\n",
      "Iter 308, Loss: 0.21166, Loss_pinn: 0.14193\n",
      "Iter 310, Loss: 0.21752, Loss_pinn: 0.11650\n",
      "Iter 312, Loss: 0.20570, Loss_pinn: 0.13727\n",
      "Iter 314, Loss: 0.20350, Loss_pinn: 0.13258\n",
      "Iter 316, Loss: 0.20164, Loss_pinn: 0.14799\n",
      "Iter 318, Loss: 0.19810, Loss_pinn: 0.12961\n",
      "Iter 320, Loss: 0.19541, Loss_pinn: 0.12555\n",
      "Iter 322, Loss: 0.19032, Loss_pinn: 0.12112\n",
      "Iter 324, Loss: 0.18793, Loss_pinn: 0.13281\n",
      "Iter 326, Loss: 0.18705, Loss_pinn: 0.13418\n",
      "Iter 328, Loss: 0.18606, Loss_pinn: 0.13196\n",
      "Iter 330, Loss: 0.18580, Loss_pinn: 0.13215\n",
      "Iter 332, Loss: 0.18419, Loss_pinn: 0.13477\n",
      "Iter 334, Loss: 0.18187, Loss_pinn: 0.12835\n",
      "Iter 336, Loss: 0.18129, Loss_pinn: 0.12755\n",
      "Iter 338, Loss: 0.18111, Loss_pinn: 0.12682\n",
      "Iter 340, Loss: 0.18087, Loss_pinn: 0.12742\n",
      "Iter 342, Loss: 0.18062, Loss_pinn: 0.12390\n",
      "Iter 344, Loss: 0.17953, Loss_pinn: 0.12287\n",
      "Iter 346, Loss: 0.17393, Loss_pinn: 0.12659\n",
      "Iter 348, Loss: 0.17114, Loss_pinn: 0.12269\n",
      "Iter 350, Loss: 0.17170, Loss_pinn: 0.10008\n",
      "Iter 352, Loss: 0.16637, Loss_pinn: 0.12853\n",
      "Iter 354, Loss: 0.16760, Loss_pinn: 0.11396\n",
      "Iter 356, Loss: 0.16439, Loss_pinn: 0.12087\n",
      "Iter 358, Loss: 0.16416, Loss_pinn: 0.11669\n",
      "Iter 360, Loss: 0.16398, Loss_pinn: 0.11997\n",
      "Iter 362, Loss: 0.16365, Loss_pinn: 0.12024\n",
      "Iter 364, Loss: 0.16244, Loss_pinn: 0.12017\n",
      "Iter 366, Loss: 0.16213, Loss_pinn: 0.12305\n",
      "Iter 368, Loss: 0.16184, Loss_pinn: 0.10980\n",
      "Iter 370, Loss: 0.16072, Loss_pinn: 0.11929\n",
      "Iter 372, Loss: 0.15965, Loss_pinn: 0.11608\n",
      "Iter 374, Loss: 0.15883, Loss_pinn: 0.11485\n",
      "Iter 376, Loss: 0.15863, Loss_pinn: 0.11516\n",
      "Iter 378, Loss: 0.15711, Loss_pinn: 0.11721\n",
      "Iter 380, Loss: 0.15145, Loss_pinn: 0.10801\n",
      "Iter 382, Loss: 0.15145, Loss_pinn: 0.11105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 384, Loss: 0.14473, Loss_pinn: 0.11746\n",
      "Iter 386, Loss: 0.29531, Loss_pinn: 0.14958\n",
      "Iter 388, Loss: 0.16825, Loss_pinn: 0.15776\n",
      "Iter 390, Loss: 0.19885, Loss_pinn: 0.13801\n",
      "Iter 392, Loss: 0.13079, Loss_pinn: 0.10629\n",
      "Iter 394, Loss: 0.12739, Loss_pinn: 0.10113\n",
      "Iter 396, Loss: 0.12368, Loss_pinn: 0.09947\n",
      "Iter 398, Loss: 0.12287, Loss_pinn: 0.09501\n",
      "Iter 400, Loss: 0.12122, Loss_pinn: 0.09902\n",
      "Iter 402, Loss: 0.12070, Loss_pinn: 0.09666\n",
      "Iter 404, Loss: 0.11958, Loss_pinn: 0.09549\n",
      "Iter 406, Loss: 0.11908, Loss_pinn: 0.09621\n",
      "Iter 408, Loss: 0.11878, Loss_pinn: 0.09771\n",
      "Iter 410, Loss: 0.11639, Loss_pinn: 0.10002\n",
      "Iter 412, Loss: 0.10909, Loss_pinn: 0.09249\n",
      "Iter 414, Loss: 0.11244, Loss_pinn: 0.08391\n",
      "Iter 416, Loss: 0.10452, Loss_pinn: 0.07477\n",
      "Iter 418, Loss: 0.10315, Loss_pinn: 0.07896\n",
      "Iter 420, Loss: 0.10177, Loss_pinn: 0.07752\n",
      "Iter 422, Loss: 0.10098, Loss_pinn: 0.07885\n",
      "Iter 424, Loss: 0.10069, Loss_pinn: 0.07912\n",
      "Iter 426, Loss: 0.10002, Loss_pinn: 0.07846\n",
      "Iter 428, Loss: 0.09909, Loss_pinn: 0.07763\n",
      "Iter 430, Loss: 0.09633, Loss_pinn: 0.07668\n",
      "Iter 432, Loss: 0.09637, Loss_pinn: 0.06678\n",
      "Iter 434, Loss: 0.08996, Loss_pinn: 0.06432\n",
      "Iter 436, Loss: 0.08585, Loss_pinn: 0.06719\n",
      "Iter 438, Loss: 0.09022, Loss_pinn: 0.07398\n",
      "Iter 440, Loss: 0.08408, Loss_pinn: 0.06195\n",
      "Iter 442, Loss: 0.08333, Loss_pinn: 0.06292\n",
      "Iter 444, Loss: 0.08273, Loss_pinn: 0.06046\n",
      "Iter 446, Loss: 0.08239, Loss_pinn: 0.05998\n",
      "Iter 448, Loss: 0.08184, Loss_pinn: 0.05906\n",
      "Iter 450, Loss: 0.08189, Loss_pinn: 0.05861\n",
      "Iter 452, Loss: 0.08169, Loss_pinn: 0.05860\n",
      "Iter 454, Loss: 0.08151, Loss_pinn: 0.05803\n",
      "Iter 456, Loss: 0.08149, Loss_pinn: 0.05785\n",
      "Iter 458, Loss: 0.08139, Loss_pinn: 0.05782\n",
      "Iter 460, Loss: 0.08130, Loss_pinn: 0.05821\n",
      "Iter 462, Loss: 0.08140, Loss_pinn: 0.05762\n",
      "Iter 464, Loss: 0.08115, Loss_pinn: 0.05756\n",
      "Iter 466, Loss: 0.08090, Loss_pinn: 0.05701\n",
      "Iter 468, Loss: 0.29925, Loss_pinn: 0.29169\n",
      "Iter 470, Loss: 0.08038, Loss_pinn: 0.05614\n",
      "Iter 472, Loss: 0.07993, Loss_pinn: 0.05588\n",
      "Iter 474, Loss: 0.07963, Loss_pinn: 0.05693\n",
      "Iter 476, Loss: 0.07888, Loss_pinn: 0.05356\n",
      "Iter 478, Loss: 0.07853, Loss_pinn: 0.05546\n",
      "Iter 480, Loss: 0.07760, Loss_pinn: 0.05815\n",
      "Iter 482, Loss: 0.07708, Loss_pinn: 0.05818\n",
      "Iter 484, Loss: 0.07697, Loss_pinn: 0.05766\n",
      "Iter 486, Loss: 0.07631, Loss_pinn: 0.05971\n",
      "Iter 488, Loss: 0.07532, Loss_pinn: 0.05557\n",
      "Iter 490, Loss: 0.07495, Loss_pinn: 0.05739\n",
      "Iter 492, Loss: 0.07412, Loss_pinn: 0.05607\n",
      "Iter 494, Loss: 0.07340, Loss_pinn: 0.05347\n",
      "Iter 496, Loss: 0.07289, Loss_pinn: 0.05592\n",
      "Iter 498, Loss: 0.07241, Loss_pinn: 0.05420\n",
      "Iter 500, Loss: 0.07199, Loss_pinn: 0.05505\n",
      "Iter 502, Loss: 0.07149, Loss_pinn: 0.05500\n",
      "Iter 504, Loss: 0.07127, Loss_pinn: 0.05393\n",
      "Iter 506, Loss: 0.07108, Loss_pinn: 0.05453\n",
      "Iter 508, Loss: 0.06953, Loss_pinn: 0.04938\n",
      "Iter 510, Loss: 0.06549, Loss_pinn: 0.04741\n",
      "Iter 512, Loss: 0.06382, Loss_pinn: 0.04751\n",
      "Iter 514, Loss: 0.06529, Loss_pinn: 0.04591\n",
      "Iter 516, Loss: 0.05935, Loss_pinn: 0.04551\n",
      "Iter 518, Loss: 0.05855, Loss_pinn: 0.04380\n",
      "Iter 520, Loss: 0.05811, Loss_pinn: 0.04237\n",
      "Iter 522, Loss: 0.05797, Loss_pinn: 0.04289\n",
      "Iter 524, Loss: 0.05794, Loss_pinn: 0.04275\n",
      "Iter 526, Loss: 0.05789, Loss_pinn: 0.04281\n",
      "Iter 528, Loss: 0.05764, Loss_pinn: 0.04293\n",
      "Iter 530, Loss: 0.05694, Loss_pinn: 0.04250\n",
      "Iter 532, Loss: 0.05664, Loss_pinn: 0.04209\n",
      "Iter 534, Loss: 0.05615, Loss_pinn: 0.04203\n",
      "Iter 536, Loss: 0.05597, Loss_pinn: 0.04040\n",
      "Iter 538, Loss: 0.05592, Loss_pinn: 0.04134\n",
      "Iter 540, Loss: 0.05571, Loss_pinn: 0.04079\n",
      "Iter 542, Loss: 0.05533, Loss_pinn: 0.04064\n",
      "Iter 544, Loss: 0.05518, Loss_pinn: 0.04032\n",
      "Iter 546, Loss: 0.05489, Loss_pinn: 0.03889\n",
      "Iter 548, Loss: 0.05355, Loss_pinn: 0.03623\n",
      "Iter 550, Loss: 0.05326, Loss_pinn: 0.03816\n",
      "Iter 552, Loss: 0.05321, Loss_pinn: 0.03902\n",
      "Iter 554, Loss: 0.05309, Loss_pinn: 0.03856\n",
      "Iter 556, Loss: 0.05295, Loss_pinn: 0.03839\n",
      "Iter 558, Loss: 0.05272, Loss_pinn: 0.03803\n",
      "Iter 560, Loss: 0.05245, Loss_pinn: 0.03873\n",
      "Iter 562, Loss: 0.05238, Loss_pinn: 0.03810\n",
      "Iter 564, Loss: 0.05210, Loss_pinn: 0.03903\n",
      "Iter 566, Loss: 0.05190, Loss_pinn: 0.03897\n",
      "Iter 568, Loss: 0.05165, Loss_pinn: 0.03849\n",
      "Iter 570, Loss: 0.05131, Loss_pinn: 0.03804\n",
      "Iter 572, Loss: 0.05091, Loss_pinn: 0.03659\n",
      "Iter 574, Loss: 0.05048, Loss_pinn: 0.03571\n",
      "Iter 576, Loss: 0.05015, Loss_pinn: 0.03731\n",
      "Iter 578, Loss: 0.05004, Loss_pinn: 0.03594\n",
      "Iter 580, Loss: 0.04995, Loss_pinn: 0.03618\n",
      "Iter 582, Loss: 0.04978, Loss_pinn: 0.03568\n",
      "Iter 584, Loss: 0.04972, Loss_pinn: 0.03585\n",
      "Iter 586, Loss: 0.04971, Loss_pinn: 0.03607\n",
      "Iter 588, Loss: 0.04971, Loss_pinn: 0.03592\n",
      "Iter 590, Loss: 0.04970, Loss_pinn: 0.03600\n",
      "Iter 592, Loss: 0.04968, Loss_pinn: 0.03624\n",
      "Iter 594, Loss: 0.04963, Loss_pinn: 0.03657\n",
      "Iter 596, Loss: 0.04936, Loss_pinn: 0.03747\n",
      "Iter 598, Loss: 0.07826, Loss_pinn: 0.06702\n",
      "Iter 600, Loss: 0.04897, Loss_pinn: 0.03770\n",
      "Iter 602, Loss: 0.25965, Loss_pinn: 0.20829\n",
      "Iter 604, Loss: 0.04720, Loss_pinn: 0.03445\n",
      "Iter 606, Loss: 0.04671, Loss_pinn: 0.03253\n",
      "Iter 608, Loss: 0.04672, Loss_pinn: 0.02507\n",
      "Iter 610, Loss: 0.04840, Loss_pinn: 0.03802\n",
      "Iter 612, Loss: 0.04539, Loss_pinn: 0.03316\n",
      "Iter 614, Loss: 0.04400, Loss_pinn: 0.03271\n",
      "Iter 616, Loss: 0.04381, Loss_pinn: 0.02898\n",
      "Iter 618, Loss: 0.04248, Loss_pinn: 0.02962\n",
      "Iter 620, Loss: 0.04180, Loss_pinn: 0.02619\n",
      "Iter 622, Loss: 0.04821, Loss_pinn: 0.04040\n",
      "Iter 624, Loss: 0.04123, Loss_pinn: 0.02745\n",
      "Iter 626, Loss: 0.03818, Loss_pinn: 0.02515\n",
      "Iter 628, Loss: 0.03808, Loss_pinn: 0.03252\n",
      "Iter 630, Loss: 0.03606, Loss_pinn: 0.02571\n",
      "Iter 632, Loss: 0.03457, Loss_pinn: 0.02498\n",
      "Iter 634, Loss: 0.03428, Loss_pinn: 0.02517\n",
      "Iter 636, Loss: 0.03345, Loss_pinn: 0.02386\n",
      "Iter 638, Loss: 0.03265, Loss_pinn: 0.02425\n",
      "Iter 640, Loss: 0.03198, Loss_pinn: 0.02232\n",
      "Iter 642, Loss: 0.03149, Loss_pinn: 0.02335\n",
      "Iter 644, Loss: 0.03080, Loss_pinn: 0.02210\n",
      "Iter 646, Loss: 0.03013, Loss_pinn: 0.02174\n",
      "Iter 648, Loss: 0.02925, Loss_pinn: 0.02269\n",
      "Iter 650, Loss: 0.02875, Loss_pinn: 0.02210\n",
      "Iter 652, Loss: 0.02845, Loss_pinn: 0.02152\n",
      "Iter 654, Loss: 0.02783, Loss_pinn: 0.02196\n",
      "Iter 656, Loss: 0.02742, Loss_pinn: 0.02202\n",
      "Iter 658, Loss: 0.02714, Loss_pinn: 0.02156\n",
      "Iter 660, Loss: 0.02632, Loss_pinn: 0.02159\n",
      "Iter 662, Loss: 0.02599, Loss_pinn: 0.02210\n",
      "Iter 664, Loss: 0.02555, Loss_pinn: 0.02080\n",
      "Iter 666, Loss: 0.02535, Loss_pinn: 0.02100\n",
      "Iter 668, Loss: 0.02523, Loss_pinn: 0.02044\n",
      "Iter 670, Loss: 0.02507, Loss_pinn: 0.02015\n",
      "Iter 672, Loss: 0.02486, Loss_pinn: 0.01993\n",
      "Iter 674, Loss: 0.02433, Loss_pinn: 0.01914\n",
      "Iter 676, Loss: 0.02269, Loss_pinn: 0.01778\n",
      "Iter 678, Loss: 0.02237, Loss_pinn: 0.01780\n",
      "Iter 680, Loss: 0.02162, Loss_pinn: 0.01749\n",
      "Iter 682, Loss: 0.02135, Loss_pinn: 0.01576\n",
      "Iter 684, Loss: 0.02122, Loss_pinn: 0.01653\n",
      "Iter 686, Loss: 0.02108, Loss_pinn: 0.01666\n",
      "Iter 688, Loss: 0.02099, Loss_pinn: 0.01626\n",
      "Iter 690, Loss: 0.02084, Loss_pinn: 0.01590\n",
      "Iter 692, Loss: 0.02074, Loss_pinn: 0.01610\n",
      "Iter 694, Loss: 0.02071, Loss_pinn: 0.01621\n",
      "Iter 696, Loss: 0.02069, Loss_pinn: 0.01623\n",
      "Iter 698, Loss: 0.02083, Loss_pinn: 0.01658\n",
      "Iter 700, Loss: 0.02064, Loss_pinn: 0.01624\n",
      "Iter 702, Loss: 0.02045, Loss_pinn: 0.01607\n",
      "Iter 704, Loss: 0.03775, Loss_pinn: 0.03059\n",
      "Iter 706, Loss: 0.02005, Loss_pinn: 0.01609\n",
      "Iter 708, Loss: 0.01915, Loss_pinn: 0.01503\n",
      "Iter 710, Loss: 0.14915, Loss_pinn: 0.13319\n",
      "Iter 712, Loss: 0.01754, Loss_pinn: 0.01298\n",
      "Iter 714, Loss: 0.02220, Loss_pinn: 0.01968\n",
      "Iter 716, Loss: 0.01661, Loss_pinn: 0.01260\n",
      "Iter 718, Loss: 0.01621, Loss_pinn: 0.01243\n",
      "Iter 720, Loss: 0.01599, Loss_pinn: 0.01214\n",
      "Iter 722, Loss: 0.01587, Loss_pinn: 0.01191\n",
      "Iter 724, Loss: 0.01574, Loss_pinn: 0.01180\n",
      "Iter 726, Loss: 0.01559, Loss_pinn: 0.01170\n",
      "Iter 728, Loss: 0.01535, Loss_pinn: 0.01172\n",
      "Iter 730, Loss: 0.01493, Loss_pinn: 0.01116\n",
      "Iter 732, Loss: 0.01456, Loss_pinn: 0.01081\n",
      "Iter 734, Loss: 0.01444, Loss_pinn: 0.01070\n",
      "Iter 736, Loss: 0.01432, Loss_pinn: 0.01089\n",
      "Iter 738, Loss: 0.01410, Loss_pinn: 0.01059\n",
      "Iter 740, Loss: 0.01588, Loss_pinn: 0.01321\n",
      "Iter 742, Loss: 0.01379, Loss_pinn: 0.01050\n",
      "Iter 744, Loss: 0.01374, Loss_pinn: 0.01042\n",
      "Iter 746, Loss: 0.01359, Loss_pinn: 0.01015\n",
      "Iter 748, Loss: 0.01349, Loss_pinn: 0.01012\n",
      "Iter 750, Loss: 0.01347, Loss_pinn: 0.01013\n",
      "Iter 752, Loss: 0.01342, Loss_pinn: 0.01018\n",
      "Iter 754, Loss: 0.01340, Loss_pinn: 0.01031\n",
      "Iter 756, Loss: 0.01339, Loss_pinn: 0.01033\n",
      "Iter 758, Loss: 0.01339, Loss_pinn: 0.01028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 760, Loss: 0.01336, Loss_pinn: 0.01013\n",
      "Iter 762, Loss: 0.01338, Loss_pinn: 0.01002\n",
      "Iter 764, Loss: 0.01328, Loss_pinn: 0.01015\n",
      "Iter 766, Loss: 0.01316, Loss_pinn: 0.01037\n",
      "Iter 768, Loss: 0.01313, Loss_pinn: 0.01038\n",
      "Iter 770, Loss: 0.01293, Loss_pinn: 0.01036\n",
      "Iter 772, Loss: 0.01305, Loss_pinn: 0.01057\n",
      "Iter 774, Loss: 0.01258, Loss_pinn: 0.01005\n",
      "Iter 776, Loss: 0.01240, Loss_pinn: 0.00981\n",
      "Iter 778, Loss: 0.01296, Loss_pinn: 0.01078\n",
      "Iter 780, Loss: 0.01234, Loss_pinn: 0.01071\n",
      "Iter 782, Loss: 0.01203, Loss_pinn: 0.00995\n",
      "Iter 784, Loss: 0.01187, Loss_pinn: 0.00977\n",
      "Iter 786, Loss: 0.01142, Loss_pinn: 0.00951\n",
      "Iter 788, Loss: 0.01120, Loss_pinn: 0.01037\n",
      "Iter 790, Loss: 0.01089, Loss_pinn: 0.00939\n",
      "Iter 792, Loss: 0.01050, Loss_pinn: 0.00895\n",
      "Iter 794, Loss: 0.01035, Loss_pinn: 0.00892\n",
      "Iter 796, Loss: 0.01008, Loss_pinn: 0.00948\n",
      "Iter 798, Loss: 0.00965, Loss_pinn: 0.00831\n",
      "Iter 800, Loss: 0.00942, Loss_pinn: 0.00836\n",
      "Iter 802, Loss: 0.00885, Loss_pinn: 0.00791\n",
      "Iter 804, Loss: 0.00871, Loss_pinn: 0.00753\n",
      "Iter 806, Loss: 0.00846, Loss_pinn: 0.00761\n",
      "Iter 808, Loss: 0.00816, Loss_pinn: 0.00735\n",
      "Iter 810, Loss: 0.00796, Loss_pinn: 0.00730\n",
      "Iter 812, Loss: 0.00787, Loss_pinn: 0.00711\n",
      "Iter 814, Loss: 0.00781, Loss_pinn: 0.00712\n",
      "Iter 816, Loss: 0.00776, Loss_pinn: 0.00695\n",
      "Iter 818, Loss: 0.00768, Loss_pinn: 0.00689\n",
      "Iter 820, Loss: 0.00755, Loss_pinn: 0.00695\n",
      "Iter 822, Loss: 0.00725, Loss_pinn: 0.00656\n",
      "Iter 824, Loss: 0.00711, Loss_pinn: 0.00657\n",
      "Iter 826, Loss: 0.00743, Loss_pinn: 0.00665\n",
      "Iter 828, Loss: 0.00679, Loss_pinn: 0.00603\n",
      "Iter 830, Loss: 0.00663, Loss_pinn: 0.00587\n",
      "Iter 832, Loss: 0.00631, Loss_pinn: 0.00568\n",
      "Iter 834, Loss: 0.00610, Loss_pinn: 0.00555\n",
      "Iter 836, Loss: 0.00600, Loss_pinn: 0.00540\n",
      "Iter 838, Loss: 0.00533, Loss_pinn: 0.00476\n",
      "Iter 840, Loss: 0.00514, Loss_pinn: 0.00465\n",
      "Iter 842, Loss: 0.00499, Loss_pinn: 0.00457\n",
      "Iter 844, Loss: 0.00489, Loss_pinn: 0.00449\n",
      "Iter 846, Loss: 0.00487, Loss_pinn: 0.00451\n",
      "Iter 848, Loss: 0.00486, Loss_pinn: 0.00449\n",
      "Iter 850, Loss: 0.00485, Loss_pinn: 0.00449\n",
      "Iter 852, Loss: 0.00483, Loss_pinn: 0.00448\n",
      "Iter 854, Loss: 0.00474, Loss_pinn: 0.00435\n",
      "Iter 856, Loss: 0.00472, Loss_pinn: 0.00432\n",
      "Iter 858, Loss: 0.00466, Loss_pinn: 0.00438\n",
      "Iter 860, Loss: 0.00460, Loss_pinn: 0.00434\n",
      "Iter 862, Loss: 0.00452, Loss_pinn: 0.00436\n",
      "Iter 864, Loss: 0.00448, Loss_pinn: 0.00433\n",
      "Iter 866, Loss: 0.00446, Loss_pinn: 0.00435\n",
      "Iter 868, Loss: 0.00453, Loss_pinn: 0.00446\n",
      "Iter 870, Loss: 0.00443, Loss_pinn: 0.00435\n",
      "Iter 872, Loss: 0.00506, Loss_pinn: 0.00459\n",
      "Iter 874, Loss: 0.00441, Loss_pinn: 0.00435\n",
      "Iter 876, Loss: 0.00437, Loss_pinn: 0.00432\n",
      "Iter 878, Loss: 0.00431, Loss_pinn: 0.00431\n",
      "Iter 880, Loss: 0.00421, Loss_pinn: 0.00420\n",
      "Iter 882, Loss: 0.00417, Loss_pinn: 0.00416\n",
      "Iter 884, Loss: 0.00405, Loss_pinn: 0.00405\n",
      "Iter 886, Loss: 0.00403, Loss_pinn: 0.00403\n",
      "Iter 888, Loss: 0.00572, Loss_pinn: 0.00568\n",
      "Iter 890, Loss: 0.00415, Loss_pinn: 0.00414\n",
      "Iter 892, Loss: 0.00385, Loss_pinn: 0.00385\n",
      "Iter 894, Loss: 0.00377, Loss_pinn: 0.00377\n",
      "Iter 896, Loss: 0.00367, Loss_pinn: 0.00367\n",
      "Iter 898, Loss: 0.00356, Loss_pinn: 0.00355\n",
      "Iter 900, Loss: 0.00349, Loss_pinn: 0.00349\n",
      "Iter 902, Loss: 0.00365, Loss_pinn: 0.00364\n",
      "Iter 904, Loss: 0.00329, Loss_pinn: 0.00328\n",
      "Iter 906, Loss: 0.00383, Loss_pinn: 0.00382\n",
      "Iter 908, Loss: 0.00300, Loss_pinn: 0.00298\n",
      "Iter 910, Loss: 0.00283, Loss_pinn: 0.00283\n",
      "Iter 912, Loss: 0.00280, Loss_pinn: 0.00280\n",
      "Iter 914, Loss: 0.00275, Loss_pinn: 0.00274\n",
      "Iter 916, Loss: 0.00271, Loss_pinn: 0.00271\n",
      "Iter 918, Loss: 0.00270, Loss_pinn: 0.00270\n",
      "Iter 920, Loss: 0.00267, Loss_pinn: 0.00266\n",
      "Iter 922, Loss: 0.00263, Loss_pinn: 0.00261\n",
      "Iter 924, Loss: 0.00257, Loss_pinn: 0.00255\n",
      "Iter 926, Loss: 0.00280, Loss_pinn: 0.00279\n",
      "Iter 928, Loss: 0.00234, Loss_pinn: 0.00234\n",
      "Iter 930, Loss: 0.00210, Loss_pinn: 0.00209\n",
      "Iter 932, Loss: 0.00208, Loss_pinn: 0.00208\n",
      "Iter 934, Loss: 0.00200, Loss_pinn: 0.00196\n",
      "Iter 936, Loss: 0.00185, Loss_pinn: 0.00184\n",
      "Iter 938, Loss: 0.00175, Loss_pinn: 0.00173\n",
      "Iter 940, Loss: 0.00164, Loss_pinn: 0.00159\n",
      "Iter 942, Loss: 0.00155, Loss_pinn: 0.00152\n",
      "Iter 944, Loss: 0.00146, Loss_pinn: 0.00143\n",
      "Iter 946, Loss: 0.00143, Loss_pinn: 0.00132\n",
      "Iter 948, Loss: 0.00138, Loss_pinn: 0.00131\n",
      "Iter 950, Loss: 0.00133, Loss_pinn: 0.00129\n",
      "Iter 952, Loss: 0.00131, Loss_pinn: 0.00123\n",
      "Iter 954, Loss: 0.00130, Loss_pinn: 0.00124\n",
      "Iter 956, Loss: 0.00129, Loss_pinn: 0.00123\n",
      "Iter 958, Loss: 0.00129, Loss_pinn: 0.00123\n",
      "Iter 960, Loss: 0.00129, Loss_pinn: 0.00123\n",
      "Iter 962, Loss: 0.00129, Loss_pinn: 0.00123\n",
      "Iter 964, Loss: 0.00128, Loss_pinn: 0.00122\n",
      "Iter 966, Loss: 0.00128, Loss_pinn: 0.00121\n",
      "Iter 968, Loss: 0.00126, Loss_pinn: 0.00119\n",
      "Iter 970, Loss: 0.00126, Loss_pinn: 0.00119\n",
      "Iter 972, Loss: 0.00123, Loss_pinn: 0.00119\n",
      "Iter 974, Loss: 0.00123, Loss_pinn: 0.00118\n",
      "Iter 976, Loss: 0.00121, Loss_pinn: 0.00118\n",
      "Iter 978, Loss: 0.00119, Loss_pinn: 0.00116\n",
      "Iter 980, Loss: 0.00118, Loss_pinn: 0.00114\n",
      "Iter 982, Loss: 0.00115, Loss_pinn: 0.00110\n",
      "Iter 984, Loss: 0.00114, Loss_pinn: 0.00111\n",
      "Iter 986, Loss: 0.00112, Loss_pinn: 0.00109\n",
      "Iter 988, Loss: 0.00109, Loss_pinn: 0.00104\n",
      "Iter 990, Loss: 0.00107, Loss_pinn: 0.00100\n",
      "Iter 992, Loss: 0.00106, Loss_pinn: 0.00100\n",
      "Iter 994, Loss: 0.00103, Loss_pinn: 0.00096\n",
      "Iter 996, Loss: 0.00101, Loss_pinn: 0.00093\n",
      "Iter 998, Loss: 0.00100, Loss_pinn: 0.00089\n",
      "Iter 1000, Loss: 0.00096, Loss_pinn: 0.00089\n",
      "Iter 1002, Loss: 0.00089, Loss_pinn: 0.00083\n",
      "Iter 1004, Loss: 0.00093, Loss_pinn: 0.00093\n",
      "Iter 1006, Loss: 0.00082, Loss_pinn: 0.00077\n",
      "Iter 1008, Loss: 0.00078, Loss_pinn: 0.00075\n",
      "Iter 1010, Loss: 0.00074, Loss_pinn: 0.00070\n",
      "Iter 1012, Loss: 0.00074, Loss_pinn: 0.00071\n",
      "Iter 1014, Loss: 0.00070, Loss_pinn: 0.00068\n",
      "Iter 1016, Loss: 0.00067, Loss_pinn: 0.00065\n",
      "Iter 1018, Loss: 0.00065, Loss_pinn: 0.00063\n",
      "Iter 1020, Loss: 0.00063, Loss_pinn: 0.00061\n",
      "Iter 1022, Loss: 0.00063, Loss_pinn: 0.00060\n",
      "Iter 1024, Loss: 0.00061, Loss_pinn: 0.00056\n",
      "Iter 1026, Loss: 0.00058, Loss_pinn: 0.00055\n",
      "Iter 1028, Loss: 0.00054, Loss_pinn: 0.00054\n",
      "Iter 1030, Loss: 0.00052, Loss_pinn: 0.00051\n",
      "Iter 1032, Loss: 0.00050, Loss_pinn: 0.00049\n",
      "Iter 1034, Loss: 0.00049, Loss_pinn: 0.00048\n",
      "Iter 1036, Loss: 0.00049, Loss_pinn: 0.00048\n",
      "Iter 1038, Loss: 0.00049, Loss_pinn: 0.00048\n",
      "Iter 1040, Loss: 0.00048, Loss_pinn: 0.00048\n",
      "Iter 1042, Loss: 0.00048, Loss_pinn: 0.00048\n",
      "Iter 1044, Loss: 0.00048, Loss_pinn: 0.00048\n",
      "Iter 1046, Loss: 0.00048, Loss_pinn: 0.00047\n",
      "Iter 1048, Loss: 0.00048, Loss_pinn: 0.00047\n",
      "Iter 1050, Loss: 0.00048, Loss_pinn: 0.00047\n",
      "Iter 1052, Loss: 0.00048, Loss_pinn: 0.00047\n",
      "Iter 1054, Loss: 0.00048, Loss_pinn: 0.00047\n",
      "Iter 1056, Loss: 0.00047, Loss_pinn: 0.00046\n",
      "Iter 1058, Loss: 0.00047, Loss_pinn: 0.00046\n",
      "Iter 1060, Loss: 0.00047, Loss_pinn: 0.00046\n",
      "Iter 1062, Loss: 0.00047, Loss_pinn: 0.00046\n",
      "Iter 1064, Loss: 0.00047, Loss_pinn: 0.00046\n",
      "Iter 1066, Loss: 0.00047, Loss_pinn: 0.00045\n",
      "Iter 1068, Loss: 0.00047, Loss_pinn: 0.00045\n",
      "Iter 1070, Loss: 0.00047, Loss_pinn: 0.00045\n",
      "Iter 1072, Loss: 0.00047, Loss_pinn: 0.00045\n",
      "Iter 1074, Loss: 0.00047, Loss_pinn: 0.00045\n",
      "Iter 1076, Loss: 0.00047, Loss_pinn: 0.00045\n",
      "Iter 1078, Loss: 0.00047, Loss_pinn: 0.00045\n",
      "Iter 1080, Loss: 0.00047, Loss_pinn: 0.00045\n",
      "Iter 1082, Loss: 0.00047, Loss_pinn: 0.00045\n",
      "Iter 1084, Loss: 0.00046, Loss_pinn: 0.00045\n",
      "Iter 1086, Loss: 0.00046, Loss_pinn: 0.00045\n",
      "Iter 1088, Loss: 0.00046, Loss_pinn: 0.00045\n",
      "Iter 1090, Loss: 0.00046, Loss_pinn: 0.00045\n",
      "Iter 1092, Loss: 0.00046, Loss_pinn: 0.00045\n",
      "Iter 1094, Loss: 0.00046, Loss_pinn: 0.00045\n",
      "Iter 1096, Loss: 0.00046, Loss_pinn: 0.00045\n",
      "Iter 1098, Loss: 0.00046, Loss_pinn: 0.00045\n",
      "Iter 1100, Loss: 0.00046, Loss_pinn: 0.00045\n",
      "Iter 1102, Loss: 0.00046, Loss_pinn: 0.00045\n",
      "Iter 1104, Loss: 0.00046, Loss_pinn: 0.00045\n",
      "Iter 1106, Loss: 0.00046, Loss_pinn: 0.00045\n",
      "Iter 1108, Loss: 0.00046, Loss_pinn: 0.00045\n",
      "Iter 1110, Loss: 0.00045, Loss_pinn: 0.00044\n",
      "Iter 1112, Loss: 0.00045, Loss_pinn: 0.00044\n",
      "Iter 1114, Loss: 0.00045, Loss_pinn: 0.00043\n",
      "Iter 1116, Loss: 0.00044, Loss_pinn: 0.00044\n",
      "Iter 1118, Loss: 0.00044, Loss_pinn: 0.00043\n",
      "Iter 1120, Loss: 0.00044, Loss_pinn: 0.00043\n",
      "Iter 1122, Loss: 0.00043, Loss_pinn: 0.00042\n",
      "Iter 1124, Loss: 0.00043, Loss_pinn: 0.00041\n",
      "Iter 1126, Loss: 0.00042, Loss_pinn: 0.00041\n",
      "Iter 1128, Loss: 0.00042, Loss_pinn: 0.00041\n",
      "Iter 1130, Loss: 0.00042, Loss_pinn: 0.00041\n",
      "Iter 1132, Loss: 0.00042, Loss_pinn: 0.00041\n",
      "Iter 1134, Loss: 0.00042, Loss_pinn: 0.00041\n",
      "Iter 1136, Loss: 0.00042, Loss_pinn: 0.00041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1138, Loss: 0.00041, Loss_pinn: 0.00041\n",
      "Iter 1140, Loss: 0.00041, Loss_pinn: 0.00041\n",
      "Iter 1142, Loss: 0.00041, Loss_pinn: 0.00041\n",
      "Iter 1144, Loss: 0.00041, Loss_pinn: 0.00040\n",
      "Iter 1146, Loss: 0.00052, Loss_pinn: 0.00051\n",
      "Iter 1148, Loss: 0.00059, Loss_pinn: 0.00057\n",
      "Iter 1150, Loss: 0.00037, Loss_pinn: 0.00037\n",
      "Iter 1152, Loss: 0.00052, Loss_pinn: 0.00051\n",
      "Iter 1154, Loss: 0.00035, Loss_pinn: 0.00035\n",
      "Iter 1156, Loss: 0.00032, Loss_pinn: 0.00032\n",
      "Iter 1158, Loss: 0.00031, Loss_pinn: 0.00031\n",
      "Iter 1160, Loss: 0.00045, Loss_pinn: 0.00045\n",
      "Iter 1162, Loss: 0.00029, Loss_pinn: 0.00029\n",
      "Iter 1164, Loss: 0.00037, Loss_pinn: 0.00037\n",
      "Iter 1166, Loss: 0.00028, Loss_pinn: 0.00028\n",
      "Iter 1168, Loss: 0.00027, Loss_pinn: 0.00027\n",
      "Iter 1170, Loss: 0.00026, Loss_pinn: 0.00026\n",
      "Iter 1172, Loss: 0.00025, Loss_pinn: 0.00025\n",
      "Iter 1174, Loss: 0.00025, Loss_pinn: 0.00025\n",
      "Iter 1176, Loss: 0.00025, Loss_pinn: 0.00025\n",
      "Iter 1178, Loss: 0.00024, Loss_pinn: 0.00024\n",
      "Iter 1180, Loss: 0.00025, Loss_pinn: 0.00025\n",
      "Iter 1182, Loss: 0.00023, Loss_pinn: 0.00023\n",
      "Iter 1184, Loss: 0.00023, Loss_pinn: 0.00023\n",
      "Iter 1186, Loss: 0.00023, Loss_pinn: 0.00023\n",
      "Iter 1188, Loss: 0.00023, Loss_pinn: 0.00022\n",
      "Iter 1190, Loss: 0.00022, Loss_pinn: 0.00022\n",
      "Iter 1192, Loss: 0.00022, Loss_pinn: 0.00022\n",
      "Iter 1194, Loss: 0.00022, Loss_pinn: 0.00022\n",
      "Iter 1196, Loss: 0.00021, Loss_pinn: 0.00021\n",
      "Iter 1198, Loss: 0.00021, Loss_pinn: 0.00021\n",
      "Iter 1200, Loss: 0.00021, Loss_pinn: 0.00021\n",
      "Iter 1202, Loss: 0.00021, Loss_pinn: 0.00021\n",
      "Iter 1204, Loss: 0.00021, Loss_pinn: 0.00021\n",
      "Iter 1206, Loss: 0.00021, Loss_pinn: 0.00021\n",
      "Iter 1208, Loss: 0.00021, Loss_pinn: 0.00021\n",
      "Iter 1210, Loss: 0.00021, Loss_pinn: 0.00021\n",
      "Iter 1212, Loss: 0.00020, Loss_pinn: 0.00020\n",
      "Iter 1214, Loss: 0.00020, Loss_pinn: 0.00020\n",
      "Iter 1216, Loss: 0.00020, Loss_pinn: 0.00020\n",
      "Iter 1218, Loss: 0.00020, Loss_pinn: 0.00020\n",
      "Iter 1220, Loss: 0.00020, Loss_pinn: 0.00020\n",
      "Iter 1222, Loss: 0.00023, Loss_pinn: 0.00023\n",
      "Iter 1224, Loss: 0.00021, Loss_pinn: 0.00021\n",
      "Iter 1226, Loss: 0.00019, Loss_pinn: 0.00019\n",
      "Iter 1228, Loss: 0.00019, Loss_pinn: 0.00019\n",
      "Iter 1230, Loss: 0.00018, Loss_pinn: 0.00018\n",
      "Iter 1232, Loss: 0.00018, Loss_pinn: 0.00018\n",
      "Iter 1234, Loss: 0.00017, Loss_pinn: 0.00017\n",
      "Iter 1236, Loss: 0.00017, Loss_pinn: 0.00017\n",
      "Iter 1238, Loss: 0.00016, Loss_pinn: 0.00016\n",
      "Iter 1240, Loss: 0.00016, Loss_pinn: 0.00016\n",
      "Iter 1242, Loss: 0.00015, Loss_pinn: 0.00015\n",
      "Iter 1244, Loss: 0.00014, Loss_pinn: 0.00014\n",
      "Iter 1246, Loss: 0.00014, Loss_pinn: 0.00014\n",
      "Iter 1248, Loss: 0.00013, Loss_pinn: 0.00013\n",
      "Iter 1250, Loss: 0.00014, Loss_pinn: 0.00014\n",
      "Iter 1252, Loss: 0.00013, Loss_pinn: 0.00013\n",
      "Iter 1254, Loss: 0.00013, Loss_pinn: 0.00013\n",
      "Iter 1256, Loss: 0.00013, Loss_pinn: 0.00013\n",
      "Iter 1258, Loss: 0.00013, Loss_pinn: 0.00013\n",
      "Iter 1260, Loss: 0.00013, Loss_pinn: 0.00013\n",
      "Iter 1262, Loss: 0.00013, Loss_pinn: 0.00013\n",
      "Iter 1264, Loss: 0.00013, Loss_pinn: 0.00013\n",
      "Iter 1266, Loss: 0.00013, Loss_pinn: 0.00013\n",
      "Iter 1268, Loss: 0.00013, Loss_pinn: 0.00013\n",
      "Iter 1270, Loss: 0.00012, Loss_pinn: 0.00012\n",
      "Iter 1272, Loss: 0.00012, Loss_pinn: 0.00012\n",
      "Iter 1274, Loss: 0.00011, Loss_pinn: 0.00011\n",
      "Iter 1276, Loss: 0.00012, Loss_pinn: 0.00012\n",
      "Iter 1278, Loss: 0.00010, Loss_pinn: 0.00010\n",
      "Iter 1280, Loss: 0.00010, Loss_pinn: 0.00010\n",
      "Iter 1282, Loss: 0.00010, Loss_pinn: 0.00010\n",
      "Iter 1284, Loss: 0.00010, Loss_pinn: 0.00010\n",
      "Iter 1286, Loss: 0.00009, Loss_pinn: 0.00009\n",
      "Iter 1288, Loss: 0.00009, Loss_pinn: 0.00009\n",
      "Iter 1290, Loss: 0.00010, Loss_pinn: 0.00010\n",
      "Iter 1292, Loss: 0.00009, Loss_pinn: 0.00008\n",
      "Iter 1294, Loss: 0.00008, Loss_pinn: 0.00008\n",
      "Iter 1296, Loss: 0.00008, Loss_pinn: 0.00008\n",
      "Iter 1298, Loss: 0.00008, Loss_pinn: 0.00008\n",
      "Iter 1300, Loss: 0.00008, Loss_pinn: 0.00008\n",
      "Iter 1302, Loss: 0.00008, Loss_pinn: 0.00008\n",
      "Iter 1304, Loss: 0.00008, Loss_pinn: 0.00008\n",
      "Iter 1306, Loss: 0.00008, Loss_pinn: 0.00008\n",
      "Iter 1308, Loss: 0.00008, Loss_pinn: 0.00008\n",
      "Iter 1310, Loss: 0.00007, Loss_pinn: 0.00007\n",
      "Iter 1312, Loss: 0.00008, Loss_pinn: 0.00007\n",
      "Iter 1314, Loss: 0.00007, Loss_pinn: 0.00007\n",
      "Iter 1316, Loss: 0.00007, Loss_pinn: 0.00007\n",
      "Iter 1318, Loss: 0.00007, Loss_pinn: 0.00007\n",
      "Iter 1320, Loss: 0.00007, Loss_pinn: 0.00006\n",
      "Iter 1322, Loss: 0.00006, Loss_pinn: 0.00006\n",
      "Iter 1324, Loss: 0.00006, Loss_pinn: 0.00006\n",
      "Iter 1326, Loss: 0.00006, Loss_pinn: 0.00006\n",
      "Iter 1328, Loss: 0.00006, Loss_pinn: 0.00006\n",
      "Iter 1330, Loss: 0.00006, Loss_pinn: 0.00006\n",
      "Iter 1332, Loss: 0.00006, Loss_pinn: 0.00005\n",
      "Iter 1334, Loss: 0.00005, Loss_pinn: 0.00005\n",
      "Iter 1336, Loss: 0.00006, Loss_pinn: 0.00006\n",
      "Iter 1338, Loss: 0.00005, Loss_pinn: 0.00005\n",
      "Iter 1340, Loss: 0.00005, Loss_pinn: 0.00004\n",
      "Iter 1342, Loss: 0.00005, Loss_pinn: 0.00004\n",
      "Iter 1344, Loss: 0.00004, Loss_pinn: 0.00004\n",
      "Iter 1346, Loss: 0.00004, Loss_pinn: 0.00004\n",
      "Iter 1348, Loss: 0.00004, Loss_pinn: 0.00004\n",
      "Iter 1350, Loss: 0.00004, Loss_pinn: 0.00004\n",
      "Iter 1352, Loss: 0.00004, Loss_pinn: 0.00004\n",
      "Iter 1354, Loss: 0.00004, Loss_pinn: 0.00004\n",
      "Iter 1356, Loss: 0.00004, Loss_pinn: 0.00004\n",
      "Iter 1358, Loss: 0.00004, Loss_pinn: 0.00004\n",
      "Iter 1360, Loss: 0.00004, Loss_pinn: 0.00004\n",
      "Iter 1362, Loss: 0.00004, Loss_pinn: 0.00004\n",
      "Iter 1364, Loss: 0.00004, Loss_pinn: 0.00004\n",
      "Iter 1366, Loss: 0.00004, Loss_pinn: 0.00004\n",
      "Iter 1368, Loss: 0.00004, Loss_pinn: 0.00004\n",
      "Iter 1370, Loss: 0.00004, Loss_pinn: 0.00003\n",
      "Iter 1372, Loss: 0.00004, Loss_pinn: 0.00003\n",
      "Iter 1374, Loss: 0.00004, Loss_pinn: 0.00003\n",
      "Iter 1376, Loss: 0.00003, Loss_pinn: 0.00003\n",
      "Iter 1378, Loss: 0.00003, Loss_pinn: 0.00003\n",
      "Iter 1380, Loss: 0.00003, Loss_pinn: 0.00003\n",
      "Iter 1382, Loss: 0.00003, Loss_pinn: 0.00003\n",
      "Iter 1384, Loss: 0.00003, Loss_pinn: 0.00003\n",
      "Iter 1386, Loss: 0.00003, Loss_pinn: 0.00003\n",
      "Iter 1388, Loss: 0.00003, Loss_pinn: 0.00003\n",
      "Iter 1390, Loss: 0.00003, Loss_pinn: 0.00003\n",
      "Iter 1392, Loss: 0.00003, Loss_pinn: 0.00003\n",
      "Iter 1394, Loss: 0.00003, Loss_pinn: 0.00003\n",
      "Iter 1396, Loss: 0.00003, Loss_pinn: 0.00003\n",
      "Iter 1398, Loss: 0.00003, Loss_pinn: 0.00003\n",
      "Iter 1400, Loss: 0.00003, Loss_pinn: 0.00003\n",
      "Iter 1402, Loss: 0.00003, Loss_pinn: 0.00003\n",
      "Iter 1404, Loss: 0.00003, Loss_pinn: 0.00003\n",
      "Iter 1406, Loss: 0.00003, Loss_pinn: 0.00003\n",
      "Iter 1408, Loss: 0.00003, Loss_pinn: 0.00003\n",
      "Iter 1410, Loss: 0.00003, Loss_pinn: 0.00003\n",
      "CPU times: user 2min 50s, sys: 581 ms, total: 2min 51s\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "               \n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd52a59",
   "metadata": {},
   "source": [
    "## Predict and Plot the Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ff2236a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_figure(figsize, xlim, ylim):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.spines[['top', 'right']].set_visible(False)\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_ode_solution(ax, times, sol, *args, **kwargs):\n",
    "    \n",
    "    ax.plot(times, sol, '.-', *args, **kwargs)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "2bf316f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = np.linspace(0, 3, 200)\n",
    "test_input = times[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f2f012bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = model.predict(test_input).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "21cf58fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = generate_figure(figsize=(8,8), xlim=[-0.1, 3], ylim=[0, 25])\n",
    "\n",
    "ax = plot_ode_solution(ax, times, test_output, color=\"#03468F\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5454f822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20.0833]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.net_y(torch.tensor([[3.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a86a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
