{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c314520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['pgf.texsystem'] = 'pdflatex'\n",
    "matplotlib.rcParams.update({'font.family': 'serif', 'font.size': 10})\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "from scipy import integrate\n",
    "import time\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6d595",
   "metadata": {},
   "source": [
    "In this example, we solve a Lotka-Volterra Equation of the general form\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{dr}{dt} = \\alpha r - \\beta rp \\\\\n",
    "\\frac{dp}{dt} = \\gamma rp - \\delta p\n",
    "\\end{align*}\n",
    "\n",
    "where $r$ is the number of prey, $p$ is the number of some predator, and $\\alpha$, $\\beta$, $\\gamma$, and $\\delta$ are real parameters describing the interactions of the two species. For the sake of simplicity, we assume that these parameters are all equal to one, i.e. we seek to solve the ODE pair\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{dr}{dt} = r - rp \\\\\n",
    "\\frac{dp}{dt} = rp - p\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "135abf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA support \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92eed791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class DNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.Tanh\n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x = (t, y0)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300b459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PINN: physics-informed neural network\n",
    "class PINN():\n",
    "\n",
    "    def __init__(self, X_pinn, X_semigroup, X_smooth, layers, T):\n",
    "\n",
    "        # neural network architecture\n",
    "        self.layers = layers\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "        \n",
    "        # semigroup PINN step time\n",
    "        self.T = torch.tensor(T).float().to(device)\n",
    "\n",
    "        # training data\n",
    "        self.t_pinn = torch.tensor(X_pinn[:, :1], requires_grad=True).float().to(device)\n",
    "        self.y_pinn = torch.tensor(X_pinn[:, 1:], requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.s_semigroup = torch.tensor(X_semigroup[:, :1], requires_grad=True).float().to(device)\n",
    "        self.t_semigroup = torch.tensor(X_semigroup[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.y_semigroup = torch.tensor(X_semigroup[:, 2:], requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.t_smooth = torch.tensor(X_smooth[:, :1], requires_grad=True).float().to(device)\n",
    "        self.y_smooth = torch.tensor(X_smooth[:, 1:], requires_grad=True).float().to(device)\n",
    "        \n",
    "        # optimization\n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(), lr=1.0, max_iter=50000, max_eval=50000, \n",
    "            history_size=50, tolerance_grad=1e-5, tolerance_change=np.finfo(float).eps, \n",
    "            line_search_fn=\"strong_wolfe\"\n",
    "        )\n",
    "\n",
    "        self.iter = 0\n",
    "    \n",
    "    \n",
    "    def net_y(self, t, y0):\n",
    "        \n",
    "        # The M(t, y0) = y0 + t N(t, y0) scheme seems to drastically increase the accuracy\n",
    "        # This works perfectly fine with automatic differentiation\n",
    "        y = y0 + t * self.dnn(torch.cat([t, y0], dim=1))\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    \n",
    "    def net_derivative(self, t, y0):\n",
    "        \"\"\"\n",
    "        Pytorch automatic differentiation to compute the derivative of the neural network\n",
    "        \"\"\"\n",
    "        y = self.net_y(t, y0)\n",
    "        \n",
    "        # vectors for the autograd vector Jacobian product \n",
    "        # to compute the derivatives w.r.t. every output dimension\n",
    "        vectors = [torch.zeros_like(y) for i in range(2)]\n",
    "        \n",
    "        for i, vec in enumerate(vectors):\n",
    "            \n",
    "            vec[:,i] = 1.\n",
    "        \n",
    "        # list of derivative tensors\n",
    "        # the first entry is a tensor with \\partial_t PINN(t, y0) for all (t, y0) in the batch,\n",
    "        # each input (t, y0) corresponds to one row in each tensor\n",
    "        derivatives = [\n",
    "            torch.autograd.grad(\n",
    "                y, t, \n",
    "                grad_outputs=vec,\n",
    "                retain_graph=True,\n",
    "                create_graph=True\n",
    "            )[0]\n",
    "            for vec in vectors\n",
    "        ]\n",
    "        \n",
    "        return derivatives\n",
    "    \n",
    "    \n",
    "    def loss_function(self):\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = self.net_y(self.t_pinn, self.y_pinn)\n",
    "        deriv_pred = self.net_derivative(self.t_pinn, self.y_pinn)\n",
    "        \n",
    "        \"\"\" Changed this \"\"\"\n",
    "\n",
    "        # in our case, dy1/dt = y1 - y1 * y2, dy2/dt = y1 * y2 -y2\n",
    "        loss_pinn1 = torch.mean((deriv_pred[0] - y_pred[:,0:1] + y_pred[:,0:1] * y_pred[:,1:2]) ** 2)\n",
    "        loss_pinn2 = torch.mean((deriv_pred[1] - y_pred[:,0:1] * y_pred[:,1:2] + y_pred[:,1:2]) ** 2)\n",
    "        loss_pinn = loss_pinn1 + loss_pinn2 \n",
    "        \n",
    "        # The general semigroup loss for autonomous ODEs\n",
    "        y_pred_tps = self.net_y(self.s_semigroup + self.t_semigroup, self.y_semigroup)\n",
    "        y_pred_s = self.net_y(self.s_semigroup, self.y_semigroup)\n",
    "        y_pred_restart = self.net_y(self.t_semigroup, y_pred_s)\n",
    "        loss_semigroup = torch.mean((y_pred_tps - y_pred_restart) ** 2)\n",
    "        \n",
    "        # The smoothness loss\n",
    "        y_pred_smooth = self.net_y(self.t_smooth, self.y_smooth)\n",
    "        deriv_pred_below = self.net_derivative(self.t_smooth, self.y_smooth)\n",
    "        deriv_pred_above = self.net_derivative(torch.zeros_like(self.t_smooth, requires_grad=True), y_pred_smooth)\n",
    "        \n",
    "        loss_smooth = .0\n",
    "        \n",
    "        for t1, t2 in zip(deriv_pred_below, deriv_pred_above):\n",
    "            \n",
    "            loss_smooth += torch.mean((t1 - t2) ** 2)\n",
    "        \n",
    "        loss = loss_pinn + loss_smooth + loss_semigroup\n",
    "        \n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "        \n",
    "        if self.iter % 100 == 0:\n",
    "            print(\n",
    "                f\"Iter {self.iter}, Loss: {loss.item():.5f}, Loss_pinn: {loss_pinn.item():.5f} \" \\\n",
    "                f\"Loss_smooth: {loss_smooth.item():.5f}, Loss_semigroup: {loss_semigroup.item():.5f}\"\n",
    "            )\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        self.dnn.train()\n",
    "        self.optimizer.step(self.loss_function)\n",
    "    \n",
    "    \n",
    "    def predict(self, t, y0):\n",
    "        \n",
    "        t = torch.tensor(t, requires_grad=True).float().to(device)\n",
    "        y0 = torch.tensor(y0, requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.dnn.eval()\n",
    "        y = self.net_y(t, y0)\n",
    "        y = y.detach().cpu().numpy()\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68934f41",
   "metadata": {},
   "source": [
    "### Setup data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5ae371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Changed the net to 6 x 64 \"\"\"\n",
    "\n",
    "layers = [3, 64, 64, 64, 64, 64, 64, 2]\n",
    "\n",
    "T = 1\n",
    "max_y0 = 5\n",
    "\n",
    "# standard PINN loss function training samples\n",
    "N_pinn = 10000\n",
    "N_semigroup = 10000\n",
    "N_smooth = 10000\n",
    "\n",
    "\n",
    "t_pinn = np.random.uniform(0, T, (N_pinn, 1))\n",
    "y_pinn = np.random.uniform(0, max_y0, (N_pinn, 2))\n",
    "X_pinn = np.hstack([t_pinn, y_pinn])\n",
    "\n",
    "\n",
    "r1 = np.random.uniform(0, 1, N_semigroup)\n",
    "r2 = np.random.uniform(0, 1, N_semigroup)\n",
    "s_semigroup, t_semigroup = np.sqrt(r1) * (1 - r2), r2 * np.sqrt(r1)\n",
    "s_semigroup, t_semigroup = T * s_semigroup[:, np.newaxis], T * t_semigroup[:, np.newaxis]\n",
    "y_semigroup = np.random.uniform(0, max_y0, (N_semigroup, 2))\n",
    "X_semigroup = np.hstack([s_semigroup, t_semigroup, y_semigroup])\n",
    "\n",
    "\n",
    "t_smooth = np.random.uniform(0, T, (N_smooth, 1))\n",
    "y_smooth = np.random.uniform(0, max_y0, (N_smooth, 2))\n",
    "X_smooth = np.hstack([t_smooth, y_smooth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e50f9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PINN(X_pinn, X_semigroup, X_smooth, layers, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeb83529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100, Loss: 2.08969, Loss_pinn: 1.55702 Loss_smooth: 0.52233, Loss_semigroup: 0.01035\n",
      "Iter 200, Loss: 0.39984, Loss_pinn: 0.25900 Loss_smooth: 0.13734, Loss_semigroup: 0.00350\n",
      "Iter 300, Loss: 0.14981, Loss_pinn: 0.09611 Loss_smooth: 0.05281, Loss_semigroup: 0.00089\n",
      "Iter 400, Loss: 0.08359, Loss_pinn: 0.05276 Loss_smooth: 0.02986, Loss_semigroup: 0.00097\n",
      "Iter 500, Loss: 0.04439, Loss_pinn: 0.02867 Loss_smooth: 0.01519, Loss_semigroup: 0.00054\n",
      "Iter 600, Loss: 0.02971, Loss_pinn: 0.01980 Loss_smooth: 0.00962, Loss_semigroup: 0.00028\n",
      "Iter 700, Loss: 0.02030, Loss_pinn: 0.01354 Loss_smooth: 0.00640, Loss_semigroup: 0.00037\n",
      "Iter 800, Loss: 0.01506, Loss_pinn: 0.00975 Loss_smooth: 0.00508, Loss_semigroup: 0.00023\n",
      "Iter 900, Loss: 0.01163, Loss_pinn: 0.00754 Loss_smooth: 0.00391, Loss_semigroup: 0.00018\n",
      "Iter 1000, Loss: 0.00916, Loss_pinn: 0.00581 Loss_smooth: 0.00315, Loss_semigroup: 0.00020\n",
      "Iter 1100, Loss: 0.00685, Loss_pinn: 0.00424 Loss_smooth: 0.00249, Loss_semigroup: 0.00012\n",
      "Iter 1200, Loss: 0.00533, Loss_pinn: 0.00332 Loss_smooth: 0.00192, Loss_semigroup: 0.00009\n",
      "Iter 1300, Loss: 0.00447, Loss_pinn: 0.00278 Loss_smooth: 0.00161, Loss_semigroup: 0.00008\n",
      "Iter 1400, Loss: 0.00367, Loss_pinn: 0.00233 Loss_smooth: 0.00129, Loss_semigroup: 0.00006\n",
      "Iter 1500, Loss: 0.00322, Loss_pinn: 0.00202 Loss_smooth: 0.00115, Loss_semigroup: 0.00005\n",
      "Iter 1600, Loss: 0.00283, Loss_pinn: 0.00179 Loss_smooth: 0.00100, Loss_semigroup: 0.00004\n",
      "Iter 1700, Loss: 0.00245, Loss_pinn: 0.00150 Loss_smooth: 0.00089, Loss_semigroup: 0.00005\n",
      "Iter 1800, Loss: 0.00212, Loss_pinn: 0.00135 Loss_smooth: 0.00073, Loss_semigroup: 0.00004\n",
      "Iter 1900, Loss: 0.00189, Loss_pinn: 0.00122 Loss_smooth: 0.00063, Loss_semigroup: 0.00003\n",
      "Iter 2000, Loss: 0.00174, Loss_pinn: 0.00112 Loss_smooth: 0.00058, Loss_semigroup: 0.00003\n",
      "Iter 2100, Loss: 0.00156, Loss_pinn: 0.00103 Loss_smooth: 0.00051, Loss_semigroup: 0.00003\n",
      "Iter 2200, Loss: 0.00146, Loss_pinn: 0.00097 Loss_smooth: 0.00047, Loss_semigroup: 0.00002\n",
      "Iter 2300, Loss: 0.00137, Loss_pinn: 0.00089 Loss_smooth: 0.00046, Loss_semigroup: 0.00002\n",
      "Iter 2400, Loss: 0.00130, Loss_pinn: 0.00084 Loss_smooth: 0.00043, Loss_semigroup: 0.00002\n",
      "Iter 2500, Loss: 0.00122, Loss_pinn: 0.00078 Loss_smooth: 0.00041, Loss_semigroup: 0.00003\n",
      "Iter 2600, Loss: 0.00113, Loss_pinn: 0.00073 Loss_smooth: 0.00038, Loss_semigroup: 0.00002\n",
      "Iter 2700, Loss: 0.00105, Loss_pinn: 0.00067 Loss_smooth: 0.00035, Loss_semigroup: 0.00002\n",
      "Iter 2800, Loss: 0.00099, Loss_pinn: 0.00064 Loss_smooth: 0.00033, Loss_semigroup: 0.00002\n",
      "Iter 2900, Loss: 0.00095, Loss_pinn: 0.00061 Loss_smooth: 0.00032, Loss_semigroup: 0.00002\n",
      "Iter 3000, Loss: 0.00088, Loss_pinn: 0.00058 Loss_smooth: 0.00028, Loss_semigroup: 0.00002\n",
      "Iter 3100, Loss: 0.00082, Loss_pinn: 0.00054 Loss_smooth: 0.00026, Loss_semigroup: 0.00002\n",
      "Iter 3200, Loss: 0.00077, Loss_pinn: 0.00050 Loss_smooth: 0.00025, Loss_semigroup: 0.00002\n",
      "Iter 3300, Loss: 0.00072, Loss_pinn: 0.00047 Loss_smooth: 0.00023, Loss_semigroup: 0.00002\n",
      "Iter 3400, Loss: 0.00069, Loss_pinn: 0.00045 Loss_smooth: 0.00022, Loss_semigroup: 0.00002\n",
      "Iter 3500, Loss: 0.00066, Loss_pinn: 0.00043 Loss_smooth: 0.00021, Loss_semigroup: 0.00001\n",
      "Iter 3600, Loss: 0.00063, Loss_pinn: 0.00042 Loss_smooth: 0.00020, Loss_semigroup: 0.00001\n",
      "Iter 3700, Loss: 0.00061, Loss_pinn: 0.00040 Loss_smooth: 0.00019, Loss_semigroup: 0.00001\n",
      "Iter 3800, Loss: 0.00059, Loss_pinn: 0.00038 Loss_smooth: 0.00019, Loss_semigroup: 0.00002\n",
      "Iter 3900, Loss: 0.00056, Loss_pinn: 0.00037 Loss_smooth: 0.00018, Loss_semigroup: 0.00001\n",
      "Iter 4000, Loss: 0.00053, Loss_pinn: 0.00034 Loss_smooth: 0.00018, Loss_semigroup: 0.00001\n",
      "Iter 4100, Loss: 0.00051, Loss_pinn: 0.00033 Loss_smooth: 0.00017, Loss_semigroup: 0.00001\n",
      "Iter 4200, Loss: 0.00050, Loss_pinn: 0.00032 Loss_smooth: 0.00016, Loss_semigroup: 0.00001\n",
      "Iter 4300, Loss: 0.00046, Loss_pinn: 0.00030 Loss_smooth: 0.00015, Loss_semigroup: 0.00001\n",
      "Iter 4400, Loss: 0.00044, Loss_pinn: 0.00028 Loss_smooth: 0.00014, Loss_semigroup: 0.00001\n",
      "Iter 4500, Loss: 0.00042, Loss_pinn: 0.00027 Loss_smooth: 0.00014, Loss_semigroup: 0.00001\n",
      "Iter 4600, Loss: 0.00041, Loss_pinn: 0.00026 Loss_smooth: 0.00013, Loss_semigroup: 0.00001\n",
      "Iter 4700, Loss: 0.00039, Loss_pinn: 0.00025 Loss_smooth: 0.00012, Loss_semigroup: 0.00001\n",
      "Iter 4800, Loss: 0.00038, Loss_pinn: 0.00025 Loss_smooth: 0.00012, Loss_semigroup: 0.00001\n",
      "Iter 4900, Loss: 0.00036, Loss_pinn: 0.00024 Loss_smooth: 0.00012, Loss_semigroup: 0.00001\n",
      "Iter 5000, Loss: 0.00035, Loss_pinn: 0.00023 Loss_smooth: 0.00011, Loss_semigroup: 0.00001\n",
      "Iter 5100, Loss: 0.00033, Loss_pinn: 0.00022 Loss_smooth: 0.00011, Loss_semigroup: 0.00001\n",
      "Iter 5200, Loss: 0.00032, Loss_pinn: 0.00021 Loss_smooth: 0.00011, Loss_semigroup: 0.00001\n",
      "Iter 5300, Loss: 0.00031, Loss_pinn: 0.00020 Loss_smooth: 0.00010, Loss_semigroup: 0.00001\n",
      "Iter 5400, Loss: 0.00030, Loss_pinn: 0.00019 Loss_smooth: 0.00010, Loss_semigroup: 0.00001\n",
      "Iter 5500, Loss: 0.00028, Loss_pinn: 0.00018 Loss_smooth: 0.00010, Loss_semigroup: 0.00001\n",
      "Iter 5600, Loss: 0.00027, Loss_pinn: 0.00017 Loss_smooth: 0.00009, Loss_semigroup: 0.00001\n",
      "Iter 5700, Loss: 0.00026, Loss_pinn: 0.00017 Loss_smooth: 0.00009, Loss_semigroup: 0.00001\n",
      "Iter 5800, Loss: 0.00026, Loss_pinn: 0.00016 Loss_smooth: 0.00009, Loss_semigroup: 0.00001\n",
      "Iter 5900, Loss: 0.00025, Loss_pinn: 0.00016 Loss_smooth: 0.00008, Loss_semigroup: 0.00001\n",
      "Iter 6000, Loss: 0.00024, Loss_pinn: 0.00016 Loss_smooth: 0.00008, Loss_semigroup: 0.00001\n",
      "Iter 6100, Loss: 0.00024, Loss_pinn: 0.00015 Loss_smooth: 0.00008, Loss_semigroup: 0.00001\n",
      "Iter 6200, Loss: 0.00023, Loss_pinn: 0.00015 Loss_smooth: 0.00007, Loss_semigroup: 0.00001\n",
      "Iter 6300, Loss: 0.00022, Loss_pinn: 0.00015 Loss_smooth: 0.00007, Loss_semigroup: 0.00001\n",
      "Iter 6400, Loss: 0.00021, Loss_pinn: 0.00014 Loss_smooth: 0.00007, Loss_semigroup: 0.00001\n",
      "Iter 6500, Loss: 0.00021, Loss_pinn: 0.00014 Loss_smooth: 0.00006, Loss_semigroup: 0.00001\n",
      "Iter 6600, Loss: 0.00020, Loss_pinn: 0.00014 Loss_smooth: 0.00006, Loss_semigroup: 0.00000\n",
      "Iter 6700, Loss: 0.00020, Loss_pinn: 0.00013 Loss_smooth: 0.00006, Loss_semigroup: 0.00000\n",
      "Iter 6800, Loss: 0.00019, Loss_pinn: 0.00013 Loss_smooth: 0.00006, Loss_semigroup: 0.00001\n",
      "Iter 6900, Loss: 0.00019, Loss_pinn: 0.00013 Loss_smooth: 0.00006, Loss_semigroup: 0.00001\n",
      "Iter 7000, Loss: 0.00018, Loss_pinn: 0.00012 Loss_smooth: 0.00005, Loss_semigroup: 0.00000\n",
      "Iter 7100, Loss: 0.00018, Loss_pinn: 0.00012 Loss_smooth: 0.00005, Loss_semigroup: 0.00000\n",
      "Iter 7200, Loss: 0.00017, Loss_pinn: 0.00012 Loss_smooth: 0.00005, Loss_semigroup: 0.00000\n",
      "Iter 7300, Loss: 0.00017, Loss_pinn: 0.00011 Loss_smooth: 0.00005, Loss_semigroup: 0.00000\n",
      "Iter 7400, Loss: 0.00017, Loss_pinn: 0.00011 Loss_smooth: 0.00005, Loss_semigroup: 0.00000\n",
      "Iter 7500, Loss: 0.00016, Loss_pinn: 0.00011 Loss_smooth: 0.00005, Loss_semigroup: 0.00000\n",
      "Iter 7600, Loss: 0.00016, Loss_pinn: 0.00011 Loss_smooth: 0.00005, Loss_semigroup: 0.00000\n",
      "Iter 7700, Loss: 0.00016, Loss_pinn: 0.00011 Loss_smooth: 0.00005, Loss_semigroup: 0.00000\n",
      "Iter 7800, Loss: 0.00015, Loss_pinn: 0.00010 Loss_smooth: 0.00005, Loss_semigroup: 0.00000\n",
      "Iter 7900, Loss: 0.00015, Loss_pinn: 0.00010 Loss_smooth: 0.00004, Loss_semigroup: 0.00000\n",
      "Iter 8000, Loss: 0.00015, Loss_pinn: 0.00010 Loss_smooth: 0.00004, Loss_semigroup: 0.00000\n",
      "Iter 8100, Loss: 0.00014, Loss_pinn: 0.00010 Loss_smooth: 0.00004, Loss_semigroup: 0.00000\n",
      "Iter 8200, Loss: 0.00014, Loss_pinn: 0.00010 Loss_smooth: 0.00004, Loss_semigroup: 0.00000\n",
      "Iter 8300, Loss: 0.00014, Loss_pinn: 0.00009 Loss_smooth: 0.00004, Loss_semigroup: 0.00000\n",
      "Iter 8400, Loss: 0.00014, Loss_pinn: 0.00009 Loss_smooth: 0.00004, Loss_semigroup: 0.00000\n",
      "Iter 8500, Loss: 0.00014, Loss_pinn: 0.00009 Loss_smooth: 0.00004, Loss_semigroup: 0.00000\n",
      "Iter 8600, Loss: 0.00013, Loss_pinn: 0.00009 Loss_smooth: 0.00004, Loss_semigroup: 0.00000\n",
      "Iter 8700, Loss: 0.00013, Loss_pinn: 0.00009 Loss_smooth: 0.00004, Loss_semigroup: 0.00000\n",
      "Iter 8800, Loss: 0.00013, Loss_pinn: 0.00009 Loss_smooth: 0.00004, Loss_semigroup: 0.00000\n",
      "Iter 8900, Loss: 0.00013, Loss_pinn: 0.00008 Loss_smooth: 0.00004, Loss_semigroup: 0.00000\n",
      "Iter 9000, Loss: 0.00012, Loss_pinn: 0.00008 Loss_smooth: 0.00004, Loss_semigroup: 0.00000\n",
      "Iter 9100, Loss: 0.00012, Loss_pinn: 0.00008 Loss_smooth: 0.00004, Loss_semigroup: 0.00000\n",
      "Iter 9200, Loss: 0.00012, Loss_pinn: 0.00008 Loss_smooth: 0.00004, Loss_semigroup: 0.00000\n",
      "Iter 9300, Loss: 0.00012, Loss_pinn: 0.00008 Loss_smooth: 0.00004, Loss_semigroup: 0.00000\n",
      "Iter 9400, Loss: 0.00011, Loss_pinn: 0.00007 Loss_smooth: 0.00004, Loss_semigroup: 0.00000\n",
      "Iter 9500, Loss: 0.00011, Loss_pinn: 0.00007 Loss_smooth: 0.00004, Loss_semigroup: 0.00000\n",
      "Iter 9600, Loss: 0.00011, Loss_pinn: 0.00007 Loss_smooth: 0.00003, Loss_semigroup: 0.00000\n",
      "Iter 9700, Loss: 0.00011, Loss_pinn: 0.00007 Loss_smooth: 0.00003, Loss_semigroup: 0.00000\n",
      "Iter 9800, Loss: 0.00011, Loss_pinn: 0.00007 Loss_smooth: 0.00003, Loss_semigroup: 0.00000\n",
      "Iter 9900, Loss: 0.00010, Loss_pinn: 0.00007 Loss_smooth: 0.00003, Loss_semigroup: 0.00000\n",
      "Iter 10000, Loss: 0.00010, Loss_pinn: 0.00006 Loss_smooth: 0.00003, Loss_semigroup: 0.00000\n",
      "Iter 10100, Loss: 0.00010, Loss_pinn: 0.00006 Loss_smooth: 0.00003, Loss_semigroup: 0.00000\n",
      "Iter 10200, Loss: 0.00010, Loss_pinn: 0.00006 Loss_smooth: 0.00003, Loss_semigroup: 0.00000\n",
      "Iter 10300, Loss: 0.00010, Loss_pinn: 0.00006 Loss_smooth: 0.00003, Loss_semigroup: 0.00000\n",
      "Iter 10400, Loss: 0.00009, Loss_pinn: 0.00006 Loss_smooth: 0.00003, Loss_semigroup: 0.00000\n",
      "Iter 10500, Loss: 0.00009, Loss_pinn: 0.00006 Loss_smooth: 0.00003, Loss_semigroup: 0.00000\n",
      "Iter 10600, Loss: 0.00009, Loss_pinn: 0.00006 Loss_smooth: 0.00003, Loss_semigroup: 0.00000\n",
      "Iter 10700, Loss: 0.00009, Loss_pinn: 0.00006 Loss_smooth: 0.00003, Loss_semigroup: 0.00000\n",
      "Iter 10800, Loss: 0.00009, Loss_pinn: 0.00006 Loss_smooth: 0.00003, Loss_semigroup: 0.00000\n",
      "Iter 10900, Loss: 0.00009, Loss_pinn: 0.00006 Loss_smooth: 0.00003, Loss_semigroup: 0.00000\n",
      "Iter 11000, Loss: 0.00009, Loss_pinn: 0.00006 Loss_smooth: 0.00003, Loss_semigroup: 0.00000\n",
      "Iter 11100, Loss: 0.00008, Loss_pinn: 0.00006 Loss_smooth: 0.00003, Loss_semigroup: 0.00000\n",
      "Iter 11200, Loss: 0.00008, Loss_pinn: 0.00006 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 11300, Loss: 0.00008, Loss_pinn: 0.00005 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 11400, Loss: 0.00008, Loss_pinn: 0.00005 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 11500, Loss: 0.00008, Loss_pinn: 0.00005 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 11600, Loss: 0.00008, Loss_pinn: 0.00005 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 11700, Loss: 0.00008, Loss_pinn: 0.00005 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 11800, Loss: 0.00007, Loss_pinn: 0.00005 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 11900, Loss: 0.00007, Loss_pinn: 0.00005 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 12000, Loss: 0.00007, Loss_pinn: 0.00005 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 12100, Loss: 0.00007, Loss_pinn: 0.00005 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 12200, Loss: 0.00007, Loss_pinn: 0.00005 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 12300, Loss: 0.00007, Loss_pinn: 0.00005 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 12400, Loss: 0.00007, Loss_pinn: 0.00004 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 12500, Loss: 0.00007, Loss_pinn: 0.00004 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 12600, Loss: 0.00007, Loss_pinn: 0.00004 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 12700, Loss: 0.00006, Loss_pinn: 0.00004 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 12800, Loss: 0.00006, Loss_pinn: 0.00004 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 12900, Loss: 0.00006, Loss_pinn: 0.00004 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 13000, Loss: 0.00006, Loss_pinn: 0.00004 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 13100, Loss: 0.00006, Loss_pinn: 0.00004 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 13200, Loss: 0.00006, Loss_pinn: 0.00004 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 13300, Loss: 0.00006, Loss_pinn: 0.00004 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 13400, Loss: 0.00006, Loss_pinn: 0.00004 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 13500, Loss: 0.00006, Loss_pinn: 0.00004 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 13600, Loss: 0.00006, Loss_pinn: 0.00004 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 13700, Loss: 0.00006, Loss_pinn: 0.00004 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 13800, Loss: 0.00005, Loss_pinn: 0.00003 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 13900, Loss: 0.00005, Loss_pinn: 0.00003 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 14000, Loss: 0.00005, Loss_pinn: 0.00003 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 14100, Loss: 0.00005, Loss_pinn: 0.00003 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 14200, Loss: 0.00005, Loss_pinn: 0.00003 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 14300, Loss: 0.00005, Loss_pinn: 0.00003 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 14400, Loss: 0.00005, Loss_pinn: 0.00003 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 14500, Loss: 0.00005, Loss_pinn: 0.00003 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 14600, Loss: 0.00005, Loss_pinn: 0.00003 Loss_smooth: 0.00002, Loss_semigroup: 0.00000\n",
      "Iter 14700, Loss: 0.00005, Loss_pinn: 0.00003 Loss_smooth: 0.00001, Loss_semigroup: 0.00000\n",
      "Iter 14800, Loss: 0.00005, Loss_pinn: 0.00003 Loss_smooth: 0.00001, Loss_semigroup: 0.00000\n",
      "Iter 14900, Loss: 0.00005, Loss_pinn: 0.00003 Loss_smooth: 0.00001, Loss_semigroup: 0.00000\n",
      "Iter 15000, Loss: 0.00005, Loss_pinn: 0.00003 Loss_smooth: 0.00001, Loss_semigroup: 0.00000\n",
      "Iter 15100, Loss: 0.00005, Loss_pinn: 0.00003 Loss_smooth: 0.00001, Loss_semigroup: 0.00000\n",
      "Iter 15200, Loss: 0.00004, Loss_pinn: 0.00003 Loss_smooth: 0.00001, Loss_semigroup: 0.00000\n",
      "Iter 15300, Loss: 0.00004, Loss_pinn: 0.00003 Loss_smooth: 0.00001, Loss_semigroup: 0.00000\n",
      "Iter 15400, Loss: 0.00004, Loss_pinn: 0.00003 Loss_smooth: 0.00001, Loss_semigroup: 0.00000\n",
      "Iter 15500, Loss: 0.00004, Loss_pinn: 0.00003 Loss_smooth: 0.00001, Loss_semigroup: 0.00000\n",
      "Iter 15600, Loss: 0.00004, Loss_pinn: 0.00003 Loss_smooth: 0.00001, Loss_semigroup: 0.00000\n",
      "Iter 15700, Loss: 0.00004, Loss_pinn: 0.00003 Loss_smooth: 0.00001, Loss_semigroup: 0.00000\n",
      "Iter 15800, Loss: 0.00004, Loss_pinn: 0.00003 Loss_smooth: 0.00001, Loss_semigroup: 0.00000\n",
      "Iter 15900, Loss: 0.00004, Loss_pinn: 0.00003 Loss_smooth: 0.00001, Loss_semigroup: 0.00000\n",
      "Iter 16000, Loss: 0.00004, Loss_pinn: 0.00003 Loss_smooth: 0.00001, Loss_semigroup: 0.00000\n",
      "Wall time: 4h 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "               \n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a835b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "\n",
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "torch.save(model.dnn.state_dict(), path + '/model_new.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3e0e6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('layers.layer_0.weight', tensor([[ 0.6039,  0.0127, -0.1380],\n",
      "        [-0.2618,  0.1185,  0.0675],\n",
      "        [-0.2961,  0.9878,  0.2246],\n",
      "        [-0.1508,  0.3020,  0.5934],\n",
      "        [-0.2849,  0.3702,  0.5397],\n",
      "        [ 0.6226,  0.6795,  0.2412],\n",
      "        [ 0.1156,  0.1567,  1.0192],\n",
      "        [ 0.1775, -0.4479, -0.6348],\n",
      "        [-0.3354, -0.3373, -0.2952],\n",
      "        [-0.8970, -0.1149, -0.8433],\n",
      "        [ 0.1445,  0.6219,  0.1104],\n",
      "        [-0.2401, -0.2484, -0.6234],\n",
      "        [ 0.0831, -1.0437, -0.0473],\n",
      "        [ 0.2342,  0.2442, -0.1028],\n",
      "        [ 0.4935,  0.7557,  0.1155],\n",
      "        [-0.1002, -0.0389,  0.2192],\n",
      "        [-0.2170,  0.0863,  0.1278],\n",
      "        [-0.4783, -0.0154, -0.0582],\n",
      "        [-0.1928, -0.0089,  0.3347],\n",
      "        [-0.6260,  0.0084, -0.0961],\n",
      "        [-0.3185,  0.7246, -0.0065],\n",
      "        [ 0.4496, -0.5629, -0.1996],\n",
      "        [ 0.1292,  0.1244,  0.5017],\n",
      "        [-0.5663,  0.0378, -0.2409],\n",
      "        [ 0.1310,  0.2961,  0.1681],\n",
      "        [ 0.8657,  1.7634,  0.1343],\n",
      "        [-0.2918,  0.4962,  0.6793],\n",
      "        [-0.4842, -0.3519, -0.5107],\n",
      "        [-0.3739, -0.0478, -0.0635],\n",
      "        [-0.3421, -0.4117, -0.4647],\n",
      "        [-0.3750,  0.9669, -0.0020],\n",
      "        [-1.2925, -0.1455, -0.0071],\n",
      "        [-0.3177, -0.4420, -0.5609],\n",
      "        [-0.2806,  0.0341, -0.3845],\n",
      "        [ 0.0669, -0.3817,  0.1082],\n",
      "        [ 0.5015, -0.2252, -0.1595],\n",
      "        [ 0.6409,  0.1278, -0.0318],\n",
      "        [-0.5552, -0.7153,  0.0141],\n",
      "        [ 0.7774, -0.0720, -0.0853],\n",
      "        [-0.4275, -0.4752, -0.0179],\n",
      "        [ 0.1650,  0.2650,  0.0036],\n",
      "        [ 0.3456,  0.0651, -0.1237],\n",
      "        [ 0.2309,  0.7963,  0.0678],\n",
      "        [ 0.4281,  0.0537,  0.5624],\n",
      "        [-0.8144, -0.0990, -0.1909],\n",
      "        [ 0.0927,  1.4985,  0.0268],\n",
      "        [ 0.0023,  1.2380, -0.0023],\n",
      "        [ 0.8349,  1.8910,  0.1118],\n",
      "        [-0.2789, -0.1129, -0.6287],\n",
      "        [ 0.7768,  0.1326,  0.0035],\n",
      "        [-0.2018, -0.3534, -0.7347],\n",
      "        [ 0.1623, -0.1679, -0.9637],\n",
      "        [ 0.4511,  0.6196,  0.0711],\n",
      "        [ 0.3436, -0.1586, -0.0315],\n",
      "        [-0.0572, -0.4125, -0.4963],\n",
      "        [-0.2318, -0.9545,  0.0834],\n",
      "        [ 0.1758, -0.5894, -0.1174],\n",
      "        [ 0.0534, -0.4789, -0.3302],\n",
      "        [ 0.2947,  0.1244,  0.6492],\n",
      "        [ 0.3505, -0.1334,  0.1166],\n",
      "        [ 0.5843,  0.9121,  0.4608],\n",
      "        [ 0.2656, -0.4567, -0.0418],\n",
      "        [-1.2782, -1.5455, -1.6723],\n",
      "        [-0.3499,  0.0509,  0.4627]])), ('layers.layer_0.bias', tensor([ 0.0018, -0.1113,  0.0250,  0.4157,  0.5123, -0.2592,  0.1569,  0.4530,\n",
      "         0.4831, -0.5728,  0.6288, -0.5398, -0.1719,  0.3973,  0.1843, -0.4384,\n",
      "        -0.5380,  0.7262,  0.3216,  0.6783,  0.4255, -0.4198, -0.0078, -0.9314,\n",
      "        -0.5255, -0.5209, -0.1320,  0.0980, -0.6091, -0.0787,  0.5634,  0.1174,\n",
      "        -0.2725, -0.5140, -0.4792, -0.1267, -0.3055,  0.1102,  0.5549, -0.0593,\n",
      "         0.3733, -0.0353,  0.6166,  0.4487,  0.7328,  0.3159,  0.4958, -0.4885,\n",
      "        -0.1816, -0.5137, -0.1325, -0.2206,  0.0344,  0.6115, -0.3320, -0.4122,\n",
      "         0.1015,  0.4511,  0.3088,  0.5385, -0.5884, -0.2975, -0.8450,  0.3330])), ('layers.layer_1.weight', tensor([[-0.0796, -0.0401,  0.0442,  ...,  0.0530,  0.0432,  0.0193],\n",
      "        [ 0.2156,  0.0671,  0.0892,  ...,  0.0787, -0.1294,  0.0763],\n",
      "        [ 0.2743,  0.1555,  0.4077,  ...,  0.0031, -0.0127,  0.0208],\n",
      "        ...,\n",
      "        [ 0.1907,  0.0872, -0.0621,  ...,  0.0159, -0.0953,  0.0504],\n",
      "        [ 0.1897,  0.0080, -0.0823,  ..., -0.0286, -0.1459,  0.0292],\n",
      "        [-0.2513, -0.0099, -0.1282,  ...,  0.0501,  0.0337,  0.0849]])), ('layers.layer_1.bias', tensor([-0.1692,  0.0252, -0.0758, -0.3123, -0.0776,  0.0905, -0.0104,  0.1595,\n",
      "        -0.1016, -0.0708,  0.0302,  0.1663, -0.3349, -0.1449,  0.0155,  0.0242,\n",
      "         0.1032,  0.0875, -0.1130,  0.0423,  0.0950, -0.0270, -0.1688, -0.1122,\n",
      "         0.0992, -0.0462,  0.1092,  0.2712, -0.0615,  0.0340,  0.2191,  0.1332,\n",
      "        -0.2275,  0.0804,  0.0229, -0.3062, -0.1093,  0.0018, -0.0251,  0.2123,\n",
      "         0.0688,  0.1028,  0.0095, -0.2436,  0.0856, -0.0506, -0.0858, -0.0112,\n",
      "         0.0619,  0.1587,  0.0193, -0.0358, -0.0902, -0.0852, -0.0039,  0.0862,\n",
      "        -0.0168,  0.1652,  0.0595, -0.1132, -0.2767, -0.0250,  0.1119,  0.1500])), ('layers.layer_2.weight', tensor([[ 0.1017, -0.0345,  0.0727,  ..., -0.0004,  0.0022,  0.0083],\n",
      "        [-0.1769,  0.1552,  0.0679,  ..., -0.0258, -0.1315, -0.0807],\n",
      "        [-0.0350, -0.0787, -0.0573,  ..., -0.0499, -0.0823,  0.0330],\n",
      "        ...,\n",
      "        [ 0.1462, -0.0007,  0.0523,  ...,  0.0453,  0.1739, -0.1302],\n",
      "        [ 0.3589,  0.0300, -0.1134,  ..., -0.0662, -0.1499,  0.1920],\n",
      "        [ 0.0501, -0.0681,  0.0616,  ..., -0.0532,  0.1255, -0.0966]])), ('layers.layer_2.bias', tensor([-0.0573,  0.0722,  0.0400,  0.1714, -0.0419, -0.0317,  0.1818, -0.0177,\n",
      "        -0.0008, -0.0799,  0.0199, -0.0844,  0.0041,  0.0946,  0.0748,  0.0765,\n",
      "        -0.0276,  0.1604, -0.0035, -0.0744, -0.0262, -0.1582,  0.0300, -0.1597,\n",
      "        -0.1333,  0.1163, -0.3228,  0.1374,  0.0518,  0.3289, -0.1161, -0.0891,\n",
      "         0.1376,  0.0634,  0.0933,  0.0320,  0.3353,  0.0866, -0.1473,  0.0257,\n",
      "         0.0172,  0.0383,  0.0591,  0.1307, -0.0696,  0.0510,  0.0013,  0.0202,\n",
      "         0.0636, -0.0692, -0.5300,  0.1572, -0.0589,  0.0125, -0.0570,  0.1612,\n",
      "        -0.0497, -0.1360,  0.0918, -0.0327, -0.1260,  0.0335,  0.0456,  0.0340])), ('layers.layer_3.weight', tensor([[ 0.0417, -0.0948, -0.2119,  ...,  0.0337,  0.4219,  0.0300],\n",
      "        [ 0.1557, -0.0008,  0.0901,  ..., -0.0252, -0.1423, -0.0008],\n",
      "        [-0.0252, -0.1428, -0.1535,  ...,  0.0951,  0.2177,  0.1820],\n",
      "        ...,\n",
      "        [-0.0228, -0.1205,  0.0241,  ..., -0.1420,  0.3986,  0.0505],\n",
      "        [-0.1412, -0.1275, -0.1524,  ...,  0.0436,  0.0293,  0.0054],\n",
      "        [-0.1448,  0.0037, -0.0406,  ..., -0.0309,  0.0771,  0.0577]])), ('layers.layer_3.bias', tensor([-0.0563, -0.0012, -0.0206, -0.0306,  0.0750, -0.0615,  0.0086,  0.1292,\n",
      "        -0.1786, -0.0028,  0.0887, -0.0350, -0.0201,  0.0263, -0.1041, -0.1225,\n",
      "         0.0418, -0.0323,  0.0003, -0.0388, -0.0941,  0.1769, -0.0012,  0.0932,\n",
      "        -0.0755,  0.0263, -0.0377,  0.1162,  0.1113,  0.1099,  0.0532,  0.0772,\n",
      "         0.0475,  0.1284,  0.0695, -0.0730,  0.2847,  0.0328, -0.0733, -0.0425,\n",
      "        -0.0981,  0.2419,  0.0382,  0.1219,  0.0065, -0.0453, -0.0331, -0.0416,\n",
      "        -0.0043,  0.2935,  0.0927,  0.1671,  0.0080,  0.0893,  0.2463, -0.1152,\n",
      "         0.0245,  0.0216,  0.0112, -0.1420,  0.0276,  0.0374, -0.0273, -0.2829])), ('layers.layer_4.weight', tensor([[-0.0897,  0.2370, -0.0530,  ...,  0.0586,  0.0201, -0.0844],\n",
      "        [-0.0884, -0.0446,  0.1180,  ..., -0.0933, -0.0839,  0.0415],\n",
      "        [-0.2849,  0.1214, -0.0080,  ...,  0.1220, -0.1012,  0.1462],\n",
      "        ...,\n",
      "        [ 0.0750,  0.0831,  0.0235,  ...,  0.1233,  0.0910,  0.1413],\n",
      "        [ 0.0246, -0.0632,  0.1167,  ...,  0.0779, -0.0501, -0.0342],\n",
      "        [ 0.1019,  0.1644, -0.0533,  ..., -0.1054,  0.0380, -0.1262]])), ('layers.layer_4.bias', tensor([-0.1519,  0.1818,  0.3709, -0.1687, -0.5136,  0.0730, -0.1325,  0.2061,\n",
      "        -0.2453,  0.2547,  0.0141, -0.1870,  0.0182, -0.0931, -0.2682, -0.3273,\n",
      "         0.1074, -0.1701,  0.0528,  0.0470,  0.0124,  0.0222,  0.1202,  0.2904,\n",
      "        -0.1738,  0.2346, -0.1313, -0.2611,  0.0269, -0.0610, -0.2642,  0.1046,\n",
      "        -0.0613, -0.2313,  0.1959, -0.2593,  0.0098,  0.3020, -0.2300, -0.1454,\n",
      "         0.2254, -0.2238, -0.3188,  0.1583,  0.0994,  0.0301, -0.2230,  0.0605,\n",
      "        -0.0436,  0.2037, -0.0695, -0.0480,  0.0631, -0.1228,  0.0804, -0.1034,\n",
      "         0.0907,  0.0306, -0.1050,  0.2257, -0.1095,  0.0197, -0.0771, -0.0392])), ('layers.layer_5.weight', tensor([[-0.0882, -0.1755, -0.1004,  ...,  0.0568,  0.2180, -0.1231],\n",
      "        [ 0.1380,  0.1043,  0.0722,  ..., -0.1082, -0.0793,  0.0689],\n",
      "        [-0.1836, -0.0977, -0.1378,  ..., -0.0863,  0.1178,  0.0034],\n",
      "        ...,\n",
      "        [ 0.0544, -0.0068,  0.2741,  ...,  0.0707,  0.0718, -0.0045],\n",
      "        [ 0.0282,  0.1958,  0.3024,  ...,  0.1695,  0.0173,  0.1856],\n",
      "        [ 0.0304,  0.0681,  0.1371,  ...,  0.0693, -0.0349,  0.0723]])), ('layers.layer_5.bias', tensor([-0.5315,  0.3040, -0.2109,  0.3804, -0.2748, -0.3135,  0.3335,  0.5986,\n",
      "         0.3148,  0.3280,  0.1289,  0.0981, -0.0596,  0.0265,  0.2125,  0.1075,\n",
      "         0.2857, -0.3273, -0.3284, -0.6148,  0.0094,  0.4008, -0.0775,  0.3117,\n",
      "        -0.1661, -0.5341,  0.2386, -0.1415,  0.2622, -0.5357,  0.3062,  0.3885,\n",
      "         0.0437,  0.1751, -0.1920, -0.2903, -0.1264, -0.1340, -0.3008,  0.4291,\n",
      "         0.0258, -0.4424, -0.0759,  0.2866, -0.0831,  0.0890, -0.4602,  0.4397,\n",
      "         0.1563, -0.6206, -0.0063, -0.2449, -0.2608, -0.1863, -0.3237,  0.3002,\n",
      "        -0.3531, -0.1133,  0.2867,  0.4052, -0.0124,  0.4383,  0.4587,  0.1539])), ('layers.layer_6.weight', tensor([[-1.1828e+00,  1.8257e+00, -1.3611e+00,  1.4818e+00, -8.4078e-02,\n",
      "         -7.9093e-01,  5.5692e-01,  1.1233e+00,  5.3787e-01,  3.7652e-01,\n",
      "         -3.0401e-02, -2.3056e-01, -7.0879e-01, -5.6348e-01, -2.2575e-01,\n",
      "         -4.8644e-01,  3.0231e-02,  2.2147e-03, -4.3128e-01, -1.3364e+00,\n",
      "          2.9929e-01,  1.4775e+00, -3.7183e-01, -2.1643e-01, -6.6654e-01,\n",
      "         -3.8796e-01,  6.5415e-01, -9.4031e-01,  1.1246e+00, -3.1984e-01,\n",
      "          1.6295e+00,  1.0443e+00,  9.5878e-01,  7.0762e-01, -1.0720e+00,\n",
      "          4.5278e-02, -5.0047e-01, -1.0552e+00,  1.5140e-01,  1.1027e+00,\n",
      "          3.7126e-04, -1.6356e+00, -7.1347e-01,  1.1254e+00, -5.9669e-01,\n",
      "          2.5050e-01, -1.5801e+00,  7.7057e-01,  1.0391e+00, -1.3532e+00,\n",
      "          8.8068e-01, -6.7571e-01, -3.8504e-01, -1.2947e+00, -1.6950e+00,\n",
      "          8.6060e-01, -1.1744e-01, -4.9611e-02,  3.4986e-01,  7.2224e-01,\n",
      "         -5.4476e-01, -5.3203e-02,  1.6887e-01,  2.7041e-01],\n",
      "        [ 1.1967e+00, -2.2415e+00,  1.9824e+00, -1.5030e+00, -5.1132e-01,\n",
      "          6.2597e-01, -6.1970e-01, -7.6985e-01, -1.1391e+00, -4.9562e-01,\n",
      "         -3.6808e-01,  1.2002e-01,  1.1094e+00,  7.9760e-01,  5.5734e-01,\n",
      "          7.7080e-01,  2.6155e-01, -3.1029e-01,  8.6603e-02,  7.1923e-01,\n",
      "          3.5411e-01, -1.0393e+00,  7.9739e-01,  7.5496e-02, -3.5607e-02,\n",
      "          1.1357e-01, -5.9924e-01,  2.0239e+00, -1.0264e+00,  3.7105e-01,\n",
      "         -1.7315e+00, -1.3120e+00, -1.1676e+00, -8.6674e-01,  1.5769e+00,\n",
      "         -7.0459e-01,  2.1018e-01,  1.4820e+00, -3.1281e-01, -1.1205e+00,\n",
      "         -6.2485e-01,  1.6550e+00,  1.3389e+00, -7.6860e-01,  1.7649e+00,\n",
      "          2.6888e-01,  1.6311e+00, -1.0583e+00, -1.3505e+00,  6.1214e-01,\n",
      "         -1.1070e+00,  1.3890e+00,  1.8867e-01,  2.0667e+00,  1.3666e+00,\n",
      "         -8.8048e-01, -3.7538e-01,  3.6229e-01, -6.5740e-01, -2.3254e-01,\n",
      "          3.1089e-01,  7.2280e-02, -2.2100e-01, -1.9136e-01]])), ('layers.layer_6.bias', tensor([ 2.6527, -4.0855]))])\n"
     ]
    }
   ],
   "source": [
    "#model.dnn.load_state_dict(torch.load(path + '/model.pt'))\n",
    "\n",
    "print(model.dnn.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd52a59",
   "metadata": {},
   "source": [
    "## Predict and Plot the Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff2236a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_figure(figsize, xlim, ylim):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.spines[['top', 'right']].set_visible(False)\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_ode_solution(ax, y, *args, **kwargs):\n",
    "    \n",
    "    ax.plot(y[:,0], y[:,1], '.-', *args, **kwargs)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f561ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_standard(model, y0, max_t_pred, delta_t):\n",
    "    \n",
    "    times = np.linspace(0, max_t_pred, int(max_t_pred / delta_t) + 1)\n",
    "    times = times[:,np.newaxis]\n",
    "    \n",
    "    y0 = np.array([y0 for _ in range(len(times))])\n",
    "    trajectory =  model.predict(times, y0)\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "\n",
    "def predict_dac(model, y0, max_t_pred, delta_t):\n",
    "    \"\"\"\n",
    "    detla_t should devide model.max_t to guarantee equidistant steps\n",
    "    \"\"\"\n",
    "    times = np.arange(0, model.T + delta_t, delta_t)[1:]\n",
    "    times = times[:,np.newaxis]\n",
    "    n_resets = int(np.ceil(max_t_pred / model.T))\n",
    "    \n",
    "    trajectory = np.array([y0])\n",
    "    \n",
    "    for _ in range(n_resets):\n",
    "        \n",
    "        y0 = trajectory[-1]\n",
    "        y0 = np.array([y0 for _ in range(len(times))])\n",
    "        segment =  model.predict(times, y0)\n",
    "        trajectory = np.vstack([trajectory, segment])\n",
    "    \n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2f012bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" need to change the initial values here \"\"\"\n",
    "\n",
    "# Note that max_t in training is 1\n",
    "y0 = [1., 0.15]\n",
    "max_t_pred = 7.\n",
    "delta_t = 0.05\n",
    "\n",
    "validation_dac = predict_dac(model, y0, max_t_pred, delta_t)\n",
    "validation_standard = predict_standard(model, y0, max_t_pred, delta_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74e11fc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\" true solution via solver (returns np.array) \"\"\"\n",
    "\n",
    "\n",
    "def func(t, r):\n",
    "    x, y = r\n",
    "    dx_t = x - x * y\n",
    "    dy_t = x * y - y\n",
    "    return dx_t, dy_t\n",
    "\n",
    "\n",
    "def gen_truedata():\n",
    "    t = np.linspace(0, max_t_pred, int(max_t_pred / delta_t) + 1)\n",
    "\n",
    "    sol = integrate.solve_ivp(func, (0, 10), (y0[0], y0[1]), t_eval=t) \n",
    "    x_true, y_true = sol.y\n",
    "\n",
    "    return np.stack((x_true, y_true), axis = 1)\n",
    "\n",
    "\n",
    "true_solution = gen_truedata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21cf58fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "fig, ax = generate_figure(figsize=(8,8), xlim=[-7, 7], ylim=[-7, 7]) # probably need to change the limits\n",
    "\n",
    "ax = plot_ode_solution(ax, validation_standard, label=\"Standard approach\", color=\"#03468F\")\n",
    "ax = plot_ode_solution(ax, validation_dac, label=\"DaC approach\", color=\"#A51C30\")\n",
    "ax = plot_ode_solution(ax, true_solution, label=\"true solution\", color=\"orange\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"proof_of_concept.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5beae9835b3277c3a4a8c87413b972e297eaccb765a3f62b691c35696bfb6223"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
