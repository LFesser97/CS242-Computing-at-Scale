{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c314520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['pgf.texsystem'] = 'pdflatex'\n",
    "matplotlib.rcParams.update({'font.family': 'serif', 'font.size': 10})\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "from scipy import integrate\n",
    "import time\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6d595",
   "metadata": {},
   "source": [
    "In this example, we solve a Lotka-Volterra Equation of the general form\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{dr}{dt} = \\alpha r - \\beta rp \\\\\n",
    "\\frac{dp}{dt} = \\gamma rp - \\delta p\n",
    "\\end{align*}\n",
    "\n",
    "where $r$ is the number of prey, $p$ is the number of some predator, and $\\alpha$, $\\beta$, $\\gamma$, and $\\delta$ are real parameters describing the interactions of the two species. For the sake of simplicity, we assume that these parameters are all equal to one, i.e. we seek to solve the ODE pair\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{dr}{dt} = r - rp \\\\\n",
    "\\frac{dp}{dt} = rp - p\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "135abf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA support \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92eed791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class DNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.Tanh\n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x = (t, y0)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "300b459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PINN: physics-informed neural network\n",
    "class PINN():\n",
    "\n",
    "    def __init__(self, X_pinn, X_semigroup, X_smooth, layers, T):\n",
    "\n",
    "        # neural network architecture\n",
    "        self.layers = layers\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "        \n",
    "        # semigroup PINN step time\n",
    "        self.T = torch.tensor(T).float().to(device)\n",
    "\n",
    "        # training data\n",
    "        self.t_pinn = torch.tensor(X_pinn[:, :1], requires_grad=True).float().to(device)\n",
    "        self.y_pinn = torch.tensor(X_pinn[:, 1:], requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.s_semigroup = torch.tensor(X_semigroup[:, :1], requires_grad=True).float().to(device)\n",
    "        self.t_semigroup = torch.tensor(X_semigroup[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.y_semigroup = torch.tensor(X_semigroup[:, 2:], requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.t_smooth = torch.tensor(X_smooth[:, :1], requires_grad=True).float().to(device)\n",
    "        self.y_smooth = torch.tensor(X_smooth[:, 1:], requires_grad=True).float().to(device)\n",
    "        \n",
    "        # optimization\n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(), lr=1.0, max_iter=50000, max_eval=50000, \n",
    "            history_size=50, tolerance_grad=1e-5, tolerance_change=np.finfo(float).eps, \n",
    "            line_search_fn=\"strong_wolfe\"\n",
    "        )\n",
    "\n",
    "        self.iter = 0\n",
    "    \n",
    "    \n",
    "    def net_y(self, t, y0):\n",
    "        \n",
    "        # The M(t, y0) = y0 + t N(t, y0) scheme seems to drastically increase the accuracy\n",
    "        # This works perfectly fine with automatic differentiation\n",
    "        y = y0 + t * self.dnn(torch.cat([t, y0], dim=1))\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    \n",
    "    def net_derivative(self, t, y0):\n",
    "        \"\"\"\n",
    "        Pytorch automatic differentiation to compute the derivative of the neural network\n",
    "        \"\"\"\n",
    "        y = self.net_y(t, y0)\n",
    "        \n",
    "        # vectors for the autograd vector Jacobian product \n",
    "        # to compute the derivatives w.r.t. every output dimension\n",
    "        vectors = [torch.zeros_like(y) for i in range(2)]\n",
    "        \n",
    "        for i, vec in enumerate(vectors):\n",
    "            \n",
    "            vec[:,i] = 1.\n",
    "        \n",
    "        # list of derivative tensors\n",
    "        # the first entry is a tensor with \\partial_t PINN(t, y0) for all (t, y0) in the batch,\n",
    "        # each input (t, y0) corresponds to one row in each tensor\n",
    "        derivatives = [\n",
    "            torch.autograd.grad(\n",
    "                y, t, \n",
    "                grad_outputs=vec,\n",
    "                retain_graph=True,\n",
    "                create_graph=True\n",
    "            )[0]\n",
    "            for vec in vectors\n",
    "        ]\n",
    "        \n",
    "        return derivatives\n",
    "    \n",
    "    \n",
    "    def loss_function(self):\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = self.net_y(self.t_pinn, self.y_pinn)\n",
    "        deriv_pred = self.net_derivative(self.t_pinn, self.y_pinn)\n",
    "        \n",
    "        \"\"\" Changed this \"\"\"\n",
    "\n",
    "        # in our case, dy1/dt = y1 - y1 * y2, dy2/dt = y1 * y2 -y2\n",
    "        loss_pinn1 = torch.mean((deriv_pred[0] + y_pred[:,0:1] - y_pred[:,0:1] * y_pred[:,1:2]) ** 2)\n",
    "        loss_pinn2 = torch.mean((deriv_pred[1] + y_pred[:,0:1] * y_pred[:,1:2] - y_pred[:,1:2]) ** 2)\n",
    "        loss_pinn = loss_pinn1 + loss_pinn2 \n",
    "        \n",
    "        # The general semigroup loss for autonomous ODEs\n",
    "        y_pred_tps = self.net_y(self.s_semigroup + self.t_semigroup, self.y_semigroup)\n",
    "        y_pred_s = self.net_y(self.s_semigroup, self.y_semigroup)\n",
    "        y_pred_restart = self.net_y(self.t_semigroup, y_pred_s)\n",
    "        loss_semigroup = torch.mean((y_pred_tps - y_pred_restart) ** 2)\n",
    "        \n",
    "        # The smoothness loss\n",
    "        y_pred_smooth = self.net_y(self.t_smooth, self.y_smooth)\n",
    "        deriv_pred_below = self.net_derivative(self.t_smooth, self.y_smooth)\n",
    "        deriv_pred_above = self.net_derivative(torch.zeros_like(self.t_smooth, requires_grad=True), y_pred_smooth)\n",
    "        \n",
    "        loss_smooth = .0\n",
    "        \n",
    "        for t1, t2 in zip(deriv_pred_below, deriv_pred_above):\n",
    "            \n",
    "            loss_smooth += torch.mean((t1 - t2) ** 2)\n",
    "        \n",
    "        loss = loss_pinn + loss_smooth + loss_semigroup\n",
    "        \n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "        \n",
    "        if self.iter % 100 == 0:\n",
    "            print(\n",
    "                f\"Iter {self.iter}, Loss: {loss.item():.5f}, Loss_pinn: {loss_pinn.item():.5f} \" \\\n",
    "                f\"Loss_smooth: {loss_smooth.item():.5f}, Loss_semigroup: {loss_semigroup.item():.5f}\"\n",
    "            )\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        self.dnn.train()\n",
    "        self.optimizer.step(self.loss_function)\n",
    "    \n",
    "    \n",
    "    def predict(self, t, y0):\n",
    "        \n",
    "        t = torch.tensor(t, requires_grad=True).float().to(device)\n",
    "        y0 = torch.tensor(y0, requires_grad=True).float().to(device)\n",
    "        \n",
    "        self.dnn.eval()\n",
    "        y = self.net_y(t, y0)\n",
    "        y = y.detach().cpu().numpy()\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68934f41",
   "metadata": {},
   "source": [
    "### Setup data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5ae371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Changed the net to 6 x 64 \"\"\"\n",
    "\n",
    "layers = [3, 64, 64, 64, 64, 64, 64, 2]\n",
    "\n",
    "T = 1\n",
    "max_y0 = 5\n",
    "\n",
    "# standard PINN loss function training samples\n",
    "N_pinn = 10000\n",
    "N_semigroup = 10000\n",
    "N_smooth = 10000\n",
    "\n",
    "\n",
    "t_pinn = np.random.uniform(0, T, (N_pinn, 1))\n",
    "y_pinn = np.random.uniform(0, max_y0, (N_pinn, 2))\n",
    "X_pinn = np.hstack([t_pinn, y_pinn])\n",
    "\n",
    "\n",
    "r1 = np.random.uniform(0, 1, N_semigroup)\n",
    "r2 = np.random.uniform(0, 1, N_semigroup)\n",
    "s_semigroup, t_semigroup = np.sqrt(r1) * (1 - r2), r2 * np.sqrt(r1)\n",
    "s_semigroup, t_semigroup = T * s_semigroup[:, np.newaxis], T * t_semigroup[:, np.newaxis]\n",
    "y_semigroup = np.random.uniform(0, max_y0, (N_semigroup, 2))\n",
    "X_semigroup = np.hstack([s_semigroup, t_semigroup, y_semigroup])\n",
    "\n",
    "\n",
    "t_smooth = np.random.uniform(0, T, (N_smooth, 1))\n",
    "y_smooth = np.random.uniform(0, max_y0, (N_smooth, 2))\n",
    "X_smooth = np.hstack([t_smooth, y_smooth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e50f9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PINN(X_pinn, X_semigroup, X_smooth, layers, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb83529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "               \n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a835b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "\n",
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "# torch.save(model.dnn.state_dict(), path + '/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3e0e6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('layers.layer_0.weight', tensor([[ 0.6039,  0.0127, -0.1380],\n",
      "        [-0.2618,  0.1185,  0.0675],\n",
      "        [-0.2961,  0.9878,  0.2246],\n",
      "        [-0.1508,  0.3020,  0.5934],\n",
      "        [-0.2849,  0.3702,  0.5397],\n",
      "        [ 0.6226,  0.6795,  0.2412],\n",
      "        [ 0.1156,  0.1567,  1.0192],\n",
      "        [ 0.1775, -0.4479, -0.6348],\n",
      "        [-0.3354, -0.3373, -0.2952],\n",
      "        [-0.8970, -0.1149, -0.8433],\n",
      "        [ 0.1445,  0.6219,  0.1104],\n",
      "        [-0.2401, -0.2484, -0.6234],\n",
      "        [ 0.0831, -1.0437, -0.0473],\n",
      "        [ 0.2342,  0.2442, -0.1028],\n",
      "        [ 0.4935,  0.7557,  0.1155],\n",
      "        [-0.1002, -0.0389,  0.2192],\n",
      "        [-0.2170,  0.0863,  0.1278],\n",
      "        [-0.4783, -0.0154, -0.0582],\n",
      "        [-0.1928, -0.0089,  0.3347],\n",
      "        [-0.6260,  0.0084, -0.0961],\n",
      "        [-0.3185,  0.7246, -0.0065],\n",
      "        [ 0.4496, -0.5629, -0.1996],\n",
      "        [ 0.1292,  0.1244,  0.5017],\n",
      "        [-0.5663,  0.0378, -0.2409],\n",
      "        [ 0.1310,  0.2961,  0.1681],\n",
      "        [ 0.8657,  1.7634,  0.1343],\n",
      "        [-0.2918,  0.4962,  0.6793],\n",
      "        [-0.4842, -0.3519, -0.5107],\n",
      "        [-0.3739, -0.0478, -0.0635],\n",
      "        [-0.3421, -0.4117, -0.4647],\n",
      "        [-0.3750,  0.9669, -0.0020],\n",
      "        [-1.2925, -0.1455, -0.0071],\n",
      "        [-0.3177, -0.4420, -0.5609],\n",
      "        [-0.2806,  0.0341, -0.3845],\n",
      "        [ 0.0669, -0.3817,  0.1082],\n",
      "        [ 0.5015, -0.2252, -0.1595],\n",
      "        [ 0.6409,  0.1278, -0.0318],\n",
      "        [-0.5552, -0.7153,  0.0141],\n",
      "        [ 0.7774, -0.0720, -0.0853],\n",
      "        [-0.4275, -0.4752, -0.0179],\n",
      "        [ 0.1650,  0.2650,  0.0036],\n",
      "        [ 0.3456,  0.0651, -0.1237],\n",
      "        [ 0.2309,  0.7963,  0.0678],\n",
      "        [ 0.4281,  0.0537,  0.5624],\n",
      "        [-0.8144, -0.0990, -0.1909],\n",
      "        [ 0.0927,  1.4985,  0.0268],\n",
      "        [ 0.0023,  1.2380, -0.0023],\n",
      "        [ 0.8349,  1.8910,  0.1118],\n",
      "        [-0.2789, -0.1129, -0.6287],\n",
      "        [ 0.7768,  0.1326,  0.0035],\n",
      "        [-0.2018, -0.3534, -0.7347],\n",
      "        [ 0.1623, -0.1679, -0.9637],\n",
      "        [ 0.4511,  0.6196,  0.0711],\n",
      "        [ 0.3436, -0.1586, -0.0315],\n",
      "        [-0.0572, -0.4125, -0.4963],\n",
      "        [-0.2318, -0.9545,  0.0834],\n",
      "        [ 0.1758, -0.5894, -0.1174],\n",
      "        [ 0.0534, -0.4789, -0.3302],\n",
      "        [ 0.2947,  0.1244,  0.6492],\n",
      "        [ 0.3505, -0.1334,  0.1166],\n",
      "        [ 0.5843,  0.9121,  0.4608],\n",
      "        [ 0.2656, -0.4567, -0.0418],\n",
      "        [-1.2782, -1.5455, -1.6723],\n",
      "        [-0.3499,  0.0509,  0.4627]])), ('layers.layer_0.bias', tensor([ 0.0018, -0.1113,  0.0250,  0.4157,  0.5123, -0.2592,  0.1569,  0.4530,\n",
      "         0.4831, -0.5728,  0.6288, -0.5398, -0.1719,  0.3973,  0.1843, -0.4384,\n",
      "        -0.5380,  0.7262,  0.3216,  0.6783,  0.4255, -0.4198, -0.0078, -0.9314,\n",
      "        -0.5255, -0.5209, -0.1320,  0.0980, -0.6091, -0.0787,  0.5634,  0.1174,\n",
      "        -0.2725, -0.5140, -0.4792, -0.1267, -0.3055,  0.1102,  0.5549, -0.0593,\n",
      "         0.3733, -0.0353,  0.6166,  0.4487,  0.7328,  0.3159,  0.4958, -0.4885,\n",
      "        -0.1816, -0.5137, -0.1325, -0.2206,  0.0344,  0.6115, -0.3320, -0.4122,\n",
      "         0.1015,  0.4511,  0.3088,  0.5385, -0.5884, -0.2975, -0.8450,  0.3330])), ('layers.layer_1.weight', tensor([[-0.0796, -0.0401,  0.0442,  ...,  0.0530,  0.0432,  0.0193],\n",
      "        [ 0.2156,  0.0671,  0.0892,  ...,  0.0787, -0.1294,  0.0763],\n",
      "        [ 0.2743,  0.1555,  0.4077,  ...,  0.0031, -0.0127,  0.0208],\n",
      "        ...,\n",
      "        [ 0.1907,  0.0872, -0.0621,  ...,  0.0159, -0.0953,  0.0504],\n",
      "        [ 0.1897,  0.0080, -0.0823,  ..., -0.0286, -0.1459,  0.0292],\n",
      "        [-0.2513, -0.0099, -0.1282,  ...,  0.0501,  0.0337,  0.0849]])), ('layers.layer_1.bias', tensor([-0.1692,  0.0252, -0.0758, -0.3123, -0.0776,  0.0905, -0.0104,  0.1595,\n",
      "        -0.1016, -0.0708,  0.0302,  0.1663, -0.3349, -0.1449,  0.0155,  0.0242,\n",
      "         0.1032,  0.0875, -0.1130,  0.0423,  0.0950, -0.0270, -0.1688, -0.1122,\n",
      "         0.0992, -0.0462,  0.1092,  0.2712, -0.0615,  0.0340,  0.2191,  0.1332,\n",
      "        -0.2275,  0.0804,  0.0229, -0.3062, -0.1093,  0.0018, -0.0251,  0.2123,\n",
      "         0.0688,  0.1028,  0.0095, -0.2436,  0.0856, -0.0506, -0.0858, -0.0112,\n",
      "         0.0619,  0.1587,  0.0193, -0.0358, -0.0902, -0.0852, -0.0039,  0.0862,\n",
      "        -0.0168,  0.1652,  0.0595, -0.1132, -0.2767, -0.0250,  0.1119,  0.1500])), ('layers.layer_2.weight', tensor([[ 0.1017, -0.0345,  0.0727,  ..., -0.0004,  0.0022,  0.0083],\n",
      "        [-0.1769,  0.1552,  0.0679,  ..., -0.0258, -0.1315, -0.0807],\n",
      "        [-0.0350, -0.0787, -0.0573,  ..., -0.0499, -0.0823,  0.0330],\n",
      "        ...,\n",
      "        [ 0.1462, -0.0007,  0.0523,  ...,  0.0453,  0.1739, -0.1302],\n",
      "        [ 0.3589,  0.0300, -0.1134,  ..., -0.0662, -0.1499,  0.1920],\n",
      "        [ 0.0501, -0.0681,  0.0616,  ..., -0.0532,  0.1255, -0.0966]])), ('layers.layer_2.bias', tensor([-0.0573,  0.0722,  0.0400,  0.1714, -0.0419, -0.0317,  0.1818, -0.0177,\n",
      "        -0.0008, -0.0799,  0.0199, -0.0844,  0.0041,  0.0946,  0.0748,  0.0765,\n",
      "        -0.0276,  0.1604, -0.0035, -0.0744, -0.0262, -0.1582,  0.0300, -0.1597,\n",
      "        -0.1333,  0.1163, -0.3228,  0.1374,  0.0518,  0.3289, -0.1161, -0.0891,\n",
      "         0.1376,  0.0634,  0.0933,  0.0320,  0.3353,  0.0866, -0.1473,  0.0257,\n",
      "         0.0172,  0.0383,  0.0591,  0.1307, -0.0696,  0.0510,  0.0013,  0.0202,\n",
      "         0.0636, -0.0692, -0.5300,  0.1572, -0.0589,  0.0125, -0.0570,  0.1612,\n",
      "        -0.0497, -0.1360,  0.0918, -0.0327, -0.1260,  0.0335,  0.0456,  0.0340])), ('layers.layer_3.weight', tensor([[ 0.0417, -0.0948, -0.2119,  ...,  0.0337,  0.4219,  0.0300],\n",
      "        [ 0.1557, -0.0008,  0.0901,  ..., -0.0252, -0.1423, -0.0008],\n",
      "        [-0.0252, -0.1428, -0.1535,  ...,  0.0951,  0.2177,  0.1820],\n",
      "        ...,\n",
      "        [-0.0228, -0.1205,  0.0241,  ..., -0.1420,  0.3986,  0.0505],\n",
      "        [-0.1412, -0.1275, -0.1524,  ...,  0.0436,  0.0293,  0.0054],\n",
      "        [-0.1448,  0.0037, -0.0406,  ..., -0.0309,  0.0771,  0.0577]])), ('layers.layer_3.bias', tensor([-0.0563, -0.0012, -0.0206, -0.0306,  0.0750, -0.0615,  0.0086,  0.1292,\n",
      "        -0.1786, -0.0028,  0.0887, -0.0350, -0.0201,  0.0263, -0.1041, -0.1225,\n",
      "         0.0418, -0.0323,  0.0003, -0.0388, -0.0941,  0.1769, -0.0012,  0.0932,\n",
      "        -0.0755,  0.0263, -0.0377,  0.1162,  0.1113,  0.1099,  0.0532,  0.0772,\n",
      "         0.0475,  0.1284,  0.0695, -0.0730,  0.2847,  0.0328, -0.0733, -0.0425,\n",
      "        -0.0981,  0.2419,  0.0382,  0.1219,  0.0065, -0.0453, -0.0331, -0.0416,\n",
      "        -0.0043,  0.2935,  0.0927,  0.1671,  0.0080,  0.0893,  0.2463, -0.1152,\n",
      "         0.0245,  0.0216,  0.0112, -0.1420,  0.0276,  0.0374, -0.0273, -0.2829])), ('layers.layer_4.weight', tensor([[-0.0897,  0.2370, -0.0530,  ...,  0.0586,  0.0201, -0.0844],\n",
      "        [-0.0884, -0.0446,  0.1180,  ..., -0.0933, -0.0839,  0.0415],\n",
      "        [-0.2849,  0.1214, -0.0080,  ...,  0.1220, -0.1012,  0.1462],\n",
      "        ...,\n",
      "        [ 0.0750,  0.0831,  0.0235,  ...,  0.1233,  0.0910,  0.1413],\n",
      "        [ 0.0246, -0.0632,  0.1167,  ...,  0.0779, -0.0501, -0.0342],\n",
      "        [ 0.1019,  0.1644, -0.0533,  ..., -0.1054,  0.0380, -0.1262]])), ('layers.layer_4.bias', tensor([-0.1519,  0.1818,  0.3709, -0.1687, -0.5136,  0.0730, -0.1325,  0.2061,\n",
      "        -0.2453,  0.2547,  0.0141, -0.1870,  0.0182, -0.0931, -0.2682, -0.3273,\n",
      "         0.1074, -0.1701,  0.0528,  0.0470,  0.0124,  0.0222,  0.1202,  0.2904,\n",
      "        -0.1738,  0.2346, -0.1313, -0.2611,  0.0269, -0.0610, -0.2642,  0.1046,\n",
      "        -0.0613, -0.2313,  0.1959, -0.2593,  0.0098,  0.3020, -0.2300, -0.1454,\n",
      "         0.2254, -0.2238, -0.3188,  0.1583,  0.0994,  0.0301, -0.2230,  0.0605,\n",
      "        -0.0436,  0.2037, -0.0695, -0.0480,  0.0631, -0.1228,  0.0804, -0.1034,\n",
      "         0.0907,  0.0306, -0.1050,  0.2257, -0.1095,  0.0197, -0.0771, -0.0392])), ('layers.layer_5.weight', tensor([[-0.0882, -0.1755, -0.1004,  ...,  0.0568,  0.2180, -0.1231],\n",
      "        [ 0.1380,  0.1043,  0.0722,  ..., -0.1082, -0.0793,  0.0689],\n",
      "        [-0.1836, -0.0977, -0.1378,  ..., -0.0863,  0.1178,  0.0034],\n",
      "        ...,\n",
      "        [ 0.0544, -0.0068,  0.2741,  ...,  0.0707,  0.0718, -0.0045],\n",
      "        [ 0.0282,  0.1958,  0.3024,  ...,  0.1695,  0.0173,  0.1856],\n",
      "        [ 0.0304,  0.0681,  0.1371,  ...,  0.0693, -0.0349,  0.0723]])), ('layers.layer_5.bias', tensor([-0.5315,  0.3040, -0.2109,  0.3804, -0.2748, -0.3135,  0.3335,  0.5986,\n",
      "         0.3148,  0.3280,  0.1289,  0.0981, -0.0596,  0.0265,  0.2125,  0.1075,\n",
      "         0.2857, -0.3273, -0.3284, -0.6148,  0.0094,  0.4008, -0.0775,  0.3117,\n",
      "        -0.1661, -0.5341,  0.2386, -0.1415,  0.2622, -0.5357,  0.3062,  0.3885,\n",
      "         0.0437,  0.1751, -0.1920, -0.2903, -0.1264, -0.1340, -0.3008,  0.4291,\n",
      "         0.0258, -0.4424, -0.0759,  0.2866, -0.0831,  0.0890, -0.4602,  0.4397,\n",
      "         0.1563, -0.6206, -0.0063, -0.2449, -0.2608, -0.1863, -0.3237,  0.3002,\n",
      "        -0.3531, -0.1133,  0.2867,  0.4052, -0.0124,  0.4383,  0.4587,  0.1539])), ('layers.layer_6.weight', tensor([[-1.1828e+00,  1.8257e+00, -1.3611e+00,  1.4818e+00, -8.4078e-02,\n",
      "         -7.9093e-01,  5.5692e-01,  1.1233e+00,  5.3787e-01,  3.7652e-01,\n",
      "         -3.0401e-02, -2.3056e-01, -7.0879e-01, -5.6348e-01, -2.2575e-01,\n",
      "         -4.8644e-01,  3.0231e-02,  2.2147e-03, -4.3128e-01, -1.3364e+00,\n",
      "          2.9929e-01,  1.4775e+00, -3.7183e-01, -2.1643e-01, -6.6654e-01,\n",
      "         -3.8796e-01,  6.5415e-01, -9.4031e-01,  1.1246e+00, -3.1984e-01,\n",
      "          1.6295e+00,  1.0443e+00,  9.5878e-01,  7.0762e-01, -1.0720e+00,\n",
      "          4.5278e-02, -5.0047e-01, -1.0552e+00,  1.5140e-01,  1.1027e+00,\n",
      "          3.7126e-04, -1.6356e+00, -7.1347e-01,  1.1254e+00, -5.9669e-01,\n",
      "          2.5050e-01, -1.5801e+00,  7.7057e-01,  1.0391e+00, -1.3532e+00,\n",
      "          8.8068e-01, -6.7571e-01, -3.8504e-01, -1.2947e+00, -1.6950e+00,\n",
      "          8.6060e-01, -1.1744e-01, -4.9611e-02,  3.4986e-01,  7.2224e-01,\n",
      "         -5.4476e-01, -5.3203e-02,  1.6887e-01,  2.7041e-01],\n",
      "        [ 1.1967e+00, -2.2415e+00,  1.9824e+00, -1.5030e+00, -5.1132e-01,\n",
      "          6.2597e-01, -6.1970e-01, -7.6985e-01, -1.1391e+00, -4.9562e-01,\n",
      "         -3.6808e-01,  1.2002e-01,  1.1094e+00,  7.9760e-01,  5.5734e-01,\n",
      "          7.7080e-01,  2.6155e-01, -3.1029e-01,  8.6603e-02,  7.1923e-01,\n",
      "          3.5411e-01, -1.0393e+00,  7.9739e-01,  7.5496e-02, -3.5607e-02,\n",
      "          1.1357e-01, -5.9924e-01,  2.0239e+00, -1.0264e+00,  3.7105e-01,\n",
      "         -1.7315e+00, -1.3120e+00, -1.1676e+00, -8.6674e-01,  1.5769e+00,\n",
      "         -7.0459e-01,  2.1018e-01,  1.4820e+00, -3.1281e-01, -1.1205e+00,\n",
      "         -6.2485e-01,  1.6550e+00,  1.3389e+00, -7.6860e-01,  1.7649e+00,\n",
      "          2.6888e-01,  1.6311e+00, -1.0583e+00, -1.3505e+00,  6.1214e-01,\n",
      "         -1.1070e+00,  1.3890e+00,  1.8867e-01,  2.0667e+00,  1.3666e+00,\n",
      "         -8.8048e-01, -3.7538e-01,  3.6229e-01, -6.5740e-01, -2.3254e-01,\n",
      "          3.1089e-01,  7.2280e-02, -2.2100e-01, -1.9136e-01]])), ('layers.layer_6.bias', tensor([ 2.6527, -4.0855]))])\n"
     ]
    }
   ],
   "source": [
    "model.dnn.load_state_dict(torch.load(path + '/model.pt'))\n",
    "\n",
    "print(model.dnn.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd52a59",
   "metadata": {},
   "source": [
    "## Predict and Plot the Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff2236a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_figure(figsize, xlim, ylim):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.spines[['top', 'right']].set_visible(False)\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_ode_solution(ax, y, *args, **kwargs):\n",
    "    \n",
    "    ax.plot(y[:,0], y[:,1], '.-', *args, **kwargs)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f561ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_standard(model, y0, max_t_pred, delta_t):\n",
    "    \n",
    "    times = np.linspace(0, max_t_pred, int(max_t_pred / delta_t) + 1)\n",
    "    times = times[:,np.newaxis]\n",
    "    \n",
    "    y0 = np.array([y0 for _ in range(len(times))])\n",
    "    trajectory =  model.predict(times, y0)\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "\n",
    "def predict_dac(model, y0, max_t_pred, delta_t):\n",
    "    \"\"\"\n",
    "    detla_t should devide model.max_t to guarantee equidistant steps\n",
    "    \"\"\"\n",
    "    times = np.arange(0, model.T + delta_t, delta_t)[1:]\n",
    "    times = times[:,np.newaxis]\n",
    "    n_resets = int(np.ceil(max_t_pred / model.T))\n",
    "    \n",
    "    trajectory = np.array([y0])\n",
    "    \n",
    "    for _ in range(n_resets):\n",
    "        \n",
    "        y0 = trajectory[-1]\n",
    "        y0 = np.array([y0 for _ in range(len(times))])\n",
    "        segment =  model.predict(times, y0)\n",
    "        trajectory = np.vstack([trajectory, segment])\n",
    "    \n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2f012bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" need to change the initial values here \"\"\"\n",
    "\n",
    "# Note that max_t in training is 1\n",
    "y0 = [1., 0.15]\n",
    "max_t_pred = 7.\n",
    "delta_t = 0.05\n",
    "\n",
    "validation_dac = predict_dac(model, y0, max_t_pred, delta_t)\n",
    "validation_standard = predict_standard(model, y0, max_t_pred, delta_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74e11fc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\" true solution via solver (returns np.array) \"\"\"\n",
    "\n",
    "\n",
    "def func(t, r):\n",
    "    x, y = r\n",
    "    dx_t = x - x * y\n",
    "    dy_t = x * y - y\n",
    "    return dx_t, dy_t\n",
    "\n",
    "\n",
    "def gen_truedata():\n",
    "    t = np.linspace(0, max_t_pred, int(max_t_pred / delta_t) + 1)\n",
    "\n",
    "    sol = integrate.solve_ivp(func, (0, 10), (y0[0], y0[1]), t_eval=t) \n",
    "    x_true, y_true = sol.y\n",
    "\n",
    "    return np.stack((x_true, y_true), axis = 1)\n",
    "\n",
    "\n",
    "true_solution = gen_truedata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21cf58fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "fig, ax = generate_figure(figsize=(8,8), xlim=[-7, 7], ylim=[-7, 7]) # probably need to change the limits\n",
    "\n",
    "ax = plot_ode_solution(ax, validation_standard, label=\"Standard approach\", color=\"#03468F\")\n",
    "ax = plot_ode_solution(ax, validation_dac, label=\"DaC approach\", color=\"#A51C30\")\n",
    "ax = plot_ode_solution(ax, true_solution, label=\"true solution\", color=\"orange\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"proof_of_concept.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5beae9835b3277c3a4a8c87413b972e297eaccb765a3f62b691c35696bfb6223"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
